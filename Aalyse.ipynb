{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b556d323",
   "metadata": {},
   "source": [
    "# Step 1: Load Transcript Data\n",
    "\n",
    "**Input Files:**\n",
    "- `course_material/transcripts/*.p` (pickle files for each company)\n",
    "\n",
    "**Output:**\n",
    "- Variable: `transcripts` (list of transcript dictionaries)\n",
    "\n",
    "This script loads transcript data for use in subsequent analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f366064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: LOADING TRANSCRIPT DATA\n",
      "================================================================================\n",
      "Loading transcript data...\n",
      "============================================================\n",
      "✓ Loaded 57 transcripts for UNH\n",
      "✓ Loaded 65 transcripts for MET\n",
      "✓ Loaded 89 transcripts for HD\n",
      "✓ Loaded 61 transcripts for BKNG\n",
      "✓ Loaded 61 transcripts for MCO\n",
      "✓ Loaded 68 transcripts for ABT\n",
      "✓ Loaded 64 transcripts for INTU\n",
      "✓ Loaded 65 transcripts for COST\n",
      "✓ Loaded 61 transcripts for TSM\n",
      "✓ Loaded 59 transcripts for NKE\n",
      "✓ Loaded 77 transcripts for LLY\n",
      "✓ Loaded 45 transcripts for AZN\n",
      "✓ Loaded 65 transcripts for MS\n",
      "✓ Loaded 77 transcripts for BAC\n",
      "✓ Loaded 142 transcripts for F\n",
      "✓ Loaded 67 transcripts for TMO\n",
      "✓ Loaded 97 transcripts for V\n",
      "✓ Loaded 82 transcripts for QCOM\n",
      "✓ Loaded 72 transcripts for AXP\n",
      "✓ Loaded 77 transcripts for MCD\n",
      "✓ Loaded 83 transcripts for IBM\n",
      "✓ Loaded 91 transcripts for JNJ\n",
      "✓ Loaded 61 transcripts for DUK\n",
      "✓ Loaded 55 transcripts for ISRG\n",
      "✓ Loaded 76 transcripts for XOM\n",
      "✓ Loaded 70 transcripts for ASML\n",
      "✓ Loaded 46 transcripts for FB\n",
      "✓ Loaded 90 transcripts for INTC\n",
      "✓ Loaded 63 transcripts for AMT\n",
      "✓ Loaded 52 transcripts for FDX\n",
      "✓ Loaded 70 transcripts for ORCL\n",
      "✓ Loaded 80 transcripts for BA\n",
      "✓ Loaded 99 transcripts for MA\n",
      "✓ Loaded 76 transcripts for CRM\n",
      "✓ Loaded 121 transcripts for T\n",
      "✓ Loaded 43 transcripts for TSLA\n",
      "✓ Loaded 78 transcripts for MDT\n",
      "✓ Loaded 61 transcripts for UNP\n",
      "✓ Loaded 67 transcripts for PEP\n",
      "✓ Loaded 59 transcripts for BLK\n",
      "✓ Loaded 84 transcripts for AMGN\n",
      "✓ Loaded 60 transcripts for SAP\n",
      "✓ Loaded 101 transcripts for BMY\n",
      "✓ Loaded 77 transcripts for NFLX\n",
      "✓ Loaded 51 transcripts for TMUS\n",
      "✓ Loaded 76 transcripts for HON\n",
      "✓ Loaded 62 transcripts for LMT\n",
      "✓ Loaded 48 transcripts for SNY\n",
      "✓ Loaded 66 transcripts for ADBE\n",
      "✓ Loaded 92 transcripts for JPM\n",
      "✓ Loaded 45 transcripts for TD\n",
      "✓ Loaded 72 transcripts for EBAY\n",
      "✓ Loaded 69 transcripts for GS\n",
      "✓ Loaded 66 transcripts for PM\n",
      "✓ Loaded 93 transcripts for CMCSA\n",
      "✓ Loaded 100 transcripts for GM\n",
      "✓ Loaded 86 transcripts for PG\n",
      "✓ Loaded 77 transcripts for GOOG\n",
      "✓ Loaded 54 transcripts for UPS\n",
      "✓ Loaded 102 transcripts for MSFT\n",
      "============================================================\n",
      "Total transcripts loaded: 4373\n",
      "Companies with data: 60\n",
      "Companies missing data: 0\n",
      "Successfully loaded 4373 transcripts\n",
      "Transcripts are now available in memory for use in subsequent cells\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "OUT_FIG    = \"exports/figures\"\n",
    "OUT_TAB    = \"exports/tables\"\n",
    "REPORT_DIR = \"exports/JsonFile\"\n",
    "\n",
    "for d in [OUT_FIG, OUT_TAB, REPORT_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "# Define Group 7 tickers\n",
    "TickersOfCompany = [\n",
    "    'UNH', 'MET', 'HD', 'BKNG', 'MCO', 'ABT', 'INTU', 'COST', \n",
    "    'TSM', 'NKE', 'LLY', 'AZN', 'MS', 'BAC', 'F', 'TMO', 'V', \n",
    "    'QCOM', 'AXP', 'MCD', 'IBM', 'JNJ', 'DUK', 'ISRG', 'XOM', \n",
    "    'ASML', 'FB', 'INTC', 'AMT', 'FDX', 'ORCL', 'BA', 'MA', \n",
    "    'CRM', 'T', 'TSLA', 'MDT', 'UNP', 'PEP', 'BLK', 'AMGN', \n",
    "    'SAP', 'BMY', 'NFLX', 'TMUS', 'HON', 'LMT', 'SNY', 'ADBE', \n",
    "    'JPM', 'TD', 'EBAY', 'GS', 'PM', 'CMCSA', 'GM', 'PG', 'GOOG', 'UPS', 'MSFT'\n",
    "]\n",
    "# TickersOfCompany = [\n",
    "#      'MSFT', 'INTC', 'NFLX', 'MA', 'V','JPM','BMY','JNJ', 'AMGN', 'XOM','UNH'\n",
    "# ]\n",
    "\n",
    "def load_transcripts(transcript_folder: str, tickers: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load transcripts for specified tickers\n",
    "    \n",
    "    Args:\n",
    "        transcript_folder (str): Path to folder containing .p files\n",
    "        tickers (List[str]): List of ticker symbols to load\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of transcript dictionaries\n",
    "    \"\"\"\n",
    "    transcriptsDATACompany = []\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    print(\"Loading transcript data...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            file_path = os.path.join(transcript_folder, f\"{ticker}.p\")\n",
    "            \n",
    "            # Load pickle file\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            transcriptsDATACompany.extend(data)\n",
    "            success_count += 1\n",
    "            print(f\"✓ Loaded {len(data)} transcripts for {ticker}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            error_count += 1\n",
    "            print(f\"✗ No transcript file found for {ticker}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_count += 1\n",
    "            print(f\"✗ Error loading {ticker}: {str(e)}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total transcripts loaded: {len(transcriptsDATACompany)}\")\n",
    "    print(f\"Companies with data: {success_count}\")\n",
    "    print(f\"Companies missing data: {error_count}\")\n",
    "    \n",
    "    return transcriptsDATACompany\n",
    "\n",
    "def step1_load_transcripts():\n",
    "    \"\"\"Main function to load transcript data\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1: LOADING TRANSCRIPT DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    transcripts = load_transcripts(\"course_material/transcripts\", TickersOfCompany)\n",
    "    \n",
    "    if not transcripts:\n",
    "        print(\"No transcripts loaded. Please check file paths.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Successfully loaded {len(transcripts)} transcripts\")\n",
    "    print(\"Transcripts are now available in memory for use in subsequent cells\")\n",
    "    \n",
    "    return transcripts\n",
    "\n",
    "# Load transcripts and store in variable for use in subsequent cells\n",
    "transcripts = step1_load_transcripts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb0b05",
   "metadata": {},
   "source": [
    "# Step 2: Extract Event Dates\n",
    "\n",
    "**Input:**\n",
    "- Variable: `transcripts` (from Step 1)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/JsonFile/event_dates.json`\n",
    "- `exports/tables/WRDS_Event_Dates.csv`\n",
    "- `exports/tables/Event_Dates_Simple.txt`\n",
    "This script extracts event dates from loaded transcripts for WRDS upload.\n",
    "Uses transcripts variable directly from previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a953ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 2: EXTRACTING EVENT DATES FOR WRDS\n",
      "================================================================================\n",
      "[OK] Using 4373 transcripts from memory\n",
      "Extracting event dates and information...\n",
      "============================================================\n",
      "✓ Extracted 4373 events\n",
      "✓ Removed 25 duplicate events\n",
      "✓ Final unique events: 4348\n",
      "✓ Date range: 2005-10-31 00:40:28 to 2021-03-12 01:38:07\n",
      "✓ Unique companies: 60\n",
      "\n",
      "Saving event data for WRDS...\n",
      "============================================================\n",
      "✓ WRDS CSV saved to: exports/tables\\WRDS_Event_Dates.csv\n",
      "✓ Excel file saved to: exports/tables\\Event_Analysis_Data.xlsx\n",
      "✓ JSON format saved to: exports/JsonFile\\event_dates.json\n",
      "✓ Text format saved to: exports/tables\\Event_Dates_Simple.txt\n",
      "\n",
      "============================================================\n",
      "EVENT SUMMARY STATISTICS\n",
      "============================================================\n",
      "Total events: 4348\n",
      "Unique companies: 60\n",
      "Date range: 2005-10-31 to 2021-03-12\n",
      "Events per company (average): 72.5\n",
      "\n",
      "Events by year:\n",
      "  2005: 9 events\n",
      "  2006: 104 events\n",
      "  2007: 167 events\n",
      "  2008: 232 events\n",
      "  2009: 212 events\n",
      "  2010: 181 events\n",
      "  2011: 247 events\n",
      "  2012: 304 events\n",
      "  2013: 445 events\n",
      "  2014: 353 events\n",
      "  2015: 347 events\n",
      "  2016: 334 events\n",
      "  2017: 359 events\n",
      "  2018: 367 events\n",
      "  2019: 347 events\n",
      "  2020: 298 events\n",
      "  2021: 42 events\n",
      "\n",
      "Top 10 companies by number of events:\n",
      "  F: 139 events\n",
      "  T: 121 events\n",
      "  MSFT: 101 events\n",
      "  GM: 100 events\n",
      "  BMY: 99 events\n",
      "  MA: 99 events\n",
      "  V: 96 events\n",
      "  JPM: 92 events\n",
      "  CMCSA: 91 events\n",
      "  HD: 89 events\n",
      "\n",
      "Sample event dates (first 10):\n",
      "ticker event_date_formatted  quarter  fiscal_year\n",
      "   ABT           2007-07-18        2         2007\n",
      "   ABT           2007-10-17        3         2007\n",
      "   ABT           2008-01-23        4         2007\n",
      "   ABT           2008-04-16        1         2008\n",
      "   ABT           2008-07-17        2         2008\n",
      "   ABT           2008-10-16        3         2008\n",
      "   ABT           2009-01-12       -1         2008\n",
      "   ABT           2009-01-21        4         2008\n",
      "   ABT           2009-04-18        1         2009\n",
      "   ABT           2009-07-15        2         2009\n",
      "\n",
      "============================================================\n",
      "INSTRUCTIONS FOR WRDS EVENT STUDY\n",
      "============================================================\n",
      "1. Upload file: exports/tables\\WRDS_Event_Dates.csv\n",
      "2. WRDS Event Study Settings:\n",
      "   - Estimation Window: -250 to -46 days (before event)\n",
      "   - Event Window: -1 to +1 days (around event)\n",
      "   - Market Index: CRSP Value-Weighted Index\n",
      "   - Return Type: Daily returns\n",
      "3. Download CAR results with t-statistics\n",
      "4. Recommended event windows to test:\n",
      "   - CAR(-1,+1): Three-day window around announcement\n",
      "   - CAR(0,+1): Two-day window from announcement\n",
      "   - CAR(-2,+2): Five-day window for robustness\n",
      "============================================================\n",
      "[OK] Event dates extracted and saved for WRDS\n",
      "Files created:\n",
      "  - WRDS_Event_Dates.csv (Upload this to WRDS)\n",
      "  - Event_Analysis_Data.xlsx (Detailed analysis)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configuration\n",
    "OUT_TAB = \"exports/tables\"\n",
    "JSON_DIR = \"exports/JsonFile\"\n",
    "\n",
    "def extract_events(transcripts: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract event information from transcripts\n",
    "    \n",
    "    Args:\n",
    "        transcripts (List[Dict]): List of transcript dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with event information\n",
    "    \"\"\"\n",
    "    print(\"Extracting event dates and information...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    event_data = []\n",
    "    \n",
    "    for transcript in transcripts:\n",
    "        try:\n",
    "            event_info = {\n",
    "                'ticker': transcript.get('symbol', ''),\n",
    "                'quarter': transcript.get('quarter', ''),\n",
    "                'fiscal_year': transcript.get('year', ''),\n",
    "                'event_date': transcript.get('time', ''),\n",
    "                'event_type': 'Earnings Call',\n",
    "                'transcript_id': transcript.get('id', ''),\n",
    "                'title': transcript.get('title', ''),\n",
    "                'audio_url': transcript.get('audio', ''),\n",
    "                'participants_count': len(transcript.get('participant', [])) if transcript.get('participant') else 0,\n",
    "                'transcript_sections': len(transcript.get('transcript', [])) if transcript.get('transcript') else 0\n",
    "            }\n",
    "            event_data.append(event_info)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing transcript: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    events_df = pd.DataFrame(event_data)\n",
    "    \n",
    "    if len(events_df) > 0:\n",
    "        # Convert event_date to proper datetime format\n",
    "        events_df['event_date'] = pd.to_datetime(events_df['event_date'])\n",
    "        \n",
    "        # Sort by ticker and date\n",
    "        events_df = events_df.sort_values(['ticker', 'event_date'])\n",
    "        \n",
    "        # Add formatted date columns\n",
    "        events_df['event_date_formatted'] = events_df['event_date'].dt.strftime('%Y-%m-%d')\n",
    "        events_df['year'] = events_df['event_date'].dt.year\n",
    "        events_df['month'] = events_df['event_date'].dt.month\n",
    "        events_df['day'] = events_df['event_date'].dt.day\n",
    "        events_df['quarter_year'] = events_df['fiscal_year'].astype(str) + 'Q' + events_df['quarter'].astype(str)\n",
    "        \n",
    "        # Remove duplicate events (same ticker and date)\n",
    "        pre_dedup_count = len(events_df)\n",
    "        events_df = events_df.drop_duplicates(subset=['ticker', 'event_date_formatted'], keep='first')\n",
    "        post_dedup_count = len(events_df)\n",
    "        duplicates_removed = pre_dedup_count - post_dedup_count\n",
    "        \n",
    "        print(f\"✓ Extracted {pre_dedup_count} events\")\n",
    "        if duplicates_removed > 0:\n",
    "            print(f\"✓ Removed {duplicates_removed} duplicate events\")\n",
    "            print(f\"✓ Final unique events: {post_dedup_count}\")\n",
    "        print(f\"✓ Date range: {events_df['event_date'].min()} to {events_df['event_date'].max()}\")\n",
    "        print(f\"✓ Unique companies: {events_df['ticker'].nunique()}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"  No events extracted\")\n",
    "        \n",
    "    return events_df\n",
    "\n",
    "def save_for_wrds(events_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Save event data in formats suitable for WRDS upload\n",
    "    \n",
    "    Args:\n",
    "        events_df (pd.DataFrame): DataFrame with event information\n",
    "    \"\"\"\n",
    "    if events_df.empty:\n",
    "        print(\"  No events to save\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nSaving event data for WRDS...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. WRDS-compatible CSV format\n",
    "    wrds_format = events_df[['ticker', 'event_date_formatted', 'event_type']].copy()\n",
    "    wrds_format.columns = ['Ticker', 'Event_Date', 'Event_Type']\n",
    "    \n",
    "    wrds_csv_path = os.path.join(OUT_TAB, \"WRDS_Event_Dates.csv\")\n",
    "    wrds_format.to_csv(wrds_csv_path, index=False)\n",
    "    print(f\"✓ WRDS CSV saved to: {wrds_csv_path}\")\n",
    "    \n",
    "    # 2. Comprehensive Excel file with multiple sheets\n",
    "    excel_path = os.path.join(OUT_TAB, \"Event_Analysis_Data.xlsx\")\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Full event data\n",
    "        events_df.to_excel(writer, sheet_name='All_Events', index=False)\n",
    "        \n",
    "        # WRDS format\n",
    "        wrds_format.to_excel(writer, sheet_name='WRDS_Format', index=False)\n",
    "        \n",
    "        # Summary by company\n",
    "        summary_by_company = events_df.groupby('ticker').agg({\n",
    "            'event_date': ['count', 'min', 'max'],\n",
    "            'quarter': 'nunique',\n",
    "            'participants_count': 'mean',\n",
    "            'transcript_sections': 'mean'\n",
    "        }).round(2)\n",
    "        summary_by_company.columns = ['Total_Events', 'First_Event', 'Last_Event', \n",
    "                                    'Unique_Quarters', 'Avg_Participants', 'Avg_Sections']\n",
    "        summary_by_company.to_excel(writer, sheet_name='Company_Summary')\n",
    "        \n",
    "        # Summary by year\n",
    "        summary_by_year = events_df.groupby('year').agg({\n",
    "            'ticker': 'nunique',\n",
    "            'event_date': 'count',\n",
    "            'participants_count': 'mean',\n",
    "            'transcript_sections': 'mean'\n",
    "        }).round(2)\n",
    "        summary_by_year.columns = ['Unique_Companies', 'Total_Events', \n",
    "                                 'Avg_Participants', 'Avg_Sections']\n",
    "        summary_by_year.to_excel(writer, sheet_name='Year_Summary')\n",
    "        \n",
    "        # Summary by quarter\n",
    "        summary_by_quarter = events_df.groupby(['year', 'quarter']).agg({\n",
    "            'ticker': 'nunique',\n",
    "            'event_date': 'count'\n",
    "        }).round(2)\n",
    "        summary_by_quarter.columns = ['Unique_Companies', 'Total_Events']\n",
    "        summary_by_quarter.to_excel(writer, sheet_name='Quarter_Summary')\n",
    "    \n",
    "    print(f\"✓ Excel file saved to: {excel_path}\")\n",
    "    \n",
    "    # 3. JSON format for programmatic access\n",
    "    json_path = os.path.join(JSON_DIR, \"event_dates.json\")\n",
    "    events_json = events_df.to_dict('records')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(events_json, f, indent=2, default=str)\n",
    "    print(f\"✓ JSON format saved to: {json_path}\")\n",
    "    \n",
    "    # 4. Simple text format for quick reference\n",
    "    txt_path = os.path.join(OUT_TAB, \"Event_Dates_Simple.txt\")\n",
    "    with open(txt_path, 'w') as f:\n",
    "        f.write(\"TICKER\\tEVENT_DATE\\tQUARTER\\tFISCAL_YEAR\\n\")\n",
    "        for _, row in events_df.iterrows():\n",
    "            f.write(f\"{row['ticker']}\\t{row['event_date_formatted']}\\t\"\n",
    "                   f\"Q{row['quarter']}\\t{row['fiscal_year']}\\n\")\n",
    "    print(f\"✓ Text format saved to: {txt_path}\")\n",
    "    \n",
    "    # Print summary statistics and instructions\n",
    "    print_summary_statistics(events_df)\n",
    "    print_wrds_instructions(wrds_csv_path)\n",
    "\n",
    "def print_summary_statistics(events_df: pd.DataFrame):\n",
    "    \"\"\"Print summary statistics about the events\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVENT SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"Total events: {len(events_df)}\")\n",
    "    print(f\"Unique companies: {events_df['ticker'].nunique()}\")\n",
    "    print(f\"Date range: {events_df['event_date'].min().strftime('%Y-%m-%d')} to {events_df['event_date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Events per company (average): {len(events_df) / events_df['ticker'].nunique():.1f}\")\n",
    "    \n",
    "    print(\"\\nEvents by year:\")\n",
    "    year_counts = events_df['year'].value_counts().sort_index()\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} events\")\n",
    "    \n",
    "    print(f\"\\nTop 10 companies by number of events:\")\n",
    "    company_counts = events_df['ticker'].value_counts().head(10)\n",
    "    for ticker, count in company_counts.items():\n",
    "        print(f\"  {ticker}: {count} events\")\n",
    "    \n",
    "    print(f\"\\nSample event dates (first 10):\")\n",
    "    sample_events = events_df[['ticker', 'event_date_formatted', 'quarter', 'fiscal_year']].head(10)\n",
    "    print(sample_events.to_string(index=False))\n",
    "\n",
    "def print_wrds_instructions(wrds_csv_path: str):\n",
    "    \"\"\"Print instructions for using WRDS Event Study\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INSTRUCTIONS FOR WRDS EVENT STUDY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"1. Upload file: {wrds_csv_path}\")\n",
    "    print(\"2. WRDS Event Study Settings:\")\n",
    "    print(\"   - Estimation Window: -250 to -46 days (before event)\")\n",
    "    print(\"   - Event Window: -1 to +1 days (around event)\")\n",
    "    print(\"   - Market Index: CRSP Value-Weighted Index\")\n",
    "    print(\"   - Return Type: Daily returns\")\n",
    "    print(\"3. Download CAR results with t-statistics\")\n",
    "    print(\"4. Recommended event windows to test:\")\n",
    "    print(\"   - CAR(-1,+1): Three-day window around announcement\")\n",
    "    print(\"   - CAR(0,+1): Two-day window from announcement\")\n",
    "    print(\"   - CAR(-2,+2): Five-day window for robustness\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "def step2_extract_event_dates(transcripts_data):\n",
    "    \"\"\"Main function to extract event dates\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 2: EXTRACTING EVENT DATES FOR WRDS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check if transcripts data is available\n",
    "    if not transcripts_data:\n",
    "        print(\"[ERROR] No transcript data available.\")\n",
    "        print(\"Please run the previous cell (Step 1) first to load transcripts.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"[OK] Using {len(transcripts_data)} transcripts from memory\")\n",
    "    \n",
    "    # Extract and save events\n",
    "    event_data = extract_events(transcripts_data)\n",
    "    save_for_wrds(event_data)\n",
    "    \n",
    "    print(\"[OK] Event dates extracted and saved for WRDS\")\n",
    "    print(\"Files created:\")\n",
    "    print(\"  - WRDS_Event_Dates.csv (Upload this to WRDS)\")\n",
    "    print(\"  - Event_Analysis_Data.xlsx (Detailed analysis)\")\n",
    "    \n",
    "    return event_data\n",
    "\n",
    "# Extract event dates using transcripts from previous cell\n",
    "if 'transcripts' in locals() and transcripts:\n",
    "    event_data = step2_extract_event_dates(transcripts)\n",
    "else:\n",
    "    print(\"[WARNING] Transcripts not found. Please run Step 1 first.\")\n",
    "    event_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36579eeb",
   "metadata": {},
   "source": [
    "# Step 3: Sentiment Analysis\n",
    "\n",
    "**Input Files:**\n",
    "- `Loughran-McDonald_MasterDictionary_1993-2024.csv`\n",
    "- Variable: `transcripts` (from Step 1)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/JsonFile/sentiment_results.json`\n",
    "- `exports/tables/Sentiment_Analysis_Results.csv`\n",
    "This script performs sentiment analysis on loaded transcripts using spaCy and \n",
    "Loughran-McDonald dictionary following the proper pattern from reference notebooks.\n",
    "Uses transcripts variable directly from cell 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0c25f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ammaj\\AppData\\Roaming\\Python\\Python313\\site-packages\\cupy\\_environment.py:215: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ammaj\\AppData\\Roaming\\Python\\Python313\\site-packages\\cupy\\_environment.py:215: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#': Expected package name at the start of dependency specifier\n",
      "    #\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ spaCy model loaded successfully\n",
      "[OK] Found transcripts variable, starting spaCy-based sentiment analysis...\n",
      "================================================================================\n",
      "STEP 3: SENTIMENT ANALYSIS WITH SPACY\n",
      "================================================================================\n",
      "[OK] Using 4373 transcripts from memory\n",
      "Loading Loughran-McDonald sentiment dictionary...\n",
      "✓ Loading from Excel file: LoughranMcDonald_SentimentWordLists_2018.xlsx\n",
      "✓ Converted to Python lists\n",
      "✓ Processed with spaCy (lemmatized, lowercase)\n",
      "✓ Converted to sets for fast word matching\n",
      "✓ Final counts - Positive: 298, Negative: 1816, Uncertainty: 238\n",
      "\n",
      "=== PROCESSING TRANSCRIPTS ONE BY ONE ===\n",
      "✓ Processed 1/4373: UNH Q4\n",
      "  Pos: 0.0378, Neg: 0.0239, Net: 0.0139\n",
      "✓ Processed 2/4373: UNH Q3\n",
      "  Pos: 0.0383, Neg: 0.0223, Net: 0.0160\n",
      "✓ Processed 3/4373: UNH Q2\n",
      "  Pos: 0.0357, Neg: 0.0305, Net: 0.0053\n",
      "✓ Processed 4/4373: UNH Q1\n",
      "  Pos: 0.0336, Neg: 0.0402, Net: -0.0066\n",
      "✓ Processed 5/4373: UNH Q4\n",
      "  Pos: 0.0478, Neg: 0.0252, Net: 0.0226\n",
      "✓ Processed 50/4373: UNH Q1\n",
      "  Pos: 0.0435, Neg: 0.0207, Net: 0.0229\n",
      "✓ Processed 100/4373: MET Q2\n",
      "  Pos: 0.0299, Neg: 0.0248, Net: 0.0051\n",
      "✓ Processed 150/4373: HD Q0\n",
      "  Pos: 0.0341, Neg: 0.0071, Net: 0.0270\n",
      "✓ Processed 200/4373: HD Q2\n",
      "  Pos: 0.0374, Neg: 0.0260, Net: 0.0114\n",
      "✓ Processed 250/4373: BKNG Q4\n",
      "  Pos: 0.0245, Neg: 0.0254, Net: -0.0008\n",
      "✓ Processed 300/4373: MCO Q3\n",
      "  Pos: 0.0263, Neg: 0.0204, Net: 0.0058\n",
      "✓ Processed 350/4373: ABT Q1\n",
      "  Pos: 0.0386, Neg: 0.0286, Net: 0.0100\n",
      "✓ Processed 400/4373: ABT Q2\n",
      "  Pos: 0.0266, Neg: 0.0179, Net: 0.0087\n",
      "✓ Processed 450/4373: INTU Q1\n",
      "  Pos: 0.0383, Neg: 0.0163, Net: 0.0220\n",
      "✓ Processed 500/4373: COST Q1\n",
      "  Pos: 0.0169, Neg: 0.0111, Net: 0.0058\n",
      "✓ Processed 550/4373: TSM Q4\n",
      "  Pos: 0.0282, Neg: 0.0257, Net: 0.0025\n",
      "✓ Processed 600/4373: NKE Q2\n",
      "  Pos: 0.0569, Neg: 0.0142, Net: 0.0427\n",
      "✓ Processed 650/4373: NKE Q2\n",
      "  Pos: 0.0509, Neg: 0.0136, Net: 0.0373\n",
      "✓ Processed 700/4373: LLY Q4\n",
      "  Pos: 0.0282, Neg: 0.0289, Net: -0.0007\n",
      "✓ Processed 750/4373: AZN Q2\n",
      "  Pos: 0.0350, Neg: 0.0247, Net: 0.0103\n",
      "✓ Processed 800/4373: MS Q3\n",
      "  Pos: 0.0356, Neg: 0.0186, Net: 0.0170\n",
      "✓ Processed 850/4373: BAC Q3\n",
      "  Pos: 0.0345, Neg: 0.0184, Net: 0.0161\n",
      "✓ Processed 900/4373: BAC Q4\n",
      "  Pos: 0.0220, Neg: 0.0359, Net: -0.0140\n",
      "✓ Processed 950/4373: F Q2\n",
      "  Pos: 0.0378, Neg: 0.0238, Net: 0.0140\n",
      "✓ Processed 1000/4373: F Q-1\n",
      "  Pos: 0.0363, Neg: 0.0236, Net: 0.0127\n",
      "✓ Processed 1050/4373: F Q-1\n",
      "  Pos: 0.0125, Neg: 0.0257, Net: -0.0132\n",
      "✓ Processed 1100/4373: TMO Q-1\n",
      "  Pos: 0.0496, Neg: 0.0095, Net: 0.0401\n",
      "✓ Processed 1150/4373: V Q0\n",
      "  Pos: 0.0459, Neg: 0.0137, Net: 0.0323\n",
      "✓ Processed 1200/4373: V Q2\n",
      "  Pos: 0.0253, Neg: 0.0250, Net: 0.0003\n",
      "✓ Processed 1250/4373: QCOM Q4\n",
      "  Pos: 0.0397, Neg: 0.0209, Net: 0.0188\n",
      "✓ Processed 1300/4373: QCOM Q1\n",
      "  Pos: 0.0321, Neg: 0.0169, Net: 0.0152\n",
      "✓ Processed 1350/4373: AXP Q2\n",
      "  Pos: 0.0129, Neg: 0.0175, Net: -0.0046\n",
      "✓ Processed 1400/4373: MCD Q3\n",
      "  Pos: 0.0509, Neg: 0.0170, Net: 0.0338\n",
      "✓ Processed 1450/4373: MCD Q1\n",
      "  Pos: 0.0422, Neg: 0.0201, Net: 0.0222\n",
      "✓ Processed 1500/4373: IBM Q3\n",
      "  Pos: 0.0331, Neg: 0.0288, Net: 0.0043\n",
      "✓ Processed 1550/4373: JNJ Q-1\n",
      "  Pos: 0.0363, Neg: 0.0203, Net: 0.0160\n",
      "✓ Processed 1600/4373: JNJ Q0\n",
      "  Pos: 0.0385, Neg: 0.0232, Net: 0.0152\n",
      "✓ Processed 1650/4373: DUK Q4\n",
      "  Pos: 0.0287, Neg: 0.0173, Net: 0.0113\n",
      "✓ Processed 1700/4373: ISRG Q3\n",
      "  Pos: 0.0255, Neg: 0.0156, Net: 0.0100\n",
      "✓ Processed 1750/4373: XOM Q2\n",
      "  Pos: 0.0313, Neg: 0.0254, Net: 0.0059\n",
      "✓ Processed 1800/4373: XOM Q4\n",
      "  Pos: 0.0307, Neg: 0.0200, Net: 0.0107\n",
      "✓ Processed 1850/4373: ASML Q-1\n",
      "  Pos: 0.0235, Neg: 0.0101, Net: 0.0134\n",
      "✓ Processed 1900/4373: FB Q0\n",
      "  Pos: 0.0306, Neg: 0.0341, Net: -0.0034\n",
      "✓ Processed 1950/4373: INTC Q-1\n",
      "  Pos: 0.0277, Neg: 0.0183, Net: 0.0094\n",
      "✓ Processed 2000/4373: INTC Q1\n",
      "  Pos: 0.0312, Neg: 0.0236, Net: 0.0076\n",
      "✓ Processed 2050/4373: AMT Q-1\n",
      "  Pos: 0.0275, Neg: 0.0076, Net: 0.0199\n",
      "✓ Processed 2100/4373: FDX Q1\n",
      "  Pos: 0.0305, Neg: 0.0234, Net: 0.0071\n",
      "✓ Processed 2150/4373: ORCL Q4\n",
      "  Pos: 0.0225, Neg: 0.0171, Net: 0.0055\n",
      "✓ Processed 2200/4373: ORCL Q1\n",
      "  Pos: 0.0284, Neg: 0.0160, Net: 0.0124\n",
      "✓ Processed 2250/4373: BA Q2\n",
      "  Pos: 0.0406, Neg: 0.0221, Net: 0.0185\n",
      "✓ Processed 2300/4373: MA Q-1\n",
      "  Pos: 0.0274, Neg: 0.0192, Net: 0.0082\n",
      "✓ Processed 2350/4373: MA Q-1\n",
      "  Pos: 0.0383, Neg: 0.0137, Net: 0.0246\n",
      "✓ Processed 2400/4373: CRM Q-1\n",
      "  Pos: 0.0349, Neg: 0.0129, Net: 0.0220\n",
      "✓ Processed 2450/4373: CRM Q3\n",
      "  Pos: 0.0269, Neg: 0.0209, Net: 0.0060\n",
      "✓ Processed 2500/4373: T Q-1\n",
      "  Pos: 0.0397, Neg: 0.0145, Net: 0.0251\n",
      "✓ Processed 2550/4373: T Q2\n",
      "  Pos: 0.0443, Neg: 0.0152, Net: 0.0291\n",
      "✓ Processed 2600/4373: TSLA Q0\n",
      "  Pos: 0.0249, Neg: 0.0189, Net: 0.0060\n",
      "✓ Processed 2650/4373: MDT Q2\n",
      "  Pos: 0.0313, Neg: 0.0230, Net: 0.0083\n",
      "✓ Processed 2700/4373: MDT Q4\n",
      "  Pos: 0.0448, Neg: 0.0184, Net: 0.0264\n",
      "✓ Processed 2750/4373: UNP Q4\n",
      "  Pos: 0.0420, Neg: 0.0254, Net: 0.0165\n",
      "✓ Processed 2800/4373: PEP Q-1\n",
      "  Pos: 0.0456, Neg: 0.0122, Net: 0.0333\n",
      "✓ Processed 2850/4373: BLK Q3\n",
      "  Pos: 0.0460, Neg: 0.0189, Net: 0.0271\n",
      "✓ Processed 2900/4373: AMGN Q4\n",
      "  Pos: 0.0389, Neg: 0.0383, Net: 0.0006\n",
      "✓ Processed 2950/4373: AMGN Q2\n",
      "  Pos: 0.0382, Neg: 0.0286, Net: 0.0096\n",
      "✓ Processed 3000/4373: SAP Q1\n",
      "  Pos: 0.0414, Neg: 0.0188, Net: 0.0226\n",
      "✓ Processed 3050/4373: BMY Q-1\n",
      "  Pos: 0.0514, Neg: 0.0112, Net: 0.0402\n",
      "✓ Processed 3100/4373: BMY Q-1\n",
      "  Pos: 0.0464, Neg: 0.0126, Net: 0.0338\n",
      "✓ Processed 3150/4373: NFLX Q2\n",
      "  Pos: 0.0493, Neg: 0.0115, Net: 0.0378\n",
      "✓ Processed 3200/4373: NFLX Q2\n",
      "  Pos: 0.0418, Neg: 0.0254, Net: 0.0164\n",
      "✓ Processed 3250/4373: TMUS Q2\n",
      "  Pos: 0.0282, Neg: 0.0158, Net: 0.0124\n",
      "✓ Processed 3300/4373: HON Q-1\n",
      "  Pos: 0.0375, Neg: 0.0046, Net: 0.0329\n",
      "✓ Processed 3350/4373: LMT Q3\n",
      "  Pos: 0.0430, Neg: 0.0275, Net: 0.0154\n",
      "✓ Processed 3400/4373: LMT Q3\n",
      "  Pos: 0.0310, Neg: 0.0213, Net: 0.0097\n",
      "✓ Processed 3450/4373: SNY Q4\n",
      "  Pos: 0.0315, Neg: 0.0262, Net: 0.0053\n",
      "✓ Processed 3500/4373: ADBE Q1\n",
      "  Pos: 0.0321, Neg: 0.0136, Net: 0.0185\n",
      "✓ Processed 3550/4373: JPM Q-1\n",
      "  Pos: 0.0370, Neg: 0.0199, Net: 0.0171\n",
      "✓ Processed 3600/4373: JPM Q2\n",
      "  Pos: 0.0260, Neg: 0.0444, Net: -0.0184\n",
      "✓ Processed 3650/4373: TD Q4\n",
      "  Pos: 0.0363, Neg: 0.0334, Net: 0.0029\n",
      "✓ Processed 3700/4373: EBAY Q2\n",
      "  Pos: 0.0343, Neg: 0.0122, Net: 0.0221\n",
      "✓ Processed 3750/4373: GS Q1\n",
      "  Pos: 0.0308, Neg: 0.0335, Net: -0.0027\n",
      "✓ Processed 3800/4373: PM Q4\n",
      "  Pos: 0.0282, Neg: 0.0340, Net: -0.0058\n",
      "✓ Processed 3850/4373: PM Q2\n",
      "  Pos: 0.0368, Neg: 0.0318, Net: 0.0050\n",
      "✓ Processed 3900/4373: CMCSA Q1\n",
      "  Pos: 0.0428, Neg: 0.0223, Net: 0.0205\n",
      "✓ Processed 3950/4373: CMCSA Q3\n",
      "  Pos: 0.0297, Neg: 0.0182, Net: 0.0115\n",
      "✓ Processed 4000/4373: GM Q2\n",
      "  Pos: 0.0404, Neg: 0.0247, Net: 0.0157\n",
      "✓ Processed 4050/4373: GM Q2\n",
      "  Pos: 0.0434, Neg: 0.0282, Net: 0.0153\n",
      "✓ Processed 4100/4373: PG Q4\n",
      "  Pos: 0.0401, Neg: 0.0217, Net: 0.0185\n",
      "✓ Processed 4150/4373: GOOG Q2\n",
      "  Pos: 0.0449, Neg: 0.0145, Net: 0.0304\n",
      "✓ Processed 4200/4373: GOOG Q2\n",
      "  Pos: 0.0295, Neg: 0.0335, Net: -0.0040\n",
      "✓ Processed 4250/4373: UPS Q4\n",
      "  Pos: 0.0419, Neg: 0.0207, Net: 0.0211\n",
      "✓ Processed 4300/4373: MSFT Q2\n",
      "  Pos: 0.0313, Neg: 0.0166, Net: 0.0147\n",
      "✓ Processed 4350/4373: MSFT Q3\n",
      "  Pos: 0.0427, Neg: 0.0139, Net: 0.0288\n",
      "✓ Processed 4373/4373: MSFT Q2\n",
      "  Pos: 0.0359, Neg: 0.0199, Net: 0.0159\n",
      "\n",
      "✓ Sentiment analysis completed: 4373 transcripts processed\n",
      "\n",
      "=== SENTIMENT SUMMARY ===\n",
      "Average Positive Ratio: 0.0352\n",
      "Average Negative Ratio: 0.0217\n",
      "Average Uncertainty Ratio: 0.0126\n",
      "Average Net Sentiment: 0.0135\n",
      "\n",
      "=== SAVING RESULTS ===\n",
      "✓ CSV saved to: exports/tables\\Sentiment_Analysis_Results.csv\n",
      "✓ Excel saved to: exports/tables\\Sentiment_Analysis_Complete.xlsx\n",
      "✓ JSON saved to: exports/JsonFile\\sentiment_results.json\n",
      "[OK] Added compatibility columns for subsequent analysis steps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load spaCy model\n",
    "!python -m spacy download en_core_web_md # Ensure model is downloaded\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    print(\"✓ spaCy model loaded successfully\")\n",
    "except OSError:\n",
    "    print(\"[ERROR] spaCy model 'en_core_web_md' not found.\")\n",
    "    print(\"Please install it using: python -m spacy download en_core_web_md\")\n",
    "    raise\n",
    "\n",
    "def process_spacy_doc(text):\n",
    "    \"\"\"\n",
    "    Process text using spaCy for financial sentiment analysis.\n",
    "    This is the EXACT function from the reference notebook.\n",
    "    \"\"\"\n",
    "    words = [\n",
    "        word.lemma_.lower()                 # Lemmatize and lowercase\n",
    "        for word in nlp(text)              # Tokenize with spaCy\n",
    "        if not (\n",
    "            word.is_space                   # Remove spaces\n",
    "            or word.is_stop                 # Remove stop words\n",
    "            or word.is_punct                # Remove punctuation\n",
    "        )\n",
    "    ]\n",
    "    return words\n",
    "\n",
    "def extract_transcript_text(transcript):\n",
    "    \"\"\"\n",
    "    Extract all speech text from a transcript dictionary.\n",
    "    Following the exact pattern from reference notebook.\n",
    "    \"\"\"\n",
    "    text_transcript = ''\n",
    "    \n",
    "    # Loop through all speech segments\n",
    "    for speech in transcript['transcript']:\n",
    "        # Extract the speech text (first element of the speech list)\n",
    "        speech_text = speech['speech'][0]\n",
    "        \n",
    "        # Add to our full text with space separator\n",
    "        text_transcript = text_transcript + ' ' + speech_text\n",
    "    \n",
    "    return text_transcript\n",
    "\n",
    "def load_loughran_mcdonald_dictionary():\n",
    "    \"\"\"\n",
    "    Load Loughran-McDonald dictionary and process with spaCy\n",
    "    Following the exact pattern from reference notebook.\n",
    "    \"\"\"\n",
    "    print(\"Loading Loughran-McDonald sentiment dictionary...\")\n",
    "    \n",
    "    # Try both possible file names\n",
    "    excel_files = [\n",
    "        \"LoughranMcDonald_SentimentWordLists_2018.xlsx\"\n",
    "    ]\n",
    "    \n",
    "    dictionary_loaded = False\n",
    "    \n",
    "    # Try Excel format first (preferred method from reference)\n",
    "    for excel_file in excel_files:\n",
    "        if os.path.exists(excel_file):\n",
    "            if excel_file.endswith('.xlsx'):\n",
    "                try:\n",
    "                    print(f\"✓ Loading from Excel file: {excel_file}\")\n",
    "                    \n",
    "                    LM_negative = pd.read_excel(excel_file, sheet_name='Negative', header=None)\n",
    "                    LM_positive = pd.read_excel(excel_file, sheet_name='Positive', header=None)\n",
    "                    LM_uncertainty = pd.read_excel(excel_file, sheet_name='Uncertainty', header=None)\n",
    "                    \n",
    "                    # Convert DataFrames to lists (simplified approach from reference)\n",
    "                    LM_positive = list(LM_positive[0])     # Get first column as list\n",
    "                    LM_negative = list(LM_negative[0])     # Get first column as list  \n",
    "                    LM_uncertainty = list(LM_uncertainty[0])  # Get first column as list\n",
    "                    \n",
    "                    print(\"✓ Converted to Python lists\")\n",
    "                    \n",
    "                    # Process with spaCy for consistency (from reference)\n",
    "                    LM_positive = process_spacy_doc(' '.join(LM_positive))\n",
    "                    LM_negative = process_spacy_doc(' '.join(LM_negative))\n",
    "                    LM_uncertainty = process_spacy_doc(' '.join(LM_uncertainty))\n",
    "                    \n",
    "                    print(\"✓ Processed with spaCy (lemmatized, lowercase)\")\n",
    "                    \n",
    "                    # Convert to sets for fast lookup\n",
    "                    LM_positive = set(LM_positive)\n",
    "                    LM_negative = set(LM_negative)\n",
    "                    LM_uncertainty = set(LM_uncertainty)\n",
    "                    \n",
    "                    print(f\"✓ Converted to sets for fast word matching\")\n",
    "                    print(f\"✓ Final counts - Positive: {len(LM_positive)}, Negative: {len(LM_negative)}, Uncertainty: {len(LM_uncertainty)}\")\n",
    "                    \n",
    "                    dictionary_loaded = True\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Could not load from {excel_file}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "            elif excel_file.endswith('.csv'):\n",
    "                try:\n",
    "                    print(f\"✓ Loading from CSV file: {excel_file}\")\n",
    "                    \n",
    "                    # Load CSV format\n",
    "                    df = pd.read_csv(excel_file)\n",
    "                    \n",
    "                    # Extract sentiment categories\n",
    "                    pos_words = df[df['Positive'] > 0]['Word'].str.lower().tolist()\n",
    "                    neg_words = df[df['Negative'] > 0]['Word'].str.lower().tolist()\n",
    "                    unc_words = df[df['Uncertainty'] > 0]['Word'].str.lower().tolist()\n",
    "                    \n",
    "                    # Process with spaCy\n",
    "                    LM_positive = set(process_spacy_doc(' '.join(pos_words)))\n",
    "                    LM_negative = set(process_spacy_doc(' '.join(neg_words)))\n",
    "                    LM_uncertainty = set(process_spacy_doc(' '.join(unc_words)))\n",
    "                    \n",
    "                    print(f\"✓ Final counts - Positive: {len(LM_positive)}, Negative: {len(LM_negative)}, Uncertainty: {len(LM_uncertainty)}\")\n",
    "                    \n",
    "                    dictionary_loaded = True\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Could not load from {excel_file}: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    if not dictionary_loaded:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find Loughran-McDonald dictionary. Please ensure one of these files exists:\\n\"\n",
    "            \"- LoughranMcDonald_SentimentWordLists_2018.xlsx\\n\"\n",
    "        )\n",
    "    \n",
    "    return LM_positive, LM_negative, LM_uncertainty\n",
    "\n",
    "def analyze_transcript_sentiment(transcript, LM_positive, LM_negative, LM_uncertainty):\n",
    "    \"\"\"\n",
    "    Analyze sentiment for a single transcript.\n",
    "    Following the exact pattern from reference notebook.\n",
    "    \"\"\"\n",
    "    # Extract text using the exact pattern from reference\n",
    "    text_transcript = extract_transcript_text(transcript)\n",
    "    \n",
    "    # Process with spaCy\n",
    "    processed_words = process_spacy_doc(text_transcript)\n",
    "    total_words = len(processed_words)\n",
    "    \n",
    "    if total_words == 0:\n",
    "        return {\n",
    "            'ticker': transcript.get('symbol', ''),\n",
    "            'quarter': transcript.get('quarter', ''),\n",
    "            'fiscal_year': transcript.get('year', ''),\n",
    "            'event_date': transcript.get('time', ''),\n",
    "            'transcript_id': transcript.get('id', ''),\n",
    "            'total_words': 0,\n",
    "            'positive_count': 0,\n",
    "            'negative_count': 0,\n",
    "            'uncertainty_count': 0,\n",
    "            'positive_ratio': 0.0,\n",
    "            'negative_ratio': 0.0,\n",
    "            'uncertainty_ratio': 0.0,\n",
    "            'net_sentiment': 0.0,\n",
    "            'sentiment_polarity': 0.0\n",
    "        }\n",
    "    \n",
    "    # Count sentiment words using sets for fast lookup\n",
    "    pos_count = sum(1 for word in processed_words if word in LM_positive)\n",
    "    neg_count = sum(1 for word in processed_words if word in LM_negative)\n",
    "    unc_count = sum(1 for word in processed_words if word in LM_uncertainty)\n",
    "    \n",
    "    # Calculate ratios\n",
    "    pos_ratio = pos_count / total_words\n",
    "    neg_ratio = neg_count / total_words\n",
    "    unc_ratio = unc_count / total_words\n",
    "    \n",
    "    # Calculate composite measures\n",
    "    net_sentiment = pos_ratio - neg_ratio\n",
    "    \n",
    "    # Sentiment polarity: (Positive - Negative) / (Positive + Negative)\n",
    "    pos_neg_sum = pos_count + neg_count\n",
    "    sentiment_polarity = (pos_count - neg_count) / pos_neg_sum if pos_neg_sum > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'ticker': transcript.get('symbol', ''),\n",
    "        'quarter': transcript.get('quarter', ''),\n",
    "        'fiscal_year': transcript.get('year', ''),\n",
    "        'event_date': transcript.get('time', ''),\n",
    "        'transcript_id': transcript.get('id', ''),\n",
    "        'total_words': total_words,\n",
    "        'positive_count': pos_count,\n",
    "        'negative_count': neg_count,\n",
    "        'uncertainty_count': unc_count,\n",
    "        'positive_ratio': pos_ratio,\n",
    "        'negative_ratio': neg_ratio,\n",
    "        'uncertainty_ratio': unc_ratio,\n",
    "        'net_sentiment': net_sentiment,\n",
    "        'sentiment_polarity': sentiment_polarity\n",
    "    }\n",
    "\n",
    "def step3_sentiment_analysis_with_spacy(transcripts_data):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on transcripts one by one using spaCy.\n",
    "    Following memory-efficient pattern from reference notebooks.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 3: SENTIMENT ANALYSIS WITH SPACY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Check if transcripts data is available\n",
    "    if not transcripts_data:\n",
    "        print(\"[ERROR] No transcript data available.\")\n",
    "        print(\"Please run Step 1 first to load transcripts.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"[OK] Using {len(transcripts_data)} transcripts from memory\")\n",
    "    \n",
    "    # Load Loughran-McDonald dictionary with spaCy processing\n",
    "    try:\n",
    "        LM_positive, LM_negative, LM_uncertainty = load_loughran_mcdonald_dictionary()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Could not load sentiment dictionary: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Process transcripts one by one (memory-efficient approach)\n",
    "    print(\"\\n=== PROCESSING TRANSCRIPTS ONE BY ONE ===\")\n",
    "    \n",
    "    results = []\n",
    "    total_transcripts = len(transcripts_data)\n",
    "    \n",
    "    for i, transcript in enumerate(transcripts_data):\n",
    "        try:\n",
    "            # Analyze sentiment for this transcript\n",
    "            result = analyze_transcript_sentiment(transcript, LM_positive, LM_negative, LM_uncertainty)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Progress indicator (every 50 transcripts or first few)\n",
    "            if (i + 1) % 50 == 0 or i < 5 or i == total_transcripts - 1:\n",
    "                print(f\"✓ Processed {i + 1}/{total_transcripts}: {result['ticker']} Q{result['quarter']}\")\n",
    "                print(f\"  Pos: {result['positive_ratio']:.4f}, Neg: {result['negative_ratio']:.4f}, Net: {result['net_sentiment']:.4f}\")\n",
    "            \n",
    "            # Clear variables to free memory (following reference pattern)\n",
    "            transcript = None\n",
    "            result = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Processing transcript {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    sentiment_df = pd.DataFrame(results)\n",
    "    \n",
    "    if not sentiment_df.empty:\n",
    "        # Convert event_date to datetime\n",
    "        sentiment_df['event_date'] = pd.to_datetime(sentiment_df['event_date'])\n",
    "        \n",
    "        # Sort by ticker and date\n",
    "        sentiment_df = sentiment_df.sort_values(['ticker', 'event_date'])\n",
    "        \n",
    "        print(f\"\\n✓ Sentiment analysis completed: {len(sentiment_df)} transcripts processed\")\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n=== SENTIMENT SUMMARY ===\")\n",
    "        print(f\"Average Positive Ratio: {sentiment_df['positive_ratio'].mean():.4f}\")\n",
    "        print(f\"Average Negative Ratio: {sentiment_df['negative_ratio'].mean():.4f}\")\n",
    "        print(f\"Average Uncertainty Ratio: {sentiment_df['uncertainty_ratio'].mean():.4f}\")\n",
    "        print(f\"Average Net Sentiment: {sentiment_df['net_sentiment'].mean():.4f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"[ERROR] No sentiment results generated\")\n",
    "        return None\n",
    "    \n",
    "    # Save results\n",
    "    save_sentiment_results(sentiment_df)\n",
    "    \n",
    "    return sentiment_df\n",
    "\n",
    "def save_sentiment_results(sentiment_df):\n",
    "    \"\"\"Save sentiment analysis results to files\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    OUT_TAB = \"exports/tables\"\n",
    "    JSON_DIR = \"exports/JsonFile\"\n",
    "    \n",
    "    print(\"\\n=== SAVING RESULTS ===\")\n",
    "    \n",
    "    # 1. CSV format for analysis\n",
    "    csv_path = os.path.join(OUT_TAB, \"Sentiment_Analysis_Results.csv\")\n",
    "    sentiment_df.to_csv(csv_path, index=False)\n",
    "    print(f\"✓ CSV saved to: {csv_path}\")\n",
    "    \n",
    "    # 2. Excel with multiple sheets\n",
    "    excel_path = os.path.join(OUT_TAB, \"Sentiment_Analysis_Complete.xlsx\")\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Full results\n",
    "        sentiment_df.to_excel(writer, sheet_name='All_Results', index=False)\n",
    "        \n",
    "        # Summary by company\n",
    "        summary_cols = ['positive_ratio', 'negative_ratio', 'uncertainty_ratio', 'net_sentiment']\n",
    "        summary_by_company = sentiment_df.groupby('ticker')[summary_cols].agg(['mean', 'std', 'count']).round(4)\n",
    "        summary_by_company.to_excel(writer, sheet_name='Company_Summary')\n",
    "        \n",
    "        # Key measures only\n",
    "        key_cols = ['ticker', 'event_date', 'quarter', 'fiscal_year', 'positive_ratio', \n",
    "                   'negative_ratio', 'uncertainty_ratio', 'net_sentiment', 'sentiment_polarity']\n",
    "        sentiment_df[key_cols].to_excel(writer, sheet_name='Key_Measures', index=False)\n",
    "    \n",
    "    print(f\"✓ Excel saved to: {excel_path}\")\n",
    "    \n",
    "    # 3. JSON format\n",
    "    json_path = os.path.join(JSON_DIR, \"sentiment_results.json\")\n",
    "    sentiment_json = sentiment_df.to_dict('records')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(sentiment_json, f, indent=2, default=str)\n",
    "    print(f\"✓ JSON saved to: {json_path}\")\n",
    "\n",
    "# Perform sentiment analysis using transcripts from cell 1\n",
    "if 'transcripts' in locals() and transcripts:\n",
    "    print(\"[OK] Found transcripts variable, starting spaCy-based sentiment analysis...\")\n",
    "    sentiment_results = step3_sentiment_analysis_with_spacy(transcripts)\n",
    "    \n",
    "    # Store results with better column names for compatibility\n",
    "    if sentiment_results is not None:\n",
    "        # Create compatibility columns for subsequent cells\n",
    "        sentiment_results['full_text_positive_ratio'] = sentiment_results['positive_ratio']\n",
    "        sentiment_results['full_text_negative_ratio'] = sentiment_results['negative_ratio']\n",
    "        sentiment_results['full_text_uncertainty_ratio'] = sentiment_results['uncertainty_ratio']\n",
    "        sentiment_results['full_text_net_sentiment_ratio'] = sentiment_results['net_sentiment']\n",
    "        sentiment_results['full_text_sentiment_polarity'] = sentiment_results['sentiment_polarity']\n",
    "        \n",
    "        print(\"[OK] Added compatibility columns for subsequent analysis steps\")\n",
    "else:\n",
    "    print(\"[WARNING] Transcripts not found. Please run Step 1 first.\")\n",
    "    sentiment_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4d48d",
   "metadata": {},
   "source": [
    "# Step 4: Merge CAR and Sentiment Data\n",
    "\n",
    "**Input Files:**\n",
    "- `CAR_DATA/CAR_-1_1_Results.csv` (from WRDS)\n",
    "- `CAR_DATA/CAR_0_1_Results.csv` (from WRDS)\n",
    "- `CAR_DATA/CAR_0_2_Results.csv` (from WRDS)\n",
    "- `exports/tables/Sentiment_Analysis_Results.csv` (from Step 3)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/tables/CAR_Sentiment_Merged.csv`\n",
    "This script merges CAR data from WRDS with sentiment analysis results.\n",
    "Supports multiple CAR windows: \n",
    "### CAR(-1,1), CAR(0,1), CAR(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671d92b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 4: MERGING CAR AND SENTIMENT DATA\n",
      "================================================================================\n",
      "Loading sentiment data...\n",
      " Sentiment data: 4373 records\n",
      "  Found 25 sentiment duplicates - removing...\n",
      "\n",
      "Loading CAR_-1_1_Results.csv...\n",
      " CAR_-1_1: 4373 records\n",
      "  Found 25 duplicates - removing...\n",
      "\n",
      "Loading CAR_0_1_Results.csv...\n",
      " CAR_0_1: 4348 records\n",
      "\n",
      "Loading CAR_0_2_Results.csv...\n",
      " CAR_0_2: 4348 records\n",
      "\n",
      "Merging CAR_-1_1...\n",
      " Matched 4166 events (95.8%)\n",
      "\n",
      "Merging CAR_0_1...\n",
      " Matched 3360 events (77.3%)\n",
      "\n",
      "Merging CAR_0_2...\n",
      " Matched 3390 events (78.0%)\n",
      "\n",
      " Final merged dataset: 4348 records\n",
      "   Companies: 60\n",
      "\n",
      " Files saved:\n",
      "   - exports/tables\\CAR_Sentiment_Merged.csv\n",
      "   - exports/tables\\CAR_Sentiment_Analysis.xlsx\n",
      "\n",
      " CORRELATIONS WITH SENTIMENT:\n",
      "\n",
      "CAR_-1_1:\n",
      "  negative_ratio: -0.0346\n",
      "  positive_ratio: 0.0196\n",
      "  uncertainty_ratio: 0.0214\n",
      "\n",
      "CAR_0_1:\n",
      "  negative_ratio: -0.0330\n",
      "  positive_ratio: 0.0188\n",
      "  uncertainty_ratio: 0.0264\n",
      "\n",
      "CAR_0_2:\n",
      "  negative_ratio: -0.0296\n",
      "  positive_ratio: 0.0199\n",
      "  uncertainty_ratio: 0.0265\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>event_date</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>total_words</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>uncertainty_count</th>\n",
       "      <th>positive_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>car_0_1_alpha</th>\n",
       "      <th>car_0_1_beta</th>\n",
       "      <th>car_0_1_rsquared</th>\n",
       "      <th>car_0_1_obs</th>\n",
       "      <th>car_0_2</th>\n",
       "      <th>car_0_2_tstat</th>\n",
       "      <th>car_0_2_alpha</th>\n",
       "      <th>car_0_2_beta</th>\n",
       "      <th>car_0_2_rsquared</th>\n",
       "      <th>car_0_2_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-07-18 19:16:50</td>\n",
       "      <td>ABT_2330</td>\n",
       "      <td>4580</td>\n",
       "      <td>122</td>\n",
       "      <td>82</td>\n",
       "      <td>71</td>\n",
       "      <td>0.026638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.645563</td>\n",
       "      <td>0.144958</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.006924</td>\n",
       "      <td>-0.258297</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.645563</td>\n",
       "      <td>0.144958</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABT</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-10-17 17:53:08</td>\n",
       "      <td>ABT_3153</td>\n",
       "      <td>4412</td>\n",
       "      <td>126</td>\n",
       "      <td>73</td>\n",
       "      <td>51</td>\n",
       "      <td>0.028558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>0.800778</td>\n",
       "      <td>0.334748</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006401</td>\n",
       "      <td>0.219363</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>0.800778</td>\n",
       "      <td>0.334748</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>2008-01-23 21:45:06</td>\n",
       "      <td>ABT_4880</td>\n",
       "      <td>4882</td>\n",
       "      <td>167</td>\n",
       "      <td>75</td>\n",
       "      <td>74</td>\n",
       "      <td>0.034207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.679371</td>\n",
       "      <td>0.366176</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.048132</td>\n",
       "      <td>-2.812429</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.679371</td>\n",
       "      <td>0.366176</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABT</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008-04-16 18:24:19</td>\n",
       "      <td>ABT_7370</td>\n",
       "      <td>4680</td>\n",
       "      <td>131</td>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>0.027991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.500453</td>\n",
       "      <td>0.247415</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.043338</td>\n",
       "      <td>-3.197722</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.500453</td>\n",
       "      <td>0.247415</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008-07-17 00:49:23</td>\n",
       "      <td>ABT_10085</td>\n",
       "      <td>5096</td>\n",
       "      <td>184</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.421022</td>\n",
       "      <td>0.158410</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>1.386714</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.421022</td>\n",
       "      <td>0.158410</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343</th>\n",
       "      <td>XOM</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-05-01 20:00:40</td>\n",
       "      <td>XOM_167742</td>\n",
       "      <td>1616</td>\n",
       "      <td>62</td>\n",
       "      <td>59</td>\n",
       "      <td>22</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4344</th>\n",
       "      <td>XOM</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-05-27 21:59:15</td>\n",
       "      <td>XOM_170399</td>\n",
       "      <td>5579</td>\n",
       "      <td>168</td>\n",
       "      <td>192</td>\n",
       "      <td>68</td>\n",
       "      <td>0.030113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002662</td>\n",
       "      <td>1.050242</td>\n",
       "      <td>0.709236</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.018166</td>\n",
       "      <td>-0.779919</td>\n",
       "      <td>-0.002662</td>\n",
       "      <td>1.050242</td>\n",
       "      <td>0.709236</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>XOM</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07-31 17:59:08</td>\n",
       "      <td>XOM_172110</td>\n",
       "      <td>5973</td>\n",
       "      <td>216</td>\n",
       "      <td>200</td>\n",
       "      <td>88</td>\n",
       "      <td>0.036163</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>XOM</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-10-30 21:24:08</td>\n",
       "      <td>XOM_176630</td>\n",
       "      <td>5304</td>\n",
       "      <td>181</td>\n",
       "      <td>156</td>\n",
       "      <td>61</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>XOM</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>2021-02-03 02:56:06</td>\n",
       "      <td>XOM_181141</td>\n",
       "      <td>5984</td>\n",
       "      <td>289</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.048295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002057</td>\n",
       "      <td>1.323466</td>\n",
       "      <td>0.307129</td>\n",
       "      <td>2</td>\n",
       "      <td>0.073329</td>\n",
       "      <td>2.518393</td>\n",
       "      <td>-0.002057</td>\n",
       "      <td>1.323466</td>\n",
       "      <td>0.307129</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4348 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker  quarter  fiscal_year          event_date transcript_id  \\\n",
       "0       ABT        2         2007 2007-07-18 19:16:50      ABT_2330   \n",
       "1       ABT        3         2007 2007-10-17 17:53:08      ABT_3153   \n",
       "2       ABT        4         2007 2008-01-23 21:45:06      ABT_4880   \n",
       "3       ABT        1         2008 2008-04-16 18:24:19      ABT_7370   \n",
       "4       ABT        2         2008 2008-07-17 00:49:23     ABT_10085   \n",
       "...     ...      ...          ...                 ...           ...   \n",
       "4343    XOM        1         2020 2020-05-01 20:00:40    XOM_167742   \n",
       "4344    XOM        0         2020 2020-05-27 21:59:15    XOM_170399   \n",
       "4345    XOM        2         2020 2020-07-31 17:59:08    XOM_172110   \n",
       "4346    XOM        3         2020 2020-10-30 21:24:08    XOM_176630   \n",
       "4347    XOM        4         2020 2021-02-03 02:56:06    XOM_181141   \n",
       "\n",
       "      total_words  positive_count  negative_count  uncertainty_count  \\\n",
       "0            4580             122              82                 71   \n",
       "1            4412             126              73                 51   \n",
       "2            4882             167              75                 74   \n",
       "3            4680             131              76                 70   \n",
       "4            5096             184              70                 70   \n",
       "...           ...             ...             ...                ...   \n",
       "4343         1616              62              59                 22   \n",
       "4344         5579             168             192                 68   \n",
       "4345         5973             216             200                 88   \n",
       "4346         5304             181             156                 61   \n",
       "4347         5984             289             129                 61   \n",
       "\n",
       "      positive_ratio  ...  car_0_1_alpha  car_0_1_beta  car_0_1_rsquared  \\\n",
       "0           0.026638  ...       0.001000      0.645563          0.144958   \n",
       "1           0.028558  ...      -0.000069      0.800778          0.334748   \n",
       "2           0.034207  ...      -0.000030      0.679371          0.366176   \n",
       "3           0.027991  ...       0.000152      0.500453          0.247415   \n",
       "4           0.036107  ...       0.000488      0.421022          0.158410   \n",
       "...              ...  ...            ...           ...               ...   \n",
       "4343        0.038366  ...            NaN           NaN               NaN   \n",
       "4344        0.030113  ...      -0.002662      1.050242          0.709236   \n",
       "4345        0.036163  ...            NaN           NaN               NaN   \n",
       "4346        0.034125  ...            NaN           NaN               NaN   \n",
       "4347        0.048295  ...      -0.002057      1.323466          0.307129   \n",
       "\n",
       "      car_0_1_obs   car_0_2  car_0_2_tstat  car_0_2_alpha  car_0_2_beta  \\\n",
       "0               2 -0.006924      -0.258297       0.001000      0.645563   \n",
       "1               2  0.006401       0.219363      -0.000069      0.800778   \n",
       "2               2 -0.048132      -2.812429      -0.000030      0.679371   \n",
       "3               2 -0.043338      -3.197722       0.000152      0.500453   \n",
       "4               2  0.005516       1.386714       0.000488      0.421022   \n",
       "...           ...       ...            ...            ...           ...   \n",
       "4343            0       NaN            NaN            NaN           NaN   \n",
       "4344            2 -0.018166      -0.779919      -0.002662      1.050242   \n",
       "4345            0       NaN            NaN            NaN           NaN   \n",
       "4346            0       NaN            NaN            NaN           NaN   \n",
       "4347            2  0.073329       2.518393      -0.002057      1.323466   \n",
       "\n",
       "      car_0_2_rsquared  car_0_2_obs  \n",
       "0             0.144958            3  \n",
       "1             0.334748            3  \n",
       "2             0.366176            3  \n",
       "3             0.247415            3  \n",
       "4             0.158410            2  \n",
       "...                ...          ...  \n",
       "4343               NaN            0  \n",
       "4344          0.709236            3  \n",
       "4345               NaN            0  \n",
       "4346               NaN            0  \n",
       "4347          0.307129            3  \n",
       "\n",
       "[4348 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "OUT_TAB = \"exports/tables\"\n",
    "\n",
    "def step4_merge_data():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 4: MERGING CAR AND SENTIMENT DATA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Define CAR files to merge\n",
    "    car_files = {\n",
    "        'car_-1_1': \"CAR_-1_1_Results.csv\",\n",
    "        'car_0_1': \"CAR_0_1_Results.csv\",\n",
    "        'car_0_2': \"CAR_0_2_Results.csv\"\n",
    "    }\n",
    "    \n",
    "    # Load sentiment data\n",
    "    sentiment_path = os.path.join(OUT_TAB, \"Sentiment_Analysis_Results.csv\")\n",
    "    if not os.path.exists(sentiment_path):\n",
    "        print(\" Sentiment data not found.\")\n",
    "        print(\"Please run 'python step3_sentiment_analysis.py' first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Loading sentiment data...\")\n",
    "    sentiment_df = pd.read_csv(sentiment_path)\n",
    "    sentiment_df['event_date'] = pd.to_datetime(sentiment_df['event_date'])\n",
    "    sentiment_df['event_date_only'] = sentiment_df['event_date'].dt.date\n",
    "    print(f\" Sentiment data: {len(sentiment_df)} records\")\n",
    "    \n",
    "    # Remove duplicates from sentiment data\n",
    "    sentiment_dups = sentiment_df.duplicated(subset=['ticker', 'event_date_only']).sum()\n",
    "    if sentiment_dups > 0:\n",
    "        print(f\"  Found {sentiment_dups} sentiment duplicates - removing...\")\n",
    "        sentiment_df = sentiment_df.drop_duplicates(subset=['ticker', 'event_date_only'], keep='first')\n",
    "    \n",
    "    # Load and merge each CAR file\n",
    "    car_data_dict = {}\n",
    "    for car_name, car_file in car_files.items():\n",
    "        car_path = os.path.join(\"CAR_DATA\", car_file)\n",
    "        \n",
    "        if not os.path.exists(car_path):\n",
    "            print(f\"  {car_file} not found - skipping this CAR window\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nLoading {car_file}...\")\n",
    "        car_df = pd.read_csv(car_path)\n",
    "        car_df['event_date'] = pd.to_datetime(car_df['event_date'])\n",
    "        car_df['event_date_only'] = car_df['event_date'].dt.date\n",
    "        print(f\" {car_name.upper()}: {len(car_df)} records\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        car_dups = car_df.duplicated(subset=['ticker', 'event_date_only']).sum()\n",
    "        if car_dups > 0:\n",
    "            print(f\"  Found {car_dups} duplicates - removing...\")\n",
    "            car_df = car_df.drop_duplicates(subset=['ticker', 'event_date_only'], keep='first')\n",
    "        \n",
    "        # Select relevant columns and rename with CAR window suffix\n",
    "        car_columns = ['ticker', 'event_date_only', 'car', 'car_tstat', \n",
    "                      'alpha', 'beta', 'r_squared', 'observations']\n",
    "        car_df_subset = car_df[car_columns].copy()\n",
    "        \n",
    "        # Rename columns to include CAR window\n",
    "        rename_dict = {\n",
    "            'car': car_name,\n",
    "            'car_tstat': f'{car_name}_tstat',\n",
    "            'alpha': f'{car_name}_alpha',\n",
    "            'beta': f'{car_name}_beta',\n",
    "            'r_squared': f'{car_name}_rsquared',\n",
    "            'observations': f'{car_name}_obs'\n",
    "        }\n",
    "        car_df_subset = car_df_subset.rename(columns=rename_dict)\n",
    "        \n",
    "        car_data_dict[car_name] = car_df_subset\n",
    "    \n",
    "    if not car_data_dict:\n",
    "        print(\" No CAR data files found!\")\n",
    "        print(\"Please run WRDS analysis first to generate CAR files.\")\n",
    "        return\n",
    "    \n",
    "    # Start with sentiment data\n",
    "    merged_df = sentiment_df.copy()\n",
    "    \n",
    "    # Merge each CAR dataset\n",
    "    for car_name, car_df in car_data_dict.items():\n",
    "        print(f\"\\nMerging {car_name.upper()}...\")\n",
    "        \n",
    "        merged_df = pd.merge(\n",
    "            merged_df,\n",
    "            car_df,\n",
    "            on=['ticker', 'event_date_only'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        matched = merged_df[car_name].notna().sum()\n",
    "        print(f\" Matched {matched} events ({matched/len(merged_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Remove rows where ALL CAR values are missing\n",
    "    car_cols = [col for col in merged_df.columns if col.startswith('car_')]\n",
    "    merged_df = merged_df.dropna(subset=car_cols, how='all')\n",
    "    \n",
    "    print(f\"\\n Final merged dataset: {len(merged_df)} records\")\n",
    "    print(f\"   Companies: {merged_df['ticker'].nunique()}\")\n",
    "    \n",
    "    # Save merged dataset\n",
    "    merged_path = os.path.join(OUT_TAB, \"CAR_Sentiment_Merged.csv\")\n",
    "    merged_df.to_csv(merged_path, index=False)\n",
    "    \n",
    "    # Save Excel version with multiple sheets\n",
    "    excel_path = os.path.join(OUT_TAB, \"CAR_Sentiment_Analysis.xlsx\")\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Main dataset\n",
    "        merged_df.to_excel(writer, sheet_name='Merged_Data', index=False)\n",
    "        \n",
    "        # Summary statistics\n",
    "        numeric_cols = merged_df.select_dtypes(include=['number']).columns\n",
    "        summary_stats = merged_df[numeric_cols].describe()\n",
    "        summary_stats.to_excel(writer, sheet_name='Summary_Statistics')\n",
    "        \n",
    "        # CAR comparison by company\n",
    "        car_comparison_cols = ['ticker'] + [col for col in merged_df.columns if col.startswith('car_') and not any(x in col for x in ['tstat', 'alpha', 'beta', 'rsquared', 'obs'])]\n",
    "        if len(car_comparison_cols) > 1:\n",
    "            car_by_company = merged_df.groupby('ticker')[car_comparison_cols[1:]].agg(['mean', 'std', 'count']).round(4)\n",
    "            car_by_company.to_excel(writer, sheet_name='CAR_by_Company')\n",
    "        \n",
    "        # Sentiment summary\n",
    "        key_sentiment_cols = [col for col in merged_df.columns if 'sentiment' in col.lower() and 'ratio' in col.lower()]\n",
    "        if key_sentiment_cols:\n",
    "            sentiment_summary = merged_df.groupby('ticker')[key_sentiment_cols].mean().round(4)\n",
    "            sentiment_summary.to_excel(writer, sheet_name='Sentiment_by_Company')\n",
    "    \n",
    "    print(\"\\n Files saved:\")\n",
    "    print(f\"   - {merged_path}\")\n",
    "    print(f\"   - {excel_path}\")\n",
    "    \n",
    "    # Display correlations for each CAR window\n",
    "    print(\"\\n CORRELATIONS WITH SENTIMENT:\")\n",
    "    sentiment_cols = ['negative_ratio', 'positive_ratio', 'uncertainty_ratio']\n",
    "    \n",
    "    for car_col in [col for col in merged_df.columns if col.startswith('car_') and col.count('_') == 2]:\n",
    "        print(f\"\\n{car_col.upper()}:\")\n",
    "        for sent_col in sentiment_cols:\n",
    "            if sent_col in merged_df.columns and car_col in merged_df.columns:\n",
    "                corr = merged_df[[car_col, sent_col]].corr().iloc[0,1]\n",
    "                print(f\"  {sent_col}: {corr:.4f}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "step4_merge_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fda5db",
   "metadata": {},
   "source": [
    "# Step 5: Collect Market Cap and Volume Data\n",
    "\n",
    "**Input Files:**\n",
    "- `exports/tables/CAR_Sentiment_Merged.csv` (from Step 4)\n",
    "- Yahoo Finance API (online data source)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/tables/Market_Cap_Daily_Data.csv`\n",
    "- `exports/tables/Market_Cap_Monthly_Lookup.csv`\n",
    "- `exports/tables/Market_Cap_Complete_Data.xlsx`\n",
    "- `exports/tables/loading_summary.csv`\n",
    "This script collects real market capitalization data from Yahoo Finance\n",
    "for all companies in our dataset and saves it for use in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a2b6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MARKET CAPITALIZATION DATA COLLECTION\n",
      "================================================================================\n",
      "================================================================================\n",
      "COLLECTING MARKET CAPITALIZATION DATA FROM YAHOO FINANCE\n",
      "================================================================================\n",
      " Found 60 companies in dataset:\n",
      "Companies: ABT, ADBE, AMGN, AMT, ASML, AXP, AZN, BA, BAC, BKNG, BLK, BMY, CMCSA, COST, CRM, DUK, EBAY, F, FB, FDX, GM, GOOG, GS, HD, HON, IBM, INTC, INTU, ISRG, JNJ, JPM, LLY, LMT, MA, MCD, MCO, MDT, MET, MS, MSFT, NFLX, NKE, ORCL, PEP, PG, PM, QCOM, SAP, SNY, T, TD, TMO, TMUS, TSLA, TSM, UNH, UNP, UPS, V, XOM\n",
      " Date range: 2004-10-31 to 2022-03-12\n",
      "\n",
      "[1/60] Processing ABT...\n",
      "Collecting data for ABT...\n",
      "  Collected 4371 records for ABT\n",
      "\n",
      "[2/60] Processing ADBE...\n",
      "Collecting data for ADBE...\n",
      "  Collected 4371 records for ADBE\n",
      "\n",
      "[3/60] Processing AMGN...\n",
      "Collecting data for AMGN...\n",
      "  Collected 4371 records for AMGN\n",
      "\n",
      "[4/60] Processing AMT...\n",
      "Collecting data for AMT...\n",
      "  Collected 4371 records for AMT\n",
      "\n",
      "[5/60] Processing ASML...\n",
      "Collecting data for ASML...\n",
      "  Collected 4371 records for ASML\n",
      "\n",
      "[6/60] Processing AXP...\n",
      "Collecting data for AXP...\n",
      "  Collected 4371 records for AXP\n",
      "\n",
      "[7/60] Processing AZN...\n",
      "Collecting data for AZN...\n",
      "  Collected 4371 records for AZN\n",
      "\n",
      "[8/60] Processing BA...\n",
      "Collecting data for BA...\n",
      "  Collected 4371 records for BA\n",
      "\n",
      "[9/60] Processing BAC...\n",
      "Collecting data for BAC...\n",
      "  Collected 4371 records for BAC\n",
      "\n",
      "[10/60] Processing BKNG...\n",
      "Collecting data for BKNG...\n",
      "  Collected 4371 records for BKNG\n",
      "\n",
      " Progress: 10/60 companies completed\n",
      "\n",
      "[11/60] Processing BLK...\n",
      "Collecting data for BLK...\n",
      "  Collected 4371 records for BLK\n",
      "\n",
      "[12/60] Processing BMY...\n",
      "Collecting data for BMY...\n",
      "  Collected 4371 records for BMY\n",
      "\n",
      "[13/60] Processing CMCSA...\n",
      "Collecting data for CMCSA...\n",
      "  Collected 4371 records for CMCSA\n",
      "\n",
      "[14/60] Processing COST...\n",
      "Collecting data for COST...\n",
      "  Collected 4371 records for COST\n",
      "\n",
      "[15/60] Processing CRM...\n",
      "Collecting data for CRM...\n",
      "  Collected 4371 records for CRM\n",
      "\n",
      "[16/60] Processing DUK...\n",
      "Collecting data for DUK...\n",
      "  Collected 4371 records for DUK\n",
      "\n",
      "[17/60] Processing EBAY...\n",
      "Collecting data for EBAY...\n",
      "  Collected 4371 records for EBAY\n",
      "\n",
      "[18/60] Processing F...\n",
      "Collecting data for F...\n",
      "  Collected 4371 records for F\n",
      "\n",
      "[19/60] Processing FB...\n",
      "Collecting data for FB...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$FB: possibly delisted; no price data found  (1d 2004-10-31 00:40:28 -> 2022-03-12 01:38:07) (Yahoo error = \"Data doesn't exist for startDate = 1099197628, endDate = 1647067087\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No historical data found for FB\n",
      "\n",
      "[20/60] Processing FDX...\n",
      "Collecting data for FDX...\n",
      "  Collected 4371 records for FDX\n",
      "\n",
      " Progress: 20/60 companies completed\n",
      "\n",
      "[21/60] Processing GM...\n",
      "Collecting data for GM...\n",
      "  Collected 2847 records for GM\n",
      "\n",
      "[22/60] Processing GOOG...\n",
      "Collecting data for GOOG...\n",
      "  Collected 4371 records for GOOG\n",
      "\n",
      "[23/60] Processing GS...\n",
      "Collecting data for GS...\n",
      "  Collected 4371 records for GS\n",
      "\n",
      "[24/60] Processing HD...\n",
      "Collecting data for HD...\n",
      "  Collected 4371 records for HD\n",
      "\n",
      "[25/60] Processing HON...\n",
      "Collecting data for HON...\n",
      "  Collected 4371 records for HON\n",
      "\n",
      "[26/60] Processing IBM...\n",
      "Collecting data for IBM...\n",
      "  Collected 4371 records for IBM\n",
      "\n",
      "[27/60] Processing INTC...\n",
      "Collecting data for INTC...\n",
      "  Collected 4371 records for INTC\n",
      "\n",
      "[28/60] Processing INTU...\n",
      "Collecting data for INTU...\n",
      "  Collected 4371 records for INTU\n",
      "\n",
      "[29/60] Processing ISRG...\n",
      "Collecting data for ISRG...\n",
      "  Collected 4371 records for ISRG\n",
      "\n",
      "[30/60] Processing JNJ...\n",
      "Collecting data for JNJ...\n",
      "  Collected 4371 records for JNJ\n",
      "\n",
      " Progress: 30/60 companies completed\n",
      "\n",
      "[31/60] Processing JPM...\n",
      "Collecting data for JPM...\n",
      "  Collected 4371 records for JPM\n",
      "\n",
      "[32/60] Processing LLY...\n",
      "Collecting data for LLY...\n",
      "  Collected 4371 records for LLY\n",
      "\n",
      "[33/60] Processing LMT...\n",
      "Collecting data for LMT...\n",
      "  Collected 4371 records for LMT\n",
      "\n",
      "[34/60] Processing MA...\n",
      "Collecting data for MA...\n",
      "  Collected 3977 records for MA\n",
      "\n",
      "[35/60] Processing MCD...\n",
      "Collecting data for MCD...\n",
      "  Collected 4371 records for MCD\n",
      "\n",
      "[36/60] Processing MCO...\n",
      "Collecting data for MCO...\n",
      "  Collected 4371 records for MCO\n",
      "\n",
      "[37/60] Processing MDT...\n",
      "Collecting data for MDT...\n",
      "  Collected 4371 records for MDT\n",
      "\n",
      "[38/60] Processing MET...\n",
      "Collecting data for MET...\n",
      "  Collected 4371 records for MET\n",
      "\n",
      "[39/60] Processing MS...\n",
      "Collecting data for MS...\n",
      "  Collected 4371 records for MS\n",
      "\n",
      "[40/60] Processing MSFT...\n",
      "Collecting data for MSFT...\n",
      "  Collected 4371 records for MSFT\n",
      "\n",
      " Progress: 40/60 companies completed\n",
      "\n",
      "[41/60] Processing NFLX...\n",
      "Collecting data for NFLX...\n",
      "  Collected 4371 records for NFLX\n",
      "\n",
      "[42/60] Processing NKE...\n",
      "Collecting data for NKE...\n",
      "  Collected 4371 records for NKE\n",
      "\n",
      "[43/60] Processing ORCL...\n",
      "Collecting data for ORCL...\n",
      "  Collected 4371 records for ORCL\n",
      "\n",
      "[44/60] Processing PEP...\n",
      "Collecting data for PEP...\n",
      "  Collected 4371 records for PEP\n",
      "\n",
      "[45/60] Processing PG...\n",
      "Collecting data for PG...\n",
      "  Collected 4371 records for PG\n",
      "\n",
      "[46/60] Processing PM...\n",
      "Collecting data for PM...\n",
      "  Collected 3523 records for PM\n",
      "\n",
      "[47/60] Processing QCOM...\n",
      "Collecting data for QCOM...\n",
      "  Collected 4371 records for QCOM\n",
      "\n",
      "[48/60] Processing SAP...\n",
      "Collecting data for SAP...\n",
      "  Collected 4371 records for SAP\n",
      "\n",
      "[49/60] Processing SNY...\n",
      "Collecting data for SNY...\n",
      "  Collected 4371 records for SNY\n",
      "\n",
      "[50/60] Processing T...\n",
      "Collecting data for T...\n",
      "  Collected 4371 records for T\n",
      "\n",
      " Progress: 50/60 companies completed\n",
      "\n",
      "[51/60] Processing TD...\n",
      "Collecting data for TD...\n",
      "  Collected 4371 records for TD\n",
      "\n",
      "[52/60] Processing TMO...\n",
      "Collecting data for TMO...\n",
      "  Collected 4371 records for TMO\n",
      "\n",
      "[53/60] Processing TMUS...\n",
      "Collecting data for TMUS...\n",
      "  Collected 3752 records for TMUS\n",
      "\n",
      "[54/60] Processing TSLA...\n",
      "Collecting data for TSLA...\n",
      "  Collected 2947 records for TSLA\n",
      "\n",
      "[55/60] Processing TSM...\n",
      "Collecting data for TSM...\n",
      "  Collected 4371 records for TSM\n",
      "\n",
      "[56/60] Processing UNH...\n",
      "Collecting data for UNH...\n",
      "  Collected 4371 records for UNH\n",
      "\n",
      "[57/60] Processing UNP...\n",
      "Collecting data for UNP...\n",
      "  Collected 4371 records for UNP\n",
      "\n",
      "[58/60] Processing UPS...\n",
      "Collecting data for UPS...\n",
      "  Collected 4371 records for UPS\n",
      "\n",
      "[59/60] Processing V...\n",
      "Collecting data for V...\n",
      "  Collected 3521 records for V\n",
      "\n",
      "[60/60] Processing XOM...\n",
      "Collecting data for XOM...\n",
      "  Collected 4371 records for XOM\n",
      "\n",
      " Progress: 60/60 companies completed\n",
      "\n",
      " Successfully collected data for 59 companies\n",
      "Total records: 252230\n",
      "\n",
      "Cleaning and validating market cap data...\n",
      " Data cleaned: 252230 → 252230 records\n",
      " Companies with data: 59\n",
      " Date range: 2004-11-01 00:00:00 to 2022-03-11 00:00:00\n",
      "\n",
      "Creating market cap lookup table...\n",
      " Created monthly lookup table: 12065 records\n",
      "\n",
      " Market cap data saved:\n",
      "  - exports/tables\\Market_Cap_Daily_Data.csv\n",
      "  - exports/tables\\Market_Cap_Monthly_Lookup.csv\n",
      "  - exports/tables\\Market_Cap_Complete_Data.xlsx\n",
      "\n",
      "================================================================================\n",
      " MARKET CAP DATA COLLECTION COMPLETED!\n",
      "================================================================================\n",
      "Use this file for merging: exports/tables\\Market_Cap_Monthly_Lookup.csv\n",
      "Ready to update the control variables script!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = \"exports/tables\"\n",
    "\n",
    "def get_companies_from_dataset(output_dir: str = OUTPUT_DIR) -> list:\n",
    "    \"\"\"Get list of companies from the merged dataset\"\"\"\n",
    "    merged_path = os.path.join(output_dir, \"CAR_Sentiment_Merged.csv\")\n",
    "    \n",
    "    if not os.path.exists(merged_path):\n",
    "        raise FileNotFoundError(f\"Merged dataset not found: {merged_path}\")\n",
    "    \n",
    "    df = pd.read_csv(merged_path)\n",
    "    companies = sorted(df['ticker'].unique().tolist())\n",
    "    \n",
    "    print(f\" Found {len(companies)} companies in dataset:\")\n",
    "    print(f\"Companies: {', '.join(companies)}\")\n",
    "    \n",
    "    return companies\n",
    "\n",
    "def get_date_range_from_dataset(output_dir: str = OUTPUT_DIR) -> tuple:\n",
    "    \"\"\"Get the date range from our dataset\"\"\"\n",
    "    merged_path = os.path.join(output_dir, \"CAR_Sentiment_Merged.csv\")\n",
    "    df = pd.read_csv(merged_path)\n",
    "    df['event_date'] = pd.to_datetime(df['event_date'])\n",
    "    \n",
    "    start_date = df['event_date'].min()\n",
    "    end_date = df['event_date'].max()\n",
    "    \n",
    "    # Extend range slightly to ensure we have data\n",
    "    start_date = start_date - timedelta(days=365)\n",
    "    end_date = end_date + timedelta(days=365)\n",
    "    \n",
    "    print(f\" Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    return start_date, end_date\n",
    "\n",
    "def collect_single_company_data(ticker: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"Collect market cap data for a single company\"\"\"\n",
    "    try:\n",
    "        print(f\"Collecting data for {ticker}...\")\n",
    "        \n",
    "        # Create ticker object\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        # Get historical data (daily)\n",
    "        hist_data = stock.history(start=start_date, end=end_date, interval='1d')\n",
    "        \n",
    "        if hist_data.empty:\n",
    "            print(f\" No historical data found for {ticker}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Get current info for shares outstanding\n",
    "        try:\n",
    "            info = stock.info\n",
    "            shares_outstanding = info.get('sharesOutstanding', None)\n",
    "            \n",
    "            # If shares outstanding not available, try other methods\n",
    "            if shares_outstanding is None:\n",
    "                shares_outstanding = info.get('impliedSharesOutstanding', None)\n",
    "            if shares_outstanding is None:\n",
    "                shares_outstanding = info.get('floatShares', None)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Could not get shares outstanding for {ticker}: {e}\")\n",
    "            shares_outstanding = None\n",
    "        \n",
    "        # Calculate market cap\n",
    "        if shares_outstanding is not None:\n",
    "            hist_data['market_cap'] = hist_data['Close'] * shares_outstanding\n",
    "        else:\n",
    "            # Estimate shares outstanding from recent market cap if available\n",
    "            try:\n",
    "                recent_market_cap = info.get('marketCap', None)\n",
    "                if recent_market_cap is not None and not hist_data.empty:\n",
    "                    # Use most recent close price to estimate shares\n",
    "                    recent_price = hist_data['Close'].iloc[-1]\n",
    "                    estimated_shares = recent_market_cap / recent_price\n",
    "                    hist_data['market_cap'] = hist_data['Close'] * estimated_shares\n",
    "                    print(f\"   Estimated shares outstanding for {ticker}: {estimated_shares:,.0f}\")\n",
    "                else:\n",
    "                    hist_data['market_cap'] = np.nan\n",
    "            except:\n",
    "                hist_data['market_cap'] = np.nan\n",
    "        \n",
    "        # Add ticker column\n",
    "        hist_data['ticker'] = ticker\n",
    "        hist_data['date'] = hist_data.index.date\n",
    "        \n",
    "        # Keep only relevant columns (including Volume for turnover analysis)\n",
    "        result = hist_data[['ticker', 'date', 'Close', 'Volume', 'market_cap']].copy()\n",
    "        result.columns = ['ticker', 'date', 'stock_price', 'volume', 'market_cap']\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        print(f\"  Collected {len(result)} records for {ticker}\")\n",
    "        \n",
    "        # Add small delay to be respectful to Yahoo Finance\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error collecting data for {ticker}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def collect_all_market_cap_data() -> pd.DataFrame:\n",
    "    \"\"\"Collect market cap data for all companies\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COLLECTING MARKET CAPITALIZATION DATA FROM YAHOO FINANCE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get companies and date range\n",
    "    companies = get_companies_from_dataset()\n",
    "    start_date, end_date = get_date_range_from_dataset()\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Collect data for each company\n",
    "    for i, ticker in enumerate(companies, 1):\n",
    "        print(f\"\\n[{i}/{len(companies)}] Processing {ticker}...\")\n",
    "        \n",
    "        company_data = collect_single_company_data(ticker, start_date, end_date)\n",
    "        \n",
    "        if not company_data.empty:\n",
    "            all_data.append(company_data)\n",
    "        \n",
    "        # Progress update\n",
    "        if i % 10 == 0:\n",
    "            print(f\"\\n Progress: {i}/{len(companies)} companies completed\")\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        combined_data = pd.concat(all_data, ignore_index=True)\n",
    "        combined_data['date'] = pd.to_datetime(combined_data['date'])\n",
    "        \n",
    "        print(f\"\\n Successfully collected data for {len(all_data)} companies\")\n",
    "        print(f\"Total records: {len(combined_data)}\")\n",
    "        \n",
    "        return combined_data\n",
    "    else:\n",
    "        print(\" No data collected\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def clean_and_validate_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean and validate the collected market cap data\"\"\"\n",
    "    print(\"\\nCleaning and validating market cap data...\")\n",
    "    \n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Remove rows with missing market cap\n",
    "    df = df.dropna(subset=['market_cap'])\n",
    "    \n",
    "    # Remove negative or zero market caps\n",
    "    df = df[df['market_cap'] > 0]\n",
    "    \n",
    "    # Remove extreme outliers (market cap > $10 trillion - likely data errors)\n",
    "    df = df[df['market_cap'] < 10e12]\n",
    "    \n",
    "    # Sort by ticker and date\n",
    "    df = df.sort_values(['ticker', 'date'])\n",
    "    \n",
    "    print(f\" Data cleaned: {original_count} → {len(df)} records\")\n",
    "    print(f\" Companies with data: {df['ticker'].nunique()}\")\n",
    "    print(f\" Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_market_cap_lookup(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create a lookup table for market cap by ticker and date\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    print(\"\\nCreating market cap lookup table...\")\n",
    "    \n",
    "    # Create monthly averages to reduce data size\n",
    "    df['year_month'] = df['date'].dt.to_period('M')\n",
    "    \n",
    "    monthly_data = df.groupby(['ticker', 'year_month']).agg({\n",
    "        'market_cap': 'mean',\n",
    "        'stock_price': 'mean',\n",
    "        'date': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Convert back to datetime\n",
    "    monthly_data['date'] = monthly_data['year_month'].dt.start_time\n",
    "    monthly_data = monthly_data.drop('year_month', axis=1)\n",
    "    \n",
    "    # Add log market cap\n",
    "    monthly_data['log_market_cap'] = np.log(monthly_data['market_cap'])\n",
    "    \n",
    "    print(f\" Created monthly lookup table: {len(monthly_data)} records\")\n",
    "    \n",
    "    return monthly_data\n",
    "\n",
    "def save_market_cap_data(df: pd.DataFrame, lookup_df: pd.DataFrame, output_dir: str = OUTPUT_DIR) -> str:\n",
    "    \"\"\"Save the market cap data\"\"\"\n",
    "    \n",
    "    # Save daily data\n",
    "    daily_path = os.path.join(output_dir, \"Market_Cap_Daily_Data.csv\")\n",
    "    df.to_csv(daily_path, index=False)\n",
    "    \n",
    "    # Save monthly lookup\n",
    "    monthly_path = os.path.join(output_dir, \"Market_Cap_Monthly_Lookup.csv\")\n",
    "    lookup_df.to_csv(monthly_path, index=False)\n",
    "    \n",
    "    # Save Excel with summary\n",
    "    excel_path = os.path.join(output_dir, \"Market_Cap_Complete_Data.xlsx\")\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Monthly lookup (main table for merging)\n",
    "        lookup_df.to_excel(writer, sheet_name='Monthly_Lookup', index=False)\n",
    "        \n",
    "        # Daily data (complete)\n",
    "        if len(df) < 50000:  # Only if not too large\n",
    "            df.to_excel(writer, sheet_name='Daily_Data', index=False)\n",
    "        \n",
    "        # Summary statistics\n",
    "        summary_stats = lookup_df.groupby('ticker').agg({\n",
    "            'market_cap': ['mean', 'std', 'min', 'max', 'count'],\n",
    "            'log_market_cap': ['mean', 'std'],\n",
    "            'date': ['min', 'max']\n",
    "        }).round(2)\n",
    "        summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns]\n",
    "        summary_stats.to_excel(writer, sheet_name='Summary_by_Company')\n",
    "        \n",
    "        # Overall summary\n",
    "        overall_summary = pd.DataFrame({\n",
    "            'Metric': ['Total Companies', 'Total Records', 'Date Range Start', 'Date Range End',\n",
    "                      'Mean Market Cap ($B)', 'Median Market Cap ($B)', 'Max Market Cap ($B)'],\n",
    "            'Value': [lookup_df['ticker'].nunique(), len(lookup_df), \n",
    "                     lookup_df['date'].min(), lookup_df['date'].max(),\n",
    "                     f\"{lookup_df['market_cap'].mean()/1e9:.1f}\", \n",
    "                     f\"{lookup_df['market_cap'].median()/1e9:.1f}\",\n",
    "                     f\"{lookup_df['market_cap'].max()/1e9:.1f}\"]\n",
    "        })\n",
    "        overall_summary.to_excel(writer, sheet_name='Overall_Summary', index=False)\n",
    "    \n",
    "    print(f\"\\n Market cap data saved:\")\n",
    "    print(f\"  - {daily_path}\")\n",
    "    print(f\"  - {monthly_path}\")\n",
    "    print(f\"  - {excel_path}\")\n",
    "    \n",
    "    return monthly_path\n",
    "\n",
    "def step5_collect_market_cap_data():\n",
    "    \"\"\"Main function to collect market cap data\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"MARKET CAPITALIZATION DATA COLLECTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Collect all market cap data\n",
    "        market_cap_data = collect_all_market_cap_data()\n",
    "        \n",
    "        if not market_cap_data.empty:\n",
    "            # Clean and validate\n",
    "            clean_data = clean_and_validate_data(market_cap_data)\n",
    "            \n",
    "            # Create lookup table\n",
    "            lookup_data = create_market_cap_lookup(clean_data)\n",
    "            \n",
    "            # Save data\n",
    "            lookup_path = save_market_cap_data(clean_data, lookup_data)\n",
    "            \n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\" MARKET CAP DATA COLLECTION COMPLETED!\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"Use this file for merging: {lookup_path}\")\n",
    "            print(\"Ready to update the control variables script!\")\n",
    "            \n",
    "        else:\n",
    "            print(\" No market cap data collected\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "step5_collect_market_cap_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827e677",
   "metadata": {},
   "source": [
    "# Step 6: Add Control Variables\n",
    "\n",
    "**Input Files:**\n",
    "- `exports/tables/CAR_Sentiment_Merged.csv` (from Step 4)\n",
    "- `exports/tables/Market_Cap_Daily_Data.csv` (from Step 5)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/tables/Final_Regression_Dataset.csv`\n",
    "\n",
    "**Control Variables Added:**\n",
    "- Firm size (log market cap)\n",
    "- Volatility (price volatility, systematic risk)\n",
    "- Temporal (year, quarter, covid period)\n",
    "- Performance (previous CAR)\n",
    "- Sentiment complexity (word count, ratios)\n",
    "- Stock price momentum (1 week, 2 week, 1 month)\n",
    "- Abnormal turnover (volume analysis)\n",
    "- Industry dummies\n",
    "\n",
    "Updated Control Variables Module - Using Real Market Cap Data\n",
    "This module adds control variables using REAL market capitalization data\n",
    "collected from Yahoo Finance instead of estimates.\n",
    "Supports multiple CAR windows: CAR(-1,1), CAR(0,1), CAR(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a04542c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADDING CONTROL VARIABLES (WITH REAL MARKET CAP DATA)\n",
      "================================================================================\n",
      " Using CAR(-1,1) as primary CAR measure\n",
      " Loaded merged dataset: 4348 records\n",
      " Available CAR windows: CAR_-1_1, CAR_0_1, CAR_0_2\n",
      "Adding firm size control variable (REAL Market Capitalization from Yahoo Finance)...\n",
      " Loaded real market cap data: 12065 records for 59 companies\n",
      "   Date range: 2004-11-01 00:00:00 to 2022-03-01 00:00:00\n",
      "Matching market cap data to earnings events...\n",
      "  Processed 500/4348 records...\n",
      "  Processed 1000/4348 records...\n",
      "  Processed 1500/4348 records...\n",
      "  Processed 2000/4348 records...\n",
      "  Processed 2500/4348 records...\n",
      "  Processed 3000/4348 records...\n",
      "  Processed 3500/4348 records...\n",
      "  Processed 4000/4348 records...\n",
      " Market cap matching completed: 4348 → 4285 records\n",
      "Creating additional firm size measures...\n",
      " Added REAL firm size controls: market_cap, log_market_cap, market_cap_quintile, firm_size_rank, large_firm\n",
      "\n",
      "Market Cap Summary ($ Billions):\n",
      "  Mean: $82.3B\n",
      "  Median: $56.8B\n",
      "  Min: $0.8B\n",
      "  Max: $1716.9B\n",
      "Adding volatility control variables...\n",
      " Added volatility controls: volatility_tstat, volatility_rsquared, systematic_risk\n",
      "Adding temporal control variables...\n",
      " Added temporal controls: year, quarter, financial_crisis, covid_period, time_trend\n",
      "Adding performance control variables...\n",
      " Added performance controls: past_car, car_consistency, market_model_quality, alpha_magnitude\n",
      "Adding sentiment complexity controls...\n",
      " Added sentiment controls: transcript_length, sentiment_balance, sentiment_strength, uncertainty_intensity\n",
      "Adding stock price momentum controls...\n",
      "Calculating pre-event price momentum for each observation...\n",
      "  Processed 500/4285 records...\n",
      "  Processed 1000/4285 records...\n",
      "  Processed 1500/4285 records...\n",
      "  Processed 2000/4285 records...\n",
      "  Processed 2500/4285 records...\n",
      "  Processed 3000/4285 records...\n",
      "  Processed 3500/4285 records...\n",
      "  Processed 4000/4285 records...\n",
      " Added momentum controls:\n",
      "   - price_momentum_1week, price_momentum_2week, price_momentum_1month\n",
      "   - price_trend (binary), price_volatility_pre_event\n",
      "   - cumulative_return_pre_event, momentum_category\n",
      "   - momentum_x_positive_sent, momentum_x_negative_sent (interaction terms)\n",
      "\n",
      " Pre-Event Momentum Summary (1-month):\n",
      "   Mean: 0.0202 (2.02%)\n",
      "   Median: 0.0190\n",
      "   Std Dev: 0.0871\n",
      "   Uptrend: 2638 events\n",
      "   Downtrend: 1647 events\n",
      "\n",
      "Adding abnormal turnover control variables...\n",
      "   Loaded market data with 252230 rows\n",
      "\n",
      "   Abnormal Turnover Summary:\n",
      "   Normal volume (mean): 15,936,202\n",
      "   Event volume (mean): 28,845,465\n",
      "   Turnover ratio (mean): 1.88\n",
      "   Abnormal turnover % (mean): 87.92%\n",
      "   High turnover events: 2115 (49.4%)\n",
      "\n",
      "   Abnormal Turnover Distribution:\n",
      "   Min: -68.47%\n",
      "   25th percentile: 11.12%\n",
      "   Median: 56.13%\n",
      "   75th percentile: 123.04%\n",
      "   Max: 1748.86%\n",
      " Added abnormal turnover controls: normal_volume, event_volume, abnormal_turnover, turnover_ratio, high_turnover\n",
      "Adding industry control variables...\n",
      " Added industry dummies: tech, financial, healthcare, energy, consumer, other\n",
      "\n",
      " All control variables added with REAL market cap data!\n",
      " Final dataset: 4285 records\n",
      " Total variables: 91\n",
      " CAR windows in final dataset: CAR_-1_1, CAR_0_1, CAR_0_2\n",
      "\n",
      " Final dataset saved to: exports/tables\\Final_Regression_Dataset.csv\n",
      "================================================================================\n",
      " CONTROL VARIABLES MODULE COMPLETED!\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>event_date</th>\n",
       "      <th>transcript_id</th>\n",
       "      <th>total_words</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>uncertainty_count</th>\n",
       "      <th>positive_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>event_volume</th>\n",
       "      <th>abnormal_turnover</th>\n",
       "      <th>turnover_ratio</th>\n",
       "      <th>high_turnover</th>\n",
       "      <th>tech</th>\n",
       "      <th>financial</th>\n",
       "      <th>healthcare</th>\n",
       "      <th>energy</th>\n",
       "      <th>consumer</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-07-18 19:16:50</td>\n",
       "      <td>ABT_2330</td>\n",
       "      <td>4580</td>\n",
       "      <td>122</td>\n",
       "      <td>82</td>\n",
       "      <td>71</td>\n",
       "      <td>0.026638</td>\n",
       "      <td>...</td>\n",
       "      <td>17396415.0</td>\n",
       "      <td>44.803069</td>\n",
       "      <td>1.448031</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABT</td>\n",
       "      <td>3</td>\n",
       "      <td>2007</td>\n",
       "      <td>2007-10-17 17:53:08</td>\n",
       "      <td>ABT_3153</td>\n",
       "      <td>4412</td>\n",
       "      <td>126</td>\n",
       "      <td>73</td>\n",
       "      <td>51</td>\n",
       "      <td>0.028558</td>\n",
       "      <td>...</td>\n",
       "      <td>20467694.5</td>\n",
       "      <td>92.405619</td>\n",
       "      <td>1.924056</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>4</td>\n",
       "      <td>2007</td>\n",
       "      <td>2008-01-23 21:45:06</td>\n",
       "      <td>ABT_4880</td>\n",
       "      <td>4882</td>\n",
       "      <td>167</td>\n",
       "      <td>75</td>\n",
       "      <td>74</td>\n",
       "      <td>0.034207</td>\n",
       "      <td>...</td>\n",
       "      <td>22273343.0</td>\n",
       "      <td>73.651755</td>\n",
       "      <td>1.736518</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABT</td>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008-04-16 18:24:19</td>\n",
       "      <td>ABT_7370</td>\n",
       "      <td>4680</td>\n",
       "      <td>131</td>\n",
       "      <td>76</td>\n",
       "      <td>70</td>\n",
       "      <td>0.027991</td>\n",
       "      <td>...</td>\n",
       "      <td>28611296.5</td>\n",
       "      <td>121.282353</td>\n",
       "      <td>2.212824</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABT</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008-07-17 00:49:23</td>\n",
       "      <td>ABT_10085</td>\n",
       "      <td>5096</td>\n",
       "      <td>184</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>0.036107</td>\n",
       "      <td>...</td>\n",
       "      <td>21447061.5</td>\n",
       "      <td>77.631008</td>\n",
       "      <td>1.776310</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343</th>\n",
       "      <td>XOM</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-05-01 20:00:40</td>\n",
       "      <td>XOM_167742</td>\n",
       "      <td>1616</td>\n",
       "      <td>62</td>\n",
       "      <td>59</td>\n",
       "      <td>22</td>\n",
       "      <td>0.038366</td>\n",
       "      <td>...</td>\n",
       "      <td>35399300.0</td>\n",
       "      <td>-27.640900</td>\n",
       "      <td>0.723591</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4344</th>\n",
       "      <td>XOM</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-05-27 21:59:15</td>\n",
       "      <td>XOM_170399</td>\n",
       "      <td>5579</td>\n",
       "      <td>168</td>\n",
       "      <td>192</td>\n",
       "      <td>68</td>\n",
       "      <td>0.030113</td>\n",
       "      <td>...</td>\n",
       "      <td>20243150.0</td>\n",
       "      <td>-40.748284</td>\n",
       "      <td>0.592517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>XOM</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-07-31 17:59:08</td>\n",
       "      <td>XOM_172110</td>\n",
       "      <td>5973</td>\n",
       "      <td>216</td>\n",
       "      <td>200</td>\n",
       "      <td>88</td>\n",
       "      <td>0.036163</td>\n",
       "      <td>...</td>\n",
       "      <td>32116200.0</td>\n",
       "      <td>33.263378</td>\n",
       "      <td>1.332634</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4346</th>\n",
       "      <td>XOM</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-10-30 21:24:08</td>\n",
       "      <td>XOM_176630</td>\n",
       "      <td>5304</td>\n",
       "      <td>181</td>\n",
       "      <td>156</td>\n",
       "      <td>61</td>\n",
       "      <td>0.034125</td>\n",
       "      <td>...</td>\n",
       "      <td>47885700.0</td>\n",
       "      <td>77.031703</td>\n",
       "      <td>1.770317</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4347</th>\n",
       "      <td>XOM</td>\n",
       "      <td>4</td>\n",
       "      <td>2020</td>\n",
       "      <td>2021-02-03 02:56:06</td>\n",
       "      <td>XOM_181141</td>\n",
       "      <td>5984</td>\n",
       "      <td>289</td>\n",
       "      <td>129</td>\n",
       "      <td>61</td>\n",
       "      <td>0.048295</td>\n",
       "      <td>...</td>\n",
       "      <td>26869800.0</td>\n",
       "      <td>-7.966594</td>\n",
       "      <td>0.920334</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4285 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker  quarter  fiscal_year          event_date transcript_id  \\\n",
       "0       ABT        2         2007 2007-07-18 19:16:50      ABT_2330   \n",
       "1       ABT        3         2007 2007-10-17 17:53:08      ABT_3153   \n",
       "2       ABT        4         2007 2008-01-23 21:45:06      ABT_4880   \n",
       "3       ABT        1         2008 2008-04-16 18:24:19      ABT_7370   \n",
       "4       ABT        2         2008 2008-07-17 00:49:23     ABT_10085   \n",
       "...     ...      ...          ...                 ...           ...   \n",
       "4343    XOM        1         2020 2020-05-01 20:00:40    XOM_167742   \n",
       "4344    XOM        0         2020 2020-05-27 21:59:15    XOM_170399   \n",
       "4345    XOM        2         2020 2020-07-31 17:59:08    XOM_172110   \n",
       "4346    XOM        3         2020 2020-10-30 21:24:08    XOM_176630   \n",
       "4347    XOM        4         2020 2021-02-03 02:56:06    XOM_181141   \n",
       "\n",
       "      total_words  positive_count  negative_count  uncertainty_count  \\\n",
       "0            4580             122              82                 71   \n",
       "1            4412             126              73                 51   \n",
       "2            4882             167              75                 74   \n",
       "3            4680             131              76                 70   \n",
       "4            5096             184              70                 70   \n",
       "...           ...             ...             ...                ...   \n",
       "4343         1616              62              59                 22   \n",
       "4344         5579             168             192                 68   \n",
       "4345         5973             216             200                 88   \n",
       "4346         5304             181             156                 61   \n",
       "4347         5984             289             129                 61   \n",
       "\n",
       "      positive_ratio  ...  event_volume  abnormal_turnover  turnover_ratio  \\\n",
       "0           0.026638  ...    17396415.0          44.803069        1.448031   \n",
       "1           0.028558  ...    20467694.5          92.405619        1.924056   \n",
       "2           0.034207  ...    22273343.0          73.651755        1.736518   \n",
       "3           0.027991  ...    28611296.5         121.282353        2.212824   \n",
       "4           0.036107  ...    21447061.5          77.631008        1.776310   \n",
       "...              ...  ...           ...                ...             ...   \n",
       "4343        0.038366  ...    35399300.0         -27.640900        0.723591   \n",
       "4344        0.030113  ...    20243150.0         -40.748284        0.592517   \n",
       "4345        0.036163  ...    32116200.0          33.263378        1.332634   \n",
       "4346        0.034125  ...    47885700.0          77.031703        1.770317   \n",
       "4347        0.048295  ...    26869800.0          -7.966594        0.920334   \n",
       "\n",
       "      high_turnover tech  financial  healthcare  energy  consumer  other  \n",
       "0                 0    0          0           1       0         0      0  \n",
       "1                 1    0          0           1       0         0      0  \n",
       "2                 1    0          0           1       0         0      0  \n",
       "3                 1    0          0           1       0         0      0  \n",
       "4                 1    0          0           1       0         0      0  \n",
       "...             ...  ...        ...         ...     ...       ...    ...  \n",
       "4343              0    0          0           0       1         0      0  \n",
       "4344              0    0          0           0       1         0      0  \n",
       "4345              0    0          0           0       1         0      0  \n",
       "4346              1    0          0           0       1         0      0  \n",
       "4347              0    0          0           0       1         0      0  \n",
       "\n",
       "[4285 rows x 91 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_merged_data(output_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"Load the merged CAR-sentiment dataset\"\"\"\n",
    "    merged_path = os.path.join(output_dir, \"CAR_Sentiment_Merged.csv\")\n",
    "    \n",
    "    if not os.path.exists(merged_path):\n",
    "        raise FileNotFoundError(f\"Merged dataset not found: {merged_path}\")\n",
    "    \n",
    "    df = pd.read_csv(merged_path)\n",
    "    df['event_date'] = pd.to_datetime(df['event_date'])\n",
    "    \n",
    "    # Add backward compatibility aliases for regression analysis\n",
    "    df['full_text_positive_ratio'] = df['positive_ratio']\n",
    "    df['full_text_negative_ratio'] = df['negative_ratio']\n",
    "    df['full_text_uncertainty_ratio'] = df['uncertainty_ratio']\n",
    "    df['full_text_sentiment_polarity'] = df['sentiment_polarity']\n",
    "    df['full_text_total_words'] = df['total_words']\n",
    "    \n",
    "    # Create 'car' as primary CAR measure (use CAR(-1,1) if available, otherwise first available)\n",
    "    if 'car_-1_1' in df.columns:\n",
    "        df['car'] = df['car_-1_1']\n",
    "        df['car_tstat'] = df['car_-1_1_tstat']\n",
    "        df['alpha'] = df['car_-1_1_alpha']\n",
    "        df['beta'] = df['car_-1_1_beta']\n",
    "        df['r_squared'] = df['car_-1_1_rsquared']\n",
    "        print(\" Using CAR(-1,1) as primary CAR measure\")\n",
    "    elif 'car_0_1' in df.columns:\n",
    "        df['car'] = df['car_0_1']\n",
    "        df['car_tstat'] = df['car_0_1_tstat']\n",
    "        df['alpha'] = df['car_0_1_alpha']\n",
    "        df['beta'] = df['car_0_1_beta']\n",
    "        df['r_squared'] = df['car_0_1_rsquared']\n",
    "        print(\" Using CAR(0,1) as primary CAR measure\")\n",
    "    elif 'car_0_2' in df.columns:\n",
    "        df['car'] = df['car_0_2']\n",
    "        df['car_tstat'] = df['car_0_2_tstat']\n",
    "        df['alpha'] = df['car_0_2_alpha']\n",
    "        df['beta'] = df['car_0_2_beta']\n",
    "        df['r_squared'] = df['car_0_2_rsquared']\n",
    "        print(\" Using CAR(0,2) as primary CAR measure\")\n",
    "    \n",
    "    print(f\" Loaded merged dataset: {len(df)} records\")\n",
    "    \n",
    "    # Show available CAR windows\n",
    "    car_windows = [col for col in df.columns if col.startswith('car_') and col.count('_') == 2]\n",
    "    if car_windows:\n",
    "        print(f\" Available CAR windows: {', '.join([c.upper() for c in car_windows])}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_real_market_cap_data(output_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"Load the real market cap data from Yahoo Finance\"\"\"\n",
    "    market_cap_path = os.path.join(output_dir, \"Market_Cap_Monthly_Lookup.csv\")\n",
    "    \n",
    "    if not os.path.exists(market_cap_path):\n",
    "        raise FileNotFoundError(f\"Market cap data not found: {market_cap_path}\")\n",
    "    \n",
    "    df = pd.read_csv(market_cap_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    print(f\" Loaded real market cap data: {len(df)} records for {df['ticker'].nunique()} companies\")\n",
    "    print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def find_closest_market_cap(ticker: str, event_date: pd.Timestamp, market_cap_data: pd.DataFrame) -> dict:\n",
    "    \"\"\"Find the closest market cap data for a given ticker and date\"\"\"\n",
    "    if market_cap_data is None or market_cap_data.empty:\n",
    "        return {'market_cap': np.nan, 'log_market_cap': np.nan, 'stock_price': np.nan}\n",
    "    \n",
    "    # Filter data for this ticker\n",
    "    ticker_data = market_cap_data[market_cap_data['ticker'] == ticker].copy()\n",
    "    \n",
    "    if ticker_data.empty:\n",
    "        return {'market_cap': np.nan, 'log_market_cap': np.nan, 'stock_price': np.nan}\n",
    "    \n",
    "    # Calculate time differences\n",
    "    ticker_data['time_diff'] = abs((ticker_data['date'] - event_date).dt.days)\n",
    "    \n",
    "    # Find the closest date (within 60 days)\n",
    "    closest_row = ticker_data[ticker_data['time_diff'] <= 60].nsmallest(1, 'time_diff')\n",
    "    \n",
    "    if closest_row.empty:\n",
    "        return {'market_cap': np.nan, 'log_market_cap': np.nan, 'stock_price': np.nan}\n",
    "    \n",
    "    row = closest_row.iloc[0]\n",
    "    return {\n",
    "        'market_cap': row['market_cap'],\n",
    "        'log_market_cap': row['log_market_cap'],\n",
    "        'stock_price': row['stock_price'],\n",
    "        'market_cap_date': row['date'],\n",
    "        'days_diff': row['time_diff']\n",
    "    }\n",
    "\n",
    "def add_real_firm_size_control(df: pd.DataFrame, output_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add firm size control variable using REAL Market Capitalization from Yahoo Finance\n",
    "    \"\"\"\n",
    "    print(\"Adding firm size control variable (REAL Market Capitalization from Yahoo Finance)...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Load real market cap data\n",
    "    market_cap_data = load_real_market_cap_data(output_dir)\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['market_cap'] = np.nan\n",
    "    df['log_market_cap'] = np.nan\n",
    "    df['stock_price'] = np.nan\n",
    "    df['market_cap_date'] = pd.NaT\n",
    "    df['market_cap_days_diff'] = np.nan\n",
    "    \n",
    "    # Match market cap for each observation\n",
    "    print(\"Matching market cap data to earnings events...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        market_cap_info = find_closest_market_cap(row['ticker'], row['event_date'], market_cap_data)\n",
    "        \n",
    "        df.at[idx, 'market_cap'] = market_cap_info['market_cap']\n",
    "        df.at[idx, 'log_market_cap'] = market_cap_info['log_market_cap']\n",
    "        df.at[idx, 'stock_price'] = market_cap_info['stock_price']\n",
    "        \n",
    "        if 'market_cap_date' in market_cap_info:\n",
    "            df.at[idx, 'market_cap_date'] = market_cap_info['market_cap_date']\n",
    "            df.at[idx, 'market_cap_days_diff'] = market_cap_info['days_diff']\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df)} records...\")\n",
    "    \n",
    "    # Remove rows where we couldn't find market cap data\n",
    "    original_count = len(df)\n",
    "    df = df.dropna(subset=['market_cap'])\n",
    "    \n",
    "    print(f\" Market cap matching completed: {original_count} → {len(df)} records\")\n",
    "        \n",
    "    # Create additional size measures\n",
    "    print(\"Creating additional firm size measures...\")\n",
    "        \n",
    "    # Size quintiles based on real market cap\n",
    "    df['market_cap_quintile'] = pd.qcut(df['market_cap'], \n",
    "                                          q=5, \n",
    "                                          labels=['Small', 'Small-Mid', 'Mid', 'Mid-Large', 'Large'])\n",
    "        \n",
    "    # Size ranks (0-1 scale)\n",
    "    df['firm_size_rank'] = df['market_cap'].rank(pct=True)\n",
    "        \n",
    "    # Binary large vs small indicator (top 30%)\n",
    "    df['large_firm'] = (df['firm_size_rank'] > 0.7).astype(int)\n",
    "        \n",
    "    # Market cap in billions for easier interpretation\n",
    "    df['market_cap_billions'] = df['market_cap'] / 1e9\n",
    "        \n",
    "    # Size categories based on common finance thresholds\n",
    "    df['size_category'] = pd.cut(df['market_cap_billions'], \n",
    "                                   bins=[0, 2, 10, 50, 200, float('inf')],\n",
    "                                   labels=['Micro', 'Small', 'Mid', 'Large', 'Mega'])\n",
    "        \n",
    "    print(\" Added REAL firm size controls: market_cap, log_market_cap, market_cap_quintile, firm_size_rank, large_firm\")\n",
    "        \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nMarket Cap Summary ($ Billions):\")\n",
    "    print(f\"  Mean: ${df['market_cap_billions'].mean():.1f}B\")\n",
    "    print(f\"  Median: ${df['market_cap_billions'].median():.1f}B\")\n",
    "    print(f\"  Min: ${df['market_cap_billions'].min():.1f}B\")\n",
    "    print(f\"  Max: ${df['market_cap_billions'].max():.1f}B\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def add_volatility_control(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add stock price volatility control (required by assignment)\n",
    "    Using CAR t-statistic and market model R-squared as volatility proxies\n",
    "    \"\"\"\n",
    "    print(\"Adding volatility control variables...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Volatility proxy 1: Absolute CAR t-statistic (using primary CAR)\n",
    "    if 'car_tstat' in df.columns:\n",
    "        df['volatility_tstat'] = np.abs(df['car_tstat'])\n",
    "    \n",
    "    # Volatility proxy 2: Inverse of market model R-squared (lower R² = higher idiosyncratic risk)\n",
    "    if 'r_squared' in df.columns:\n",
    "        df['volatility_rsquared'] = 1 - df['r_squared']\n",
    "    \n",
    "    # Volatility proxy 3: Beta-based systematic risk\n",
    "    if 'beta' in df.columns:\n",
    "        df['systematic_risk'] = np.abs(df['beta'])\n",
    "    \n",
    "    # Combined volatility measure\n",
    "    if 'volatility_tstat' in df.columns and 'volatility_rsquared' in df.columns:\n",
    "        df['volatility_combined'] = (df['volatility_tstat'] + df['volatility_rsquared']) / 2\n",
    "    \n",
    "    print(\" Added volatility controls: volatility_tstat, volatility_rsquared, systematic_risk\")\n",
    "    return df\n",
    "\n",
    "def add_temporal_controls(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add time-based control variables\"\"\"\n",
    "    print(\"Adding temporal control variables...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Year and quarter effects\n",
    "    df['year'] = df['event_date'].dt.year\n",
    "    if 'quarter' not in df.columns:\n",
    "        df['quarter'] = df['event_date'].dt.quarter\n",
    "    df['quarter'] = df['quarter'].astype(int)\n",
    "    \n",
    "    # Financial crisis indicator (2008-2009)\n",
    "    df['financial_crisis'] = ((df['year'] >= 2008) & (df['year'] <= 2009)).astype(int)\n",
    "    \n",
    "    # COVID period indicator (2020-2021)\n",
    "    df['covid_period'] = ((df['year'] >= 2020) & (df['year'] <= 2021)).astype(int)\n",
    "    \n",
    "    # Time trend\n",
    "    min_year = df['year'].min()\n",
    "    df['time_trend'] = df['year'] - min_year\n",
    "    \n",
    "    print(\" Added temporal controls: year, quarter, financial_crisis, covid_period, time_trend\")\n",
    "    return df\n",
    "\n",
    "def add_performance_controls(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add past performance and market condition controls\"\"\"\n",
    "    print(\"Adding performance control variables...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Sort by ticker and date for lagged variables\n",
    "    df = df.sort_values(['ticker', 'event_date'])\n",
    "    \n",
    "    # Past performance proxy: Lagged CAR (previous quarter's performance) - using primary CAR\n",
    "    if 'car' in df.columns:\n",
    "        df['past_car'] = df.groupby('ticker')['car'].shift(1)\n",
    "        \n",
    "        # Performance consistency: Rolling standard deviation of CAR\n",
    "        df['car_consistency'] = df.groupby('ticker')['car'].rolling(window=4, min_periods=2).std().reset_index(0, drop=True)\n",
    "    \n",
    "    # Market model quality (higher R² = better market fit)\n",
    "    if 'r_squared' in df.columns:\n",
    "        df['market_model_quality'] = df['r_squared']\n",
    "    \n",
    "    # Alpha significance (measure of abnormal performance)\n",
    "    if 'alpha' in df.columns:\n",
    "        df['alpha_magnitude'] = np.abs(df['alpha'])\n",
    "    \n",
    "    print(\" Added performance controls: past_car, car_consistency, market_model_quality, alpha_magnitude\")\n",
    "    return df\n",
    "\n",
    "def add_sentiment_complexity_controls(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add additional sentiment-based controls\"\"\"\n",
    "    print(\"Adding sentiment complexity controls...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Text length as information complexity proxy\n",
    "    df['transcript_length'] = df['total_words']\n",
    "    df['log_transcript_length'] = np.log(df['transcript_length'] + 1)\n",
    "    \n",
    "    # Sentiment balance measures\n",
    "    df['sentiment_balance'] = np.abs(df['positive_ratio'] - df['negative_ratio'])\n",
    "    \n",
    "    # Net sentiment strength (absolute value of net sentiment)\n",
    "    df['sentiment_strength'] = np.abs(df['net_sentiment'])\n",
    "    \n",
    "    # Uncertainty intensity (using available uncertainty ratio)\n",
    "    df['uncertainty_intensity'] = df['uncertainty_ratio']\n",
    "    \n",
    "    # Sentiment concentration (how concentrated is the sentiment)\n",
    "    df['sentiment_concentration'] = df['positive_ratio'] + df['negative_ratio']\n",
    "    \n",
    "    print(\" Added sentiment controls: transcript_length, sentiment_balance, sentiment_strength, uncertainty_intensity\")\n",
    "    return df\n",
    "\n",
    "def add_stock_price_momentum_controls(df: pd.DataFrame, output_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add stock price momentum controls to measure pre-event price trends\n",
    "    \n",
    "    This captures whether the stock was trending up or down before the earnings event,\n",
    "    which could reduce the impact of sentiment (momentum continuation hypothesis)\n",
    "    \"\"\"\n",
    "    print(\"Adding stock price momentum controls...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Load daily stock price data\n",
    "    daily_price_path = os.path.join(output_dir, \"Market_Cap_Daily_Data.csv\")\n",
    "    \n",
    "    if not os.path.exists(daily_price_path):\n",
    "        print(\"  Daily price data not found. Skipping momentum controls.\")\n",
    "        return df\n",
    "    \n",
    "    daily_data = pd.read_csv(daily_price_path)\n",
    "    daily_data['date'] = pd.to_datetime(daily_data['date'])\n",
    "    \n",
    "    # Initialize momentum columns\n",
    "    df['price_momentum_1week'] = np.nan\n",
    "    df['price_momentum_2week'] = np.nan\n",
    "    df['price_momentum_1month'] = np.nan\n",
    "    df['price_trend'] = np.nan  # Binary: 1 = uptrend, 0 = downtrend\n",
    "    df['price_volatility_pre_event'] = np.nan\n",
    "    df['cumulative_return_pre_event'] = np.nan\n",
    "    \n",
    "    print(\"Calculating pre-event price momentum for each observation...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        ticker = row['ticker']\n",
    "        event_date = row['event_date']\n",
    "        \n",
    "        # Get stock prices for this company\n",
    "        ticker_prices = daily_data[daily_data['ticker'] == ticker].copy()\n",
    "        ticker_prices = ticker_prices.sort_values('date')\n",
    "        \n",
    "        # Filter prices before the event\n",
    "        pre_event_prices = ticker_prices[ticker_prices['date'] < event_date].copy()\n",
    "        \n",
    "        if len(pre_event_prices) < 5:\n",
    "            continue  # Not enough data\n",
    "        \n",
    "        # Get the most recent price before event (reference price)\n",
    "        reference_price = pre_event_prices.iloc[-1]['stock_price']\n",
    "        reference_date = pre_event_prices.iloc[-1]['date']\n",
    "        \n",
    "        # Calculate 1-week momentum (5 trading days before event)\n",
    "        days_back_1week = 5\n",
    "        if len(pre_event_prices) >= days_back_1week + 1:\n",
    "            price_1week_ago = pre_event_prices.iloc[-(days_back_1week + 1)]['stock_price']\n",
    "            if price_1week_ago > 0:\n",
    "                momentum_1week = (reference_price - price_1week_ago) / price_1week_ago\n",
    "                df.at[idx, 'price_momentum_1week'] = momentum_1week\n",
    "        \n",
    "        # Calculate 2-week momentum (10 trading days before event)\n",
    "        days_back_2week = 10\n",
    "        if len(pre_event_prices) >= days_back_2week + 1:\n",
    "            price_2week_ago = pre_event_prices.iloc[-(days_back_2week + 1)]['stock_price']\n",
    "            if price_2week_ago > 0:\n",
    "                momentum_2week = (reference_price - price_2week_ago) / price_2week_ago\n",
    "                df.at[idx, 'price_momentum_2week'] = momentum_2week\n",
    "        \n",
    "        # Calculate 1-month momentum (20 trading days before event)\n",
    "        days_back_1month = 20\n",
    "        if len(pre_event_prices) >= days_back_1month + 1:\n",
    "            price_1month_ago = pre_event_prices.iloc[-(days_back_1month + 1)]['stock_price']\n",
    "            if price_1month_ago > 0:\n",
    "                momentum_1month = (reference_price - price_1month_ago) / price_1month_ago\n",
    "                df.at[idx, 'price_momentum_1month'] = momentum_1month\n",
    "                \n",
    "                # Determine price trend (1 = uptrend, 0 = downtrend)\n",
    "                df.at[idx, 'price_trend'] = 1 if momentum_1month > 0 else 0\n",
    "        \n",
    "        # Calculate pre-event volatility (standard deviation of returns in last 20 days)\n",
    "        if len(pre_event_prices) >= 21:\n",
    "            recent_prices = pre_event_prices.iloc[-21:]['stock_price'].values\n",
    "            returns = np.diff(recent_prices) / recent_prices[:-1]\n",
    "            price_volatility = np.std(returns)\n",
    "            df.at[idx, 'price_volatility_pre_event'] = price_volatility\n",
    "        \n",
    "        # Calculate cumulative return over the pre-event period (last 20 days)\n",
    "        if len(pre_event_prices) >= 21:\n",
    "            price_20days_ago = pre_event_prices.iloc[-21]['stock_price']\n",
    "            if price_20days_ago > 0:\n",
    "                cumulative_return = (reference_price - price_20days_ago) / price_20days_ago\n",
    "                df.at[idx, 'cumulative_return_pre_event'] = cumulative_return\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (idx + 1) % 500 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(df)} records...\")\n",
    "    \n",
    "    # Create interaction terms (for regression analysis)\n",
    "    # These capture how sentiment effect varies with momentum\n",
    "    \n",
    "    if 'price_momentum_1month' in df.columns and 'positive_ratio' in df.columns:\n",
    "        # Positive sentiment has less effect when stock already rising\n",
    "        df['momentum_x_positive_sent'] = df['price_momentum_1month'] * df['positive_ratio']\n",
    "        \n",
    "        # Negative sentiment has less effect when stock already falling\n",
    "        df['momentum_x_negative_sent'] = df['price_momentum_1month'] * df['negative_ratio']\n",
    "    \n",
    "    # Create momentum categories for easier interpretation\n",
    "    if 'price_momentum_1month' in df.columns:\n",
    "        df['momentum_category'] = pd.cut(\n",
    "            df['price_momentum_1month'],\n",
    "            bins=[-np.inf, -0.10, -0.02, 0.02, 0.10, np.inf],\n",
    "            labels=['Strong Down', 'Weak Down', 'Flat', 'Weak Up', 'Strong Up']\n",
    "        )\n",
    "    \n",
    "    print(\" Added momentum controls:\")\n",
    "    print(\"   - price_momentum_1week, price_momentum_2week, price_momentum_1month\")\n",
    "    print(\"   - price_trend (binary), price_volatility_pre_event\")\n",
    "    print(\"   - cumulative_return_pre_event, momentum_category\")\n",
    "    print(\"   - momentum_x_positive_sent, momentum_x_negative_sent (interaction terms)\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    if 'price_momentum_1month' in df.columns:\n",
    "        valid_momentum = df['price_momentum_1month'].dropna()\n",
    "        if len(valid_momentum) > 0:\n",
    "            print(f\"\\n Pre-Event Momentum Summary (1-month):\")\n",
    "            print(f\"   Mean: {valid_momentum.mean():.4f} ({valid_momentum.mean()*100:.2f}%)\")\n",
    "            print(f\"   Median: {valid_momentum.median():.4f}\")\n",
    "            print(f\"   Std Dev: {valid_momentum.std():.4f}\")\n",
    "            print(f\"   Uptrend: {(df['price_trend'] == 1).sum()} events\")\n",
    "            print(f\"   Downtrend: {(df['price_trend'] == 0).sum()} events\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_industry_dummies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create industry dummy variables based on ticker\"\"\"\n",
    "    print(\"Adding industry control variables...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Industry classification based on ticker knowledge\n",
    "    tech_companies = ['MSFT', 'GOOG', 'FB', 'INTC', 'ADBE', 'CRM', 'INTU', 'TSLA', 'ORCL', 'NFLX']\n",
    "    financial_companies = ['BAC', 'JPM', 'MS', 'GS', 'AXP', 'BLK', 'V', 'MA', 'MCO']\n",
    "    healthcare_companies = ['JNJ', 'ABT', 'LLY', 'AMGN', 'BMY', 'MRK', 'TMO', 'ISRG', 'MDT', 'UNH']\n",
    "    energy_companies = ['XOM']\n",
    "    consumer_companies = ['MCD', 'NKE', 'COST', 'HD', 'PG', 'PEP']\n",
    "    \n",
    "    # Create industry dummies\n",
    "    df['tech'] = df['ticker'].isin(tech_companies).astype(int)\n",
    "    df['financial'] = df['ticker'].isin(financial_companies).astype(int)\n",
    "    df['healthcare'] = df['ticker'].isin(healthcare_companies).astype(int)\n",
    "    df['energy'] = df['ticker'].isin(energy_companies).astype(int)\n",
    "    df['consumer'] = df['ticker'].isin(consumer_companies).astype(int)\n",
    "    df['other'] = (~df['ticker'].isin(\n",
    "        tech_companies + financial_companies + healthcare_companies + \n",
    "        energy_companies + consumer_companies)).astype(int)\n",
    "    \n",
    "    print(\" Added industry dummies: tech, financial, healthcare, energy, consumer, other\")\n",
    "    return df\n",
    "\n",
    "def add_abnormal_turnover_control(df: pd.DataFrame, output_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add abnormal turnover control variables based on trading volume.\n",
    "    \n",
    "    Calculates:\n",
    "    - Normal turnover: Average volume during estimation window (-60 to -10 days)\n",
    "    - Abnormal turnover: Event window volume vs normal volume\n",
    "    - Turnover ratio: Event volume / Normal volume\n",
    "    \"\"\"\n",
    "    print(\"\\nAdding abnormal turnover control variables...\")\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Load market cap data with volume\n",
    "    market_data_path = os.path.join(output_dir, \"Market_Cap_Daily_Data.csv\")\n",
    "    if not os.path.exists(market_data_path):\n",
    "        print(f\"  Warning: Market cap data not found at {market_data_path}\")\n",
    "        return df\n",
    "    \n",
    "    market_data = pd.read_csv(market_data_path)\n",
    "    market_data['date'] = pd.to_datetime(market_data['date'])\n",
    "    \n",
    "    # Check if volume column exists\n",
    "    if 'volume' not in market_data.columns:\n",
    "        print(\"  Warning: Volume column not found in market data\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"   Loaded market data with {len(market_data)} rows\")\n",
    "    \n",
    "    # Initialize columns\n",
    "    df['normal_volume'] = np.nan\n",
    "    df['event_volume'] = np.nan\n",
    "    df['abnormal_turnover'] = np.nan\n",
    "    df['turnover_ratio'] = np.nan\n",
    "    df['high_turnover'] = 0  # Binary: 1 if turnover ratio > median\n",
    "    \n",
    "    # Calculate for each company-event\n",
    "    for idx, row in df.iterrows():\n",
    "        ticker = row['ticker']\n",
    "        event_date = pd.to_datetime(row['event_date'])\n",
    "        \n",
    "        # Get company's volume data\n",
    "        company_data = market_data[market_data['ticker'] == ticker].copy()\n",
    "        company_data = company_data.sort_values('date')\n",
    "        \n",
    "        # 1. Calculate normal volume (estimation window: -60 to -10 days)\n",
    "        estimation_start = event_date - pd.Timedelta(days=60)\n",
    "        estimation_end = event_date - pd.Timedelta(days=10)\n",
    "        \n",
    "        estimation_data = company_data[\n",
    "            (company_data['date'] >= estimation_start) & \n",
    "            (company_data['date'] <= estimation_end)\n",
    "        ]\n",
    "        \n",
    "        if len(estimation_data) > 0:\n",
    "            normal_vol = estimation_data['volume'].mean()\n",
    "            df.at[idx, 'normal_volume'] = normal_vol\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 2. Calculate event window volume\n",
    "        # Use CAR(-1,1) window as primary event window\n",
    "        event_start = event_date - pd.Timedelta(days=1)\n",
    "        event_end = event_date + pd.Timedelta(days=1)\n",
    "        \n",
    "        event_data = company_data[\n",
    "            (company_data['date'] >= event_start) & \n",
    "            (company_data['date'] <= event_end)\n",
    "        ]\n",
    "        \n",
    "        if len(event_data) > 0:\n",
    "            event_vol = event_data['volume'].mean()\n",
    "            df.at[idx, 'event_volume'] = event_vol\n",
    "            \n",
    "            # 3. Calculate abnormal turnover metrics\n",
    "            if normal_vol > 0:\n",
    "                # Turnover ratio\n",
    "                turnover_ratio = event_vol / normal_vol\n",
    "                df.at[idx, 'turnover_ratio'] = turnover_ratio\n",
    "                \n",
    "                # Abnormal turnover (percentage change)\n",
    "                abnormal = ((event_vol - normal_vol) / normal_vol) * 100\n",
    "                df.at[idx, 'abnormal_turnover'] = abnormal\n",
    "    \n",
    "    # 4. Create high turnover binary indicator (above median)\n",
    "    median_turnover = df['turnover_ratio'].median()\n",
    "    df['high_turnover'] = (df['turnover_ratio'] > median_turnover).astype(int)\n",
    "    \n",
    "    # 5. Create interaction terms with sentiment\n",
    "    if 'sentiment_score' in df.columns:\n",
    "        df['abnormal_turnover_x_sentiment'] = df['abnormal_turnover'] * df['sentiment_score']\n",
    "        df['high_turnover_x_sentiment'] = df['high_turnover'] * df['sentiment_score']\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n   Abnormal Turnover Summary:\")\n",
    "    print(f\"   Normal volume (mean): {df['normal_volume'].mean():,.0f}\")\n",
    "    print(f\"   Event volume (mean): {df['event_volume'].mean():,.0f}\")\n",
    "    print(f\"   Turnover ratio (mean): {df['turnover_ratio'].mean():.2f}\")\n",
    "    print(f\"   Abnormal turnover % (mean): {df['abnormal_turnover'].mean():.2f}%\")\n",
    "    print(f\"   High turnover events: {df['high_turnover'].sum()} ({df['high_turnover'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Distribution of abnormal turnover\n",
    "    print(f\"\\n   Abnormal Turnover Distribution:\")\n",
    "    print(f\"   Min: {df['abnormal_turnover'].min():.2f}%\")\n",
    "    print(f\"   25th percentile: {df['abnormal_turnover'].quantile(0.25):.2f}%\")\n",
    "    print(f\"   Median: {df['abnormal_turnover'].median():.2f}%\")\n",
    "    print(f\"   75th percentile: {df['abnormal_turnover'].quantile(0.75):.2f}%\")\n",
    "    print(f\"   Max: {df['abnormal_turnover'].max():.2f}%\")\n",
    "    \n",
    "    print(\" Added abnormal turnover controls: normal_volume, event_volume, abnormal_turnover, turnover_ratio, high_turnover\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_all_controls(df: pd.DataFrame = None, output_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"Add all control variables to the dataset\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ADDING CONTROL VARIABLES (WITH REAL MARKET CAP DATA)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if df is None:\n",
    "        df = load_merged_data(output_dir)\n",
    "    \n",
    "    # Add all control variable categories (REAL market cap first!)\n",
    "    df = add_real_firm_size_control(df, output_dir)\n",
    "    df = add_volatility_control(df)\n",
    "    df = add_temporal_controls(df)\n",
    "    df = add_performance_controls(df)\n",
    "    df = add_sentiment_complexity_controls(df)\n",
    "    df = add_stock_price_momentum_controls(df, output_dir)  # Pre-event price momentum\n",
    "    df = add_abnormal_turnover_control(df, output_dir)  # NEW: Abnormal trading volume\n",
    "    df = create_industry_dummies(df)\n",
    "    \n",
    "    # Remove any infinite or extremely large values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    print(f\"\\n All control variables added with REAL market cap data!\")\n",
    "    print(f\" Final dataset: {len(df)} records\")\n",
    "    print(f\" Total variables: {len(df.columns)}\")\n",
    "    \n",
    "    # Show available CAR windows in final dataset\n",
    "    car_windows = [col for col in df.columns if col.startswith('car_') and col.count('_') == 2]\n",
    "    if car_windows:\n",
    "        print(f\" CAR windows in final dataset: {', '.join([c.upper() for c in car_windows])}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def step6_add_control_variables():\n",
    "    \"\"\"Main function to add control variables\"\"\"\n",
    "    \n",
    "    # Add all controls\n",
    "    df_with_controls = add_all_controls()\n",
    "    \n",
    "    # Save final dataset\n",
    "    output_path = os.path.join(\"exports/tables\", \"Final_Regression_Dataset.csv\")\n",
    "    df_with_controls.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n Final dataset saved to: {output_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\" CONTROL VARIABLES MODULE COMPLETED!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return df_with_controls\n",
    "\n",
    "step6_add_control_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724af84",
   "metadata": {},
   "source": [
    "# Step 7: Regression Analysis\n",
    "\n",
    "**Input Files:**\n",
    "- `exports/tables/Final_Regression_Dataset.csv` (from Step 6)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/tables/Complete_Regression_Analysis.xlsx`\n",
    "- `exports/tables/Regression_Results_Table.csv`\n",
    "- `exports/figures/Sentiment_CAR_Scatterplots.png`\n",
    "- `exports/figures/Regression_Diagnostics.png`\n",
    "\n",
    "**Analysis:**\n",
    "- OLS regression with HC3 robust standard errors\n",
    "- Multiple CAR windows: CAR(-1,1), CAR(0,1), CAR(0,2)\n",
    "- Diagnostic plots and residual analysis\n",
    "\n",
    "This module performs the final regression analysis to examine how sentiment\n",
    "from earnings conference calls affects Cumulative Abnormal Returns (CAR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f126b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "REGRESSION ANALYSIS: SENTIMENT IMPACT ON CAR\n",
      "================================================================================\n",
      " Loaded final dataset: 4285 observations\n",
      "\n",
      " Multiple CAR windows detected: CAR_-1_1, CAR_0_1, CAR_0_2\n",
      "Running analysis for ALL CAR windows...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RUNNING REGRESSIONS FOR ALL CAR WINDOWS\n",
      "================================================================================\n",
      " Found 3 CAR windows: CAR_-1_1, CAR_0_1, CAR_0_2\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANALYZING CAR_-1_1\n",
      "================================================================================\n",
      " Clean dataset: 4121 observations (96.2%)\n",
      "\n",
      "============================================================\n",
      "BASELINE REGRESSION: SENTIMENT ONLY\n",
      "============================================================\n",
      "R-squared: 0.0028\n",
      "Adjusted R-squared: 0.0021\n",
      "F-statistic: 4.3700 (p-value: 0.0044)\n",
      "Observations: 4121\n",
      "\n",
      "============================================================\n",
      "FULL REGRESSION: SENTIMENT + CONTROLS\n",
      "============================================================\n",
      "R-squared: 0.0086\n",
      "Adjusted R-squared: 0.0067\n",
      "F-statistic: 3.3426 (p-value: 0.0008)\n",
      "Observations: 4121\n",
      "\n",
      "============================================================\n",
      "ROBUSTNESS TESTS\n",
      "============================================================\n",
      " Completed robustness tests:\n",
      "  - Net sentiment R²: 0.0064\n",
      "  - Sentiment polarity R²: 0.0065\n",
      "  - Binary sentiment R²: 0.0071\n",
      "\n",
      " CAR_-1_1 Results:\n",
      "   R²: 0.0086\n",
      "   Adj. R²: 0.0067\n",
      "   F-statistic: 3.3426 (p=0.0008)\n",
      "   N: 4121\n",
      "   Positive sentiment: 0.1664 *\n",
      "   Negative sentiment: -0.3113 ***\n",
      "\n",
      "================================================================================\n",
      "ANALYZING CAR_0_1\n",
      "================================================================================\n",
      " Clean dataset: 3319 observations (77.5%)\n",
      "\n",
      "============================================================\n",
      "BASELINE REGRESSION: SENTIMENT ONLY\n",
      "============================================================\n",
      "R-squared: 0.0031\n",
      "Adjusted R-squared: 0.0021\n",
      "F-statistic: 3.7085 (p-value: 0.0112)\n",
      "Observations: 3319\n",
      "\n",
      "============================================================\n",
      "FULL REGRESSION: SENTIMENT + CONTROLS\n",
      "============================================================\n",
      "R-squared: 0.0063\n",
      "Adjusted R-squared: 0.0039\n",
      "F-statistic: 2.0707 (p-value: 0.0353)\n",
      "Observations: 3319\n",
      "\n",
      "============================================================\n",
      "ROBUSTNESS TESTS\n",
      "============================================================\n",
      " Completed robustness tests:\n",
      "  - Net sentiment R²: 0.0052\n",
      "  - Sentiment polarity R²: 0.0052\n",
      "  - Binary sentiment R²: 0.0061\n",
      "\n",
      " CAR_0_1 Results:\n",
      "   R²: 0.0063\n",
      "   Adj. R²: 0.0039\n",
      "   F-statistic: 2.0707 (p=0.0353)\n",
      "   N: 3319\n",
      "   Positive sentiment: 0.1541 \n",
      "   Negative sentiment: -0.2776 **\n",
      "\n",
      "================================================================================\n",
      "ANALYZING CAR_0_2\n",
      "================================================================================\n",
      " Clean dataset: 3337 observations (77.9%)\n",
      "\n",
      "============================================================\n",
      "BASELINE REGRESSION: SENTIMENT ONLY\n",
      "============================================================\n",
      "R-squared: 0.0028\n",
      "Adjusted R-squared: 0.0019\n",
      "F-statistic: 3.4390 (p-value: 0.0162)\n",
      "Observations: 3337\n",
      "\n",
      "============================================================\n",
      "FULL REGRESSION: SENTIMENT + CONTROLS\n",
      "============================================================\n",
      "R-squared: 0.0072\n",
      "Adjusted R-squared: 0.0048\n",
      "F-statistic: 2.1907 (p-value: 0.0253)\n",
      "Observations: 3337\n",
      "\n",
      "============================================================\n",
      "ROBUSTNESS TESTS\n",
      "============================================================\n",
      " Completed robustness tests:\n",
      "  - Net sentiment R²: 0.0052\n",
      "  - Sentiment polarity R²: 0.0051\n",
      "  - Binary sentiment R²: 0.0055\n",
      "\n",
      " CAR_0_2 Results:\n",
      "   R²: 0.0072\n",
      "   Adj. R²: 0.0048\n",
      "   F-statistic: 2.1907 (p=0.0253)\n",
      "   N: 3337\n",
      "   Positive sentiment: 0.1717 *\n",
      "   Negative sentiment: -0.2633 **\n",
      "\n",
      "================================================================================\n",
      "SAVING MULTI-CAR WINDOW RESULTS\n",
      "================================================================================\n",
      " Saved multi-CAR comparison to: exports/tables\\Multi_CAR_Window_Comparison.xlsx\n",
      " Saved comparison table to: exports/tables\\Multi_CAR_Window_Comparison.csv\n",
      "\n",
      "================================================================================\n",
      "PRIMARY CAR ANALYSIS (CAR column)\n",
      "================================================================================\n",
      " Clean dataset: 4121 observations (96.2%)\n",
      "\n",
      "============================================================\n",
      "BASELINE REGRESSION: SENTIMENT ONLY\n",
      "============================================================\n",
      "R-squared: 0.0028\n",
      "Adjusted R-squared: 0.0021\n",
      "F-statistic: 4.3700 (p-value: 0.0044)\n",
      "Observations: 4121\n",
      "\n",
      "============================================================\n",
      "FULL REGRESSION: SENTIMENT + CONTROLS\n",
      "============================================================\n",
      "R-squared: 0.0086\n",
      "Adjusted R-squared: 0.0067\n",
      "F-statistic: 3.3426 (p-value: 0.0008)\n",
      "Observations: 4121\n",
      "\n",
      "============================================================\n",
      "ROBUSTNESS TESTS\n",
      "============================================================\n",
      " Completed robustness tests:\n",
      "  - Net sentiment R²: 0.0064\n",
      "  - Sentiment polarity R²: 0.0065\n",
      "  - Binary sentiment R²: 0.0071\n",
      "\n",
      "Creating regression results table...\n",
      " Regression table saved: exports/tables\\Regression_Results_Table.csv\n",
      "\n",
      "Creating visualization plots...\n",
      " Plots saved:\n",
      "  - exports/figures\\Sentiment_CAR_Scatterplots.png\n",
      "  - exports/figures\\Regression_Diagnostics.png\n",
      " Detailed results saved: exports/tables\\Complete_Regression_Analysis.xlsx\n",
      "\n",
      "================================================================================\n",
      "REGRESSION RESULTS INTERPRETATION\n",
      "================================================================================\n",
      "Model explains 0.86% of CAR variation\n",
      "F-test p-value: 0.0008 (Significant)\n",
      "\n",
      "Key Findings:\n",
      "• Negative sentiment decreases CAR by 0.3113 ***\n",
      "  → 1 percentage point increase in negative sentiment leads to -31.13 percentage point change in CAR\n",
      "• Positive sentiment increases CAR by 0.1664 *\n",
      "  → 1 percentage point increase in positive sentiment leads to 16.64 percentage point change in CAR\n",
      "• Uncertainty increases CAR by 0.3323 *\n",
      "  → 1 percentage point increase in uncertainty leads to 33.23 percentage point change in CAR\n",
      "\n",
      "Statistical significance: *** p<0.01, ** p<0.05, * p<0.1\n",
      "\n",
      "================================================================================\n",
      " REGRESSION ANALYSIS COMPLETED!\n",
      "================================================================================\n",
      "Files created:\n",
      "• Complete_Regression_Analysis.xlsx\n",
      "• Regression_Results_Table.csv\n",
      "• Sentiment_CAR_Scatterplots.png\n",
      "• Regression_Diagnostics.png\n",
      "• Multi_CAR_Window_Comparison.xlsx\n",
      "• Multi_CAR_Window_Comparison.csv\n",
      "\n",
      " Ready for report writing!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Install required packages if not available\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "except ImportError:\n",
    "    print(\"Installing required statistics packages...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'statsmodels'])\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def load_final_dataset(output_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"Load the final dataset with all controls\"\"\"\n",
    "    data_path = os.path.join(output_dir, \"Final_Regression_Dataset.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Final dataset not found: {data_path}\")\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    df['event_date'] = pd.to_datetime(df['event_date'])\n",
    "    \n",
    "    print(f\" Loaded final dataset: {len(df)} observations\")\n",
    "    return df\n",
    "\n",
    "def prepare_regression_data(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"Prepare data for regression analysis\"\"\"\n",
    "    \n",
    "    # Remove rows with missing values in key variables\n",
    "    key_vars = ['car', 'negative_ratio', 'positive_ratio', \n",
    "               'uncertainty_ratio', 'volatility_combined', 'log_market_cap']\n",
    "    \n",
    "    df_clean = df.dropna(subset=key_vars).copy()\n",
    "    \n",
    "    print(f\" Clean dataset: {len(df_clean)} observations ({len(df_clean)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def run_baseline_regression(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Run baseline regression with sentiment variables only\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BASELINE REGRESSION: SENTIMENT ONLY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define variables\n",
    "    y = df['car']\n",
    "    X = df[['negative_ratio', 'positive_ratio', 'uncertainty_ratio']]\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Run regression\n",
    "    model = sm.OLS(y, X).fit(cov_type='HC3')  # Robust standard errors\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'r_squared': model.rsquared,\n",
    "        'adj_r_squared': model.rsquared_adj,\n",
    "        'f_stat': model.fvalue,\n",
    "        'f_pvalue': model.f_pvalue,\n",
    "        'n_obs': model.nobs\n",
    "    }\n",
    "    \n",
    "    print(f\"R-squared: {results['r_squared']:.4f}\")\n",
    "    print(f\"Adjusted R-squared: {results['adj_r_squared']:.4f}\")\n",
    "    print(f\"F-statistic: {results['f_stat']:.4f} (p-value: {results['f_pvalue']:.4f})\")\n",
    "    print(f\"Observations: {int(results['n_obs'])}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_full_regression(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Run full regression with sentiment and control variables\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FULL REGRESSION: SENTIMENT + CONTROLS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Define variables\n",
    "    y = df['car']\n",
    "    \n",
    "    # Sentiment variables (main variables of interest)\n",
    "    sentiment_vars = ['negative_ratio', 'positive_ratio', 'uncertainty_ratio']\n",
    "    \n",
    "    # Control variables\n",
    "    control_vars = ['volatility_combined', 'log_market_cap', 'financial_crisis', 'covid_period']\n",
    "    \n",
    "    # Add past performance if available\n",
    "    if 'past_car' in df.columns and not df['past_car'].isna().all():\n",
    "        control_vars.append('past_car')\n",
    "    \n",
    "    # Industry dummies\n",
    "    industry_vars = [col for col in df.columns if col.startswith('industry_') and col != 'industry_other']\n",
    "    \n",
    "    # Combine all variables\n",
    "    X_vars = sentiment_vars + control_vars + industry_vars\n",
    "    X = df[X_vars].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Add constant\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Run regression\n",
    "    model = sm.OLS(y, X).fit(cov_type='HC3')  # Robust standard errors\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'model': model,\n",
    "        'r_squared': model.rsquared,\n",
    "        'adj_r_squared': model.rsquared_adj,\n",
    "        'f_stat': model.fvalue,\n",
    "        'f_pvalue': model.f_pvalue,\n",
    "        'n_obs': model.nobs,\n",
    "        'sentiment_vars': sentiment_vars,\n",
    "        'control_vars': control_vars,\n",
    "        'industry_vars': industry_vars\n",
    "    }\n",
    "    \n",
    "    print(f\"R-squared: {results['r_squared']:.4f}\")\n",
    "    print(f\"Adjusted R-squared: {results['adj_r_squared']:.4f}\")\n",
    "    print(f\"F-statistic: {results['f_stat']:.4f} (p-value: {results['f_pvalue']:.4f})\")\n",
    "    print(f\"Observations: {int(results['n_obs'])}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_robustness_tests(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Run robustness tests with different specifications\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ROBUSTNESS TESTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    robustness_results = {}\n",
    "    y = df['car']\n",
    "    \n",
    "    # Test 1: Net sentiment measure only\n",
    "    X1 = df[['net_sentiment', 'uncertainty_ratio', 'volatility_combined', 'log_market_cap']]\n",
    "    X1 = X1.fillna(X1.mean())\n",
    "    X1 = sm.add_constant(X1)\n",
    "    model1 = sm.OLS(y, X1).fit(cov_type='HC3')\n",
    "    robustness_results['net_sentiment_only'] = model1\n",
    "    \n",
    "    # Test 2: Sentiment polarity measure only  \n",
    "    X2 = df[['sentiment_polarity', 'uncertainty_ratio', 'volatility_combined', 'log_market_cap']]\n",
    "    X2 = X2.fillna(X2.mean())\n",
    "    X2 = sm.add_constant(X2)\n",
    "    model2 = sm.OLS(y, X2).fit(cov_type='HC3')\n",
    "    robustness_results['sentiment_polarity_only'] = model2\n",
    "    \n",
    "    # Test 3: Binary sentiment measure\n",
    "    df_temp = df.copy()\n",
    "    df_temp['high_negative'] = (df_temp['negative_ratio'] > df_temp['negative_ratio'].median()).astype(int)\n",
    "    df_temp['high_positive'] = (df_temp['positive_ratio'] > df_temp['positive_ratio'].median()).astype(int)\n",
    "    X3 = df_temp[['high_negative', 'high_positive', 'uncertainty_ratio', \n",
    "            'volatility_combined', 'log_market_cap']]\n",
    "    X3 = X3.fillna(X3.mean())\n",
    "    X3 = sm.add_constant(X3)\n",
    "    model3 = sm.OLS(y, X3).fit(cov_type='HC3')\n",
    "    robustness_results['polarity'] = model3\n",
    "    \n",
    "    print(\" Completed robustness tests:\")\n",
    "    print(f\"  - Net sentiment R²: {model1.rsquared:.4f}\")\n",
    "    print(f\"  - Sentiment polarity R²: {model2.rsquared:.4f}\")\n",
    "    print(f\"  - Binary sentiment R²: {model3.rsquared:.4f}\")\n",
    "    \n",
    "    return robustness_results\n",
    "\n",
    "def create_regression_table(baseline_results: dict, full_results: dict, \n",
    "                           robustness_results: dict, output_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"Create a comprehensive regression results table\"\"\"\n",
    "    \n",
    "    print(\"\\nCreating regression results table...\")\n",
    "    \n",
    "    # Extract coefficients and statistics\n",
    "    models = {\n",
    "        'Baseline': baseline_results['model'],\n",
    "        'Full Model': full_results['model'],\n",
    "        'Net Sentiment': robustness_results['net_sentiment_only'],\n",
    "        'Sentiment Polarity': robustness_results['sentiment_polarity_only'],\n",
    "            'Binary Sentiment': robustness_results['polarity']\n",
    "    }\n",
    "    \n",
    "    # Create results table\n",
    "    results_data = []\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model_data = {\n",
    "            'Model': model_name,\n",
    "            'R²': f\"{model.rsquared:.4f}\",\n",
    "            'Adj. R²': f\"{model.rsquared_adj:.4f}\",\n",
    "            'F-stat': f\"{model.fvalue:.2f}\",\n",
    "            'N': int(model.nobs)\n",
    "        }\n",
    "        \n",
    "        # Add key coefficients\n",
    "        params = model.params\n",
    "        pvalues = model.pvalues\n",
    "        \n",
    "        for var in ['negative_ratio', 'positive_ratio', 'uncertainty_ratio']:\n",
    "            if var in params.index:\n",
    "                coef = params[var]\n",
    "                pval = pvalues[var]\n",
    "                significance = '***' if pval < 0.01 else '**' if pval < 0.05 else '*' if pval < 0.1 else ''\n",
    "                model_data[var] = f\"{coef:.4f}{significance}\"\n",
    "            else:\n",
    "                model_data[var] = '-'\n",
    "        \n",
    "        results_data.append(model_data)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Save results table\n",
    "    table_path = os.path.join(output_dir, \"Regression_Results_Table.csv\")\n",
    "    results_df.to_csv(table_path, index=False)\n",
    "    \n",
    "    print(f\" Regression table saved: {table_path}\")\n",
    "    return results_df\n",
    "\n",
    "def create_visualizations(df: pd.DataFrame, baseline_results: dict, full_results: dict, figure_dir: str = \"exports/figures\"):\n",
    "    \"\"\"Create regression visualization plots\"\"\"\n",
    "    \n",
    "    print(\"\\nCreating visualization plots...\")\n",
    "    \n",
    "    # Set up plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Figure 1: Sentiment vs CAR scatter plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Sentiment Measures vs Cumulative Abnormal Returns', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Negative sentiment\n",
    "    sns.scatterplot(data=df, x='negative_ratio', y='car', alpha=0.6, ax=axes[0,0])\n",
    "    sns.regplot(data=df, x='negative_ratio', y='car', scatter=False, color='red', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Negative Sentiment vs CAR')\n",
    "    axes[0,0].set_xlabel('Negative Sentiment Ratio')\n",
    "    axes[0,0].set_ylabel('CAR (-1,+1)')\n",
    "    \n",
    "    # Positive sentiment  \n",
    "    sns.scatterplot(data=df, x='positive_ratio', y='car', alpha=0.6, ax=axes[0,1])\n",
    "    sns.regplot(data=df, x='positive_ratio', y='car', scatter=False, color='green', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Positive Sentiment vs CAR')\n",
    "    axes[0,1].set_xlabel('Positive Sentiment Ratio')\n",
    "    axes[0,1].set_ylabel('CAR (-1,+1)')\n",
    "    \n",
    "    # Uncertainty\n",
    "    sns.scatterplot(data=df, x='uncertainty_ratio', y='car', alpha=0.6, ax=axes[1,0])\n",
    "    sns.regplot(data=df, x='uncertainty_ratio', y='car', scatter=False, color='orange', ax=axes[1,0])\n",
    "    axes[1,0].set_title('Uncertainty vs CAR')\n",
    "    axes[1,0].set_xlabel('Uncertainty Ratio')\n",
    "    axes[1,0].set_ylabel('CAR (-1,+1)')\n",
    "    \n",
    "    # Sentiment polarity\n",
    "    sns.scatterplot(data=df, x='sentiment_polarity', y='car', alpha=0.6, ax=axes[1,1])\n",
    "    sns.regplot(data=df, x='sentiment_polarity', y='car', scatter=False, color='purple', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Sentiment Polarity vs CAR')\n",
    "    axes[1,1].set_xlabel('Sentiment Polarity')\n",
    "    axes[1,1].set_ylabel('CAR (-1,+1)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs(figure_dir, exist_ok=True)\n",
    "    scatter_path = os.path.join(figure_dir, \"Sentiment_CAR_Scatterplots.png\")\n",
    "    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Figure 2: Residual plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Regression Diagnostics', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Residuals vs fitted\n",
    "    fitted = full_results['model'].fittedvalues\n",
    "    residuals = full_results['model'].resid\n",
    "    \n",
    "    axes[0].scatter(fitted, residuals, alpha=0.6)\n",
    "    axes[0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0].set_xlabel('Fitted Values')\n",
    "    axes[0].set_ylabel('Residuals')\n",
    "    axes[0].set_title('Residuals vs Fitted Values')\n",
    "    \n",
    "    # Q-Q plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1])\n",
    "    axes[1].set_title('Q-Q Plot of Residuals')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    diagnostics_path = os.path.join(figure_dir, \"Regression_Diagnostics.png\")\n",
    "    plt.savefig(diagnostics_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\" Plots saved:\")\n",
    "    print(f\"  - {scatter_path}\")\n",
    "    print(f\"  - {diagnostics_path}\")\n",
    "\n",
    "def save_detailed_results(baseline_results: dict, full_results: dict, \n",
    "                         robustness_results: dict, results_table: pd.DataFrame, output_dir: str = \"exports/tables\"):\n",
    "    \"\"\"Save detailed regression results to Excel\"\"\"\n",
    "    \n",
    "    excel_path = os.path.join(output_dir, \"Complete_Regression_Analysis.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # Results summary table\n",
    "        results_table.to_excel(writer, sheet_name='Results_Summary', index=False)\n",
    "        \n",
    "        # Detailed coefficients for main model\n",
    "        full_model = full_results['model']\n",
    "        coef_table = pd.DataFrame({\n",
    "            'Variable': full_model.params.index,\n",
    "            'Coefficient': full_model.params.values,\n",
    "            'Std_Error': full_model.bse.values,\n",
    "            't_statistic': full_model.tvalues.values,\n",
    "            'p_value': full_model.pvalues.values,\n",
    "            'CI_lower': full_model.conf_int()[0].values,\n",
    "            'CI_upper': full_model.conf_int()[1].values\n",
    "        })\n",
    "        coef_table.to_excel(writer, sheet_name='Detailed_Coefficients', index=False)\n",
    "        \n",
    "        # Model comparison\n",
    "        comparison_data = []\n",
    "        models = {'Baseline': baseline_results, 'Full': full_results}\n",
    "        \n",
    "        for name, results in models.items():\n",
    "            model = results['model']\n",
    "            comparison_data.append({\n",
    "                'Model': name,\n",
    "                'R_squared': model.rsquared,\n",
    "                'Adj_R_squared': model.rsquared_adj,\n",
    "                'F_statistic': model.fvalue,\n",
    "                'F_p_value': model.f_pvalue,\n",
    "                'AIC': model.aic,\n",
    "                'BIC': model.bic,\n",
    "                'Log_Likelihood': model.llf,\n",
    "                'Observations': int(model.nobs)\n",
    "            })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_df.to_excel(writer, sheet_name='Model_Comparison', index=False)\n",
    "    \n",
    "    print(f\" Detailed results saved: {excel_path}\")\n",
    "\n",
    "def print_interpretation(full_results: dict):\n",
    "    \"\"\"Print interpretation of results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"REGRESSION RESULTS INTERPRETATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    model = full_results['model']\n",
    "    params = model.params\n",
    "    pvalues = model.pvalues\n",
    "    \n",
    "    print(f\"Model explains {model.rsquared*100:.2f}% of CAR variation\")\n",
    "    print(f\"F-test p-value: {model.f_pvalue:.4f} {'(Significant)' if model.f_pvalue < 0.05 else '(Not significant)'}\")\n",
    "    \n",
    "    print(f\"\\nKey Findings:\")\n",
    "    \n",
    "    # Negative sentiment\n",
    "    if 'negative_ratio' in params.index:\n",
    "        coef = params['negative_ratio']\n",
    "        pval = pvalues['negative_ratio']\n",
    "        sig = \"***\" if pval < 0.01 else \"**\" if pval < 0.05 else \"*\" if pval < 0.1 else \"\"\n",
    "        direction = \"increases\" if coef > 0 else \"decreases\"\n",
    "        print(f\"• Negative sentiment {direction} CAR by {abs(coef):.4f} {sig}\")\n",
    "        if sig:\n",
    "            print(f\"  → 1 percentage point increase in negative sentiment leads to {coef*100:.2f} percentage point change in CAR\")\n",
    "    \n",
    "    # Positive sentiment\n",
    "    if 'positive_ratio' in params.index:\n",
    "        coef = params['positive_ratio']\n",
    "        pval = pvalues['positive_ratio'] \n",
    "        sig = \"***\" if pval < 0.01 else \"**\" if pval < 0.05 else \"*\" if pval < 0.1 else \"\"\n",
    "        direction = \"increases\" if coef > 0 else \"decreases\"\n",
    "        print(f\"• Positive sentiment {direction} CAR by {abs(coef):.4f} {sig}\")\n",
    "        if sig:\n",
    "            print(f\"  → 1 percentage point increase in positive sentiment leads to {coef*100:.2f} percentage point change in CAR\")\n",
    "    \n",
    "    # Uncertainty\n",
    "    if 'uncertainty_ratio' in params.index:\n",
    "        coef = params['uncertainty_ratio']\n",
    "        pval = pvalues['uncertainty_ratio']\n",
    "        direction = \"increases\" if coef > 0 else \"decreases\" \n",
    "        print(f\"• Uncertainty {direction} CAR by {abs(coef):.4f} {sig}\")\n",
    "        if sig:\n",
    "            print(f\"  → 1 percentage point increase in uncertainty leads to {coef*100:.2f} percentage point change in CAR\")\n",
    "    \n",
    "    print(f\"\\nStatistical significance: *** p<0.01, ** p<0.05, * p<0.1\")\n",
    "\n",
    "\n",
    "def run_regressions_for_all_car_windows(df: pd.DataFrame, output_dir: str = \"exports/tables\") -> dict:\n",
    "    \"\"\"\n",
    "    Run regression analysis for all available CAR windows\n",
    "    Returns results for each CAR window\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RUNNING REGRESSIONS FOR ALL CAR WINDOWS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Identify all available CAR windows\n",
    "    car_windows = [col for col in df.columns if col.startswith('car_') and col.count('_') == 2]\n",
    "    \n",
    "    if not car_windows:\n",
    "        print(\"  No CAR windows found in dataset!\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\" Found {len(car_windows)} CAR windows: {', '.join([c.upper() for c in car_windows])}\\n\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for car_col in car_windows:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ANALYZING {car_col.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create temporary df with this CAR as the primary measure\n",
    "        df_temp = df.copy()\n",
    "        df_temp['car'] = df_temp[car_col]\n",
    "        \n",
    "        # Get corresponding t-stat, alpha, beta, r_squared\n",
    "        car_tstat_col = f\"{car_col}_tstat\"\n",
    "        car_alpha_col = f\"{car_col}_alpha\"\n",
    "        car_beta_col = f\"{car_col}_beta\"\n",
    "        car_rsquared_col = f\"{car_col}_rsquared\"\n",
    "        \n",
    "        if car_tstat_col in df_temp.columns:\n",
    "            df_temp['car_tstat'] = df_temp[car_tstat_col]\n",
    "        if car_alpha_col in df_temp.columns:\n",
    "            df_temp['alpha'] = df_temp[car_alpha_col]\n",
    "        if car_beta_col in df_temp.columns:\n",
    "            df_temp['beta'] = df_temp[car_beta_col]\n",
    "        if car_rsquared_col in df_temp.columns:\n",
    "            df_temp['r_squared'] = df_temp[car_rsquared_col]\n",
    "        \n",
    "        # Prepare data\n",
    "        df_clean = prepare_regression_data(df_temp)\n",
    "        \n",
    "        # Run regressions\n",
    "        try:\n",
    "            baseline_results = run_baseline_regression(df_clean)\n",
    "            full_results = run_full_regression(df_clean)\n",
    "            robustness_results = run_robustness_tests(df_clean)\n",
    "            \n",
    "            all_results[car_col] = {\n",
    "                'baseline': baseline_results,\n",
    "                'full': full_results,\n",
    "                'robustness': robustness_results,\n",
    "                'car_window': car_col\n",
    "            }\n",
    "            \n",
    "            # Print key results\n",
    "            model = full_results['model']\n",
    "            print(f\"\\n {car_col.upper()} Results:\")\n",
    "            print(f\"   R²: {model.rsquared:.4f}\")\n",
    "            print(f\"   Adj. R²: {model.rsquared_adj:.4f}\")\n",
    "            print(f\"   F-statistic: {model.fvalue:.4f} (p={model.f_pvalue:.4f})\")\n",
    "            print(f\"   N: {int(model.nobs)}\")\n",
    "            \n",
    "            # Key coefficients\n",
    "            if 'positive_ratio' in model.params:\n",
    "                coef = model.params['positive_ratio']\n",
    "                pval = model.pvalues['positive_ratio']\n",
    "                sig = \"***\" if pval < 0.01 else \"**\" if pval < 0.05 else \"*\" if pval < 0.1 else \"\"\n",
    "                print(f\"   Positive sentiment: {coef:.4f} {sig}\")\n",
    "            \n",
    "            if 'negative_ratio' in model.params:\n",
    "                coef = model.params['negative_ratio']\n",
    "                pval = model.pvalues['negative_ratio']\n",
    "                sig = \"***\" if pval < 0.01 else \"**\" if pval < 0.05 else \"*\" if pval < 0.1 else \"\"\n",
    "                print(f\"   Negative sentiment: {coef:.4f} {sig}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\" Error analyzing {car_col}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Save comparative results\n",
    "    if all_results:\n",
    "        save_multi_car_results(all_results, output_dir)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def save_multi_car_results(all_results: dict, output_dir: str = \"exports/tables\"):\n",
    "    \"\"\"Save comparative results for all CAR windows\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVING MULTI-CAR WINDOW RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    \n",
    "    for car_window, results in all_results.items():\n",
    "        model = results['full']['model']\n",
    "        \n",
    "        row = {\n",
    "            'CAR_Window': car_window.upper(),\n",
    "            'R_Squared': model.rsquared,\n",
    "            'Adj_R_Squared': model.rsquared_adj,\n",
    "            'F_Statistic': model.fvalue,\n",
    "            'F_PValue': model.f_pvalue,\n",
    "            'N_Obs': int(model.nobs)\n",
    "        }\n",
    "        \n",
    "        # Add coefficients\n",
    "        for var in ['positive_ratio', 'negative_ratio', 'uncertainty_ratio']:\n",
    "            if var in model.params:\n",
    "                row[f'{var}_coef'] = model.params[var]\n",
    "                row[f'{var}_pval'] = model.pvalues[var]\n",
    "                row[f'{var}_sig'] = \"***\" if model.pvalues[var] < 0.01 else \"**\" if model.pvalues[var] < 0.05 else \"*\" if model.pvalues[var] < 0.1 else \"\"\n",
    "        \n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Save to Excel\n",
    "    excel_path = os.path.join(output_dir, \"Multi_CAR_Window_Comparison.xlsx\")\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        comparison_df.to_excel(writer, sheet_name='Comparison', index=False)\n",
    "        \n",
    "        # Add detailed results for each window\n",
    "        for car_window, results in all_results.items():\n",
    "            sheet_name = car_window.replace('car_', 'CAR_').replace('_', ',')[:31]  # Excel limit\n",
    "            model_summary = results['full']['model'].summary2().tables[1]\n",
    "            model_summary.to_excel(writer, sheet_name=sheet_name)\n",
    "    \n",
    "    print(f\" Saved multi-CAR comparison to: {excel_path}\")\n",
    "    \n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(output_dir, \"Multi_CAR_Window_Comparison.csv\")\n",
    "    comparison_df.to_csv(csv_path, index=False)\n",
    "    print(f\" Saved comparison table to: {csv_path}\")\n",
    "\n",
    "\n",
    "def step7_regression_analysis():\n",
    "    \"\"\"Main regression analysis function - supports multiple CAR windows\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"REGRESSION ANALYSIS: SENTIMENT IMPACT ON CAR\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        df = load_final_dataset()\n",
    "        \n",
    "        # Check for multiple CAR windows\n",
    "        car_windows = [col for col in df.columns if col.startswith('car_') and col.count('_') == 2]\n",
    "        \n",
    "        if len(car_windows) > 1:\n",
    "            print(f\"\\n Multiple CAR windows detected: {', '.join([c.upper() for c in car_windows])}\")\n",
    "            print(\"Running analysis for ALL CAR windows...\\n\")\n",
    "            \n",
    "            # Run regressions for all CAR windows\n",
    "            all_car_results = run_regressions_for_all_car_windows(df)\n",
    "            \n",
    "            # Also run for primary CAR (backward compatibility)\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"PRIMARY CAR ANALYSIS (CAR column)\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "        df_clean = prepare_regression_data(df)\n",
    "        \n",
    "        # Run regressions on primary CAR\n",
    "        baseline_results = run_baseline_regression(df_clean)\n",
    "        full_results = run_full_regression(df_clean)\n",
    "        robustness_results = run_robustness_tests(df_clean)\n",
    "        \n",
    "        # Create results table\n",
    "        results_table = create_regression_table(baseline_results, full_results, robustness_results)\n",
    "        \n",
    "        # Create visualizations\n",
    "        create_visualizations(df_clean, baseline_results, full_results)\n",
    "        \n",
    "        # Save detailed results\n",
    "        save_detailed_results(baseline_results, full_results, robustness_results, results_table)\n",
    "        \n",
    "        # Print interpretation\n",
    "        print_interpretation(full_results)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\" REGRESSION ANALYSIS COMPLETED!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Files created:\")\n",
    "        print(\"• Complete_Regression_Analysis.xlsx\")\n",
    "        print(\"• Regression_Results_Table.csv\")\n",
    "        print(\"• Sentiment_CAR_Scatterplots.png\")\n",
    "        print(\"• Regression_Diagnostics.png\")\n",
    "        \n",
    "        if len(car_windows) > 1:\n",
    "            print(\"• Multi_CAR_Window_Comparison.xlsx\")\n",
    "            print(\"• Multi_CAR_Window_Comparison.csv\")\n",
    "        \n",
    "        print(f\"\\n Ready for report writing!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error in regression analysis: {str(e)}\")\n",
    "        print(\"Please ensure the final dataset with controls exists.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "step7_regression_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a9d4c",
   "metadata": {},
   "source": [
    "# Step 8: Dashboard Visualization\n",
    "\n",
    "**Input Files:**\n",
    "- `exports/tables/Final_Regression_Dataset.csv` (from Step 6)\n",
    "- `exports/tables/Complete_Regression_Analysis.xlsx` (from Step 7)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/figures/Analysis_Dashboard.png` (comprehensive 6-panel dashboard)\n",
    "\n",
    "**Dashboard Panels:**\n",
    "1. CAR by sentiment score (all 3 CAR windows)\n",
    "2. Sentiment score distribution\n",
    "3. Sentiment polarity vs CAR\n",
    "4. CAR distribution by industry\n",
    "5. Market cap vs CAR\n",
    "6. Residual diagnostics\n",
    "This script analyzes sentiment-CAR relationships for each company individually\n",
    "and creates visualizations to show company-specific patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e36effb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPANY-SPECIFIC SENTIMENT ANALYSIS\n",
      "================================================================================\n",
      " Loaded data: 4121 observations for 58 companies\n",
      " Analyzed 58 companies with sufficient data\n",
      "\n",
      "Creating company-specific visualizations...\n",
      " Creating individual company figures (3 plots each: Positive, Negative, Uncertainty)...\n",
      "    Saved: TD_Sentiment_Analysis.png\n",
      "    Saved: MDT_Sentiment_Analysis.png\n",
      "    Saved: XOM_Sentiment_Analysis.png\n",
      "    Saved: SAP_Sentiment_Analysis.png\n",
      "    Saved: BAC_Sentiment_Analysis.png\n",
      "    Saved: LMT_Sentiment_Analysis.png\n",
      "    Saved: ABT_Sentiment_Analysis.png\n",
      "\n",
      "  Created 7 individual company plots\n",
      " Location: exports/figures\\company_specific\n",
      " Company summary plot saved: exports/figures\\Company_Analysis_Summary.png\n",
      " Company results saved:\n",
      "  - exports/tables\\Company_Specific_Analysis.csv\n",
      "  - exports/tables\\Company_Specific_Analysis.xlsx\n",
      "\n",
      "================================================================================\n",
      "COMPANY-SPECIFIC SENTIMENT ANALYSIS RESULTS\n",
      "================================================================================\n",
      "Total companies analyzed: 58\n",
      "Companies with significant POSITIVE sentiment effect: 3 (5.2%)\n",
      "Companies with significant NEGATIVE sentiment effect: 0 (0.0%)\n",
      "\n",
      " TOP 5 COMPANIES - Strongest Positive Sentiment Effects:\n",
      "  ABT (Healthcare): r=0.004 (n=66)\n",
      "  ADBE (Technology): r=0.042 (n=66)\n",
      "  ASML (Other): r=0.073 (n=67)\n",
      "  BA (Industrial): r=0.144 (n=80)\n",
      "  BAC (Financial): r=0.203 (n=75)\n",
      "\n",
      " BOTTOM 5 COMPANIES - Most Negative Sentiment Effects:\n",
      "  UNP (Industrial): r=-0.215 (n=59)\n",
      "  MA (Financial): r=-0.159 (n=99)\n",
      "  JPM (Financial): r=-0.141 (n=91)\n",
      "  BKNG (Other): r=-0.129 (n=61)\n",
      "  GS (Financial): r=-0.108 (n=68)\n",
      "\n",
      " INDUSTRY PATTERNS:\n",
      "  Consumer: Avg r=0.019, 0.0/6.0 significant (0.0%)\n",
      "  Energy: Avg r=0.231, 1.0/1.0 significant (100.0%)\n",
      "  Financial: Avg r=-0.032, 0.0/9.0 significant (0.0%)\n",
      "  Healthcare: Avg r=0.040, 1.0/9.0 significant (11.1%)\n",
      "  Industrial: Avg r=0.043, 0.0/6.0 significant (0.0%)\n",
      "  Other: Avg r=0.065, 1.0/16.0 significant (6.2%)\n",
      "  Technology: Avg r=0.057, 0.0/9.0 significant (0.0%)\n",
      "  Telecom: Avg r=-0.026, 0.0/2.0 significant (0.0%)\n",
      "\n",
      "================================================================================\n",
      " COMPANY-SPECIFIC ANALYSIS COMPLETED!\n",
      "================================================================================\n",
      "Files created:\n",
      "• ['exports/figures\\\\company_specific\\\\TD_Sentiment_Analysis.png', 'exports/figures\\\\company_specific\\\\MDT_Sentiment_Analysis.png', 'exports/figures\\\\company_specific\\\\XOM_Sentiment_Analysis.png', 'exports/figures\\\\company_specific\\\\SAP_Sentiment_Analysis.png', 'exports/figures\\\\company_specific\\\\BAC_Sentiment_Analysis.png', 'exports/figures\\\\company_specific\\\\LMT_Sentiment_Analysis.png', 'exports/figures\\\\company_specific\\\\ABT_Sentiment_Analysis.png']\n",
      "• exports/figures\\Company_Analysis_Summary.png\n",
      "• Company_Specific_Analysis.csv\n",
      "• Company_Specific_Analysis.xlsx\n",
      "\n",
      "Now you can see which specific companies drive your results! 🎯\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_company_data(data_dir: str = \"exports/tables\") -> pd.DataFrame:\n",
    "    \"\"\"Load the final regression dataset\"\"\"\n",
    "    data_path = os.path.join(data_dir, \"Final_Regression_Dataset.csv\")\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Dataset not found: {data_path}\")\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.dropna(subset=['car', 'positive_ratio', 'negative_ratio'])\n",
    "    \n",
    "    print(f\" Loaded data: {len(df)} observations for {df['ticker'].nunique()} companies\")\n",
    "    return df\n",
    "\n",
    "def analyze_company_correlations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate correlations for each company\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for ticker in sorted(df['ticker'].unique()):\n",
    "        company_data = df[df['ticker'] == ticker].copy()\n",
    "        \n",
    "        if len(company_data) < 5:  # Need minimum observations\n",
    "            continue\n",
    "        \n",
    "        # Calculate correlations\n",
    "        pos_corr = company_data['positive_ratio'].corr(company_data['car'])\n",
    "        neg_corr = company_data['negative_ratio'].corr(company_data['car'])\n",
    "        unc_corr = company_data['uncertainty_ratio'].corr(company_data['car'])\n",
    "        \n",
    "        # Calculate regression coefficients\n",
    "        try:\n",
    "            from scipy import stats\n",
    "            pos_slope, pos_intercept, pos_r_value, pos_p_value, pos_std_err = stats.linregress(\n",
    "                company_data['positive_ratio'], company_data['car'])\n",
    "            \n",
    "            neg_slope, neg_intercept, neg_r_value, neg_p_value, neg_std_err = stats.linregress(\n",
    "                company_data['negative_ratio'], company_data['car'])\n",
    "            \n",
    "        except:\n",
    "            pos_slope = pos_p_value = neg_slope = neg_p_value = np.nan\n",
    "        \n",
    "        # Get company info\n",
    "        avg_market_cap = company_data['market_cap_billions'].mean()\n",
    "        industry = get_company_industry(ticker)\n",
    "        \n",
    "        results.append({\n",
    "            'ticker': ticker,\n",
    "            'n_observations': len(company_data),\n",
    "            'industry': industry,\n",
    "            'avg_market_cap_b': avg_market_cap,\n",
    "            'pos_correlation': pos_corr,\n",
    "            'neg_correlation': neg_corr,\n",
    "            'unc_correlation': unc_corr,\n",
    "            'pos_slope': pos_slope,\n",
    "            'neg_slope': neg_slope,\n",
    "            'pos_p_value': pos_p_value,\n",
    "            'neg_p_value': neg_p_value,\n",
    "            'mean_car': company_data['car'].mean(),\n",
    "            'std_car': company_data['car'].std(),\n",
    "            'mean_pos_sentiment': company_data['positive_ratio'].mean(),\n",
    "            'std_pos_sentiment': company_data['positive_ratio'].std()\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add significance indicators\n",
    "    results_df['pos_significant'] = results_df['pos_p_value'] < 0.05\n",
    "    results_df['neg_significant'] = results_df['neg_p_value'] < 0.05\n",
    "    \n",
    "    print(f\" Analyzed {len(results_df)} companies with sufficient data\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def get_company_industry(ticker: str) -> str:\n",
    "    \"\"\"Classify company by industry\"\"\"\n",
    "    industries = {\n",
    "        'Technology': ['MSFT', 'GOOG', 'FB', 'INTC', 'ADBE', 'CRM', 'INTU', 'TSLA', 'ORCL', 'NFLX'],\n",
    "        'Financial': ['BAC', 'JPM', 'MS', 'GS', 'AXP', 'BLK', 'V', 'MA', 'MCO'],\n",
    "        'Healthcare': ['JNJ', 'ABT', 'LLY', 'AMGN', 'BMY', 'MRK', 'TMO', 'ISRG', 'MDT', 'UNH'],\n",
    "        'Energy': ['XOM'],\n",
    "        'Consumer': ['MCD', 'NKE', 'COST', 'HD', 'PG', 'PEP'],\n",
    "        'Industrial': ['BA', 'HON', 'UNP', 'LMT', 'FDX', 'UPS'],\n",
    "        'Telecom': ['T', 'VZ', 'TMUS']\n",
    "    }\n",
    "    \n",
    "    for industry, tickers in industries.items():\n",
    "        if ticker in tickers:\n",
    "            return industry\n",
    "    return 'Other'\n",
    "\n",
    "def create_company_grid_plot(df: pd.DataFrame, results_df: pd.DataFrame, figure_dir: str = \"exports/figures\"):\n",
    "        \"\"\"Create SEPARATE figures for each company with 3 plots: positive, negative, uncertainty vs CAR\"\"\"\n",
    "        \n",
    "        # Select top companies by positive correlation and significance\n",
    "        top_pos_companies = results_df[\n",
    "            (results_df['n_observations'] >= 10) & \n",
    "            (results_df['pos_significant'] == True)\n",
    "        ].nlargest(6, 'pos_correlation')\n",
    "        \n",
    "        # Select top companies by negative correlation and significance  \n",
    "        top_neg_companies = results_df[\n",
    "            (results_df['n_observations'] >= 10) & \n",
    "            (results_df['neg_significant'] == True)\n",
    "        ].nsmallest(6, 'neg_correlation')\n",
    "        \n",
    "        # If not enough significant companies, use top correlations regardless\n",
    "        if len(top_pos_companies) < 3:\n",
    "            print(\" Not enough companies with significant positive relationships\")\n",
    "            top_pos_companies = results_df[results_df['n_observations'] >= 10].nlargest(6, 'pos_correlation')\n",
    "            \n",
    "        if len(top_neg_companies) < 3:\n",
    "            print(\" Not enough companies with significant negative relationships\")\n",
    "            top_neg_companies = results_df[results_df['n_observations'] >= 10].nsmallest(6, 'neg_correlation')\n",
    "        \n",
    "        # Combine for display\n",
    "        top_companies = pd.concat([top_pos_companies, top_neg_companies]).drop_duplicates()\n",
    "        \n",
    "        # Create separate directory for individual company plots\n",
    "        company_plots_dir = os.path.join(figure_dir, \"company_specific\")\n",
    "        os.makedirs(company_plots_dir, exist_ok=True)\n",
    "        \n",
    "        saved_files = []\n",
    "        \n",
    "        print(f\" Creating individual company figures (3 plots each: Positive, Negative, Uncertainty)...\")\n",
    "        \n",
    "        # Create SEPARATE figure for EACH company with 3 subplots\n",
    "        for i, (_, company_info) in enumerate(top_companies.iterrows()):\n",
    "            ticker = company_info['ticker']\n",
    "            company_data = df[df['ticker'] == ticker].copy()\n",
    "            \n",
    "            # Create figure with 3 subplots (1 row, 3 columns)\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "            \n",
    "            # ===== PLOT 1: POSITIVE SENTIMENT =====\n",
    "            ax_pos = axes[0]\n",
    "            ax_pos.scatter(company_data['positive_ratio'], \n",
    "                          company_data['car'], \n",
    "                          alpha=0.6, s=50, color='#2E7D32', edgecolors='white', linewidth=0.5, zorder=2)\n",
    "            \n",
    "            # Positive sentiment trend line\n",
    "            if len(company_data) > 1:\n",
    "                valid_mask = company_data['positive_ratio'].notna() & company_data['car'].notna()\n",
    "                if valid_mask.sum() > 1:\n",
    "                    z = np.polyfit(company_data.loc[valid_mask, 'positive_ratio'], \n",
    "                                  company_data.loc[valid_mask, 'car'], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_line = np.linspace(company_data['positive_ratio'].min(), \n",
    "                                       company_data['positive_ratio'].max(), 100)\n",
    "                    ax_pos.plot(x_line, p(x_line), color='black', linestyle='--', \n",
    "                               linewidth=3, dashes=(5, 3), alpha=0.8, label='Trend Line', zorder=3)\n",
    "            \n",
    "            # Positive formatting\n",
    "            pos_correlation = company_info['pos_correlation']\n",
    "            pos_p_value = company_info['pos_p_value']\n",
    "            pos_significance = \"***\" if pos_p_value < 0.001 else \"**\" if pos_p_value < 0.01 else \"*\" if pos_p_value < 0.05 else \"\"\n",
    "            \n",
    "            ax_pos.set_title(f'POSITIVE Sentiment\\nr={pos_correlation:.3f}{pos_significance}', \n",
    "                            fontsize=12, fontweight='bold', color='#2E7D32')\n",
    "            ax_pos.set_xlabel('Positive Sentiment Ratio', fontsize=11, fontweight='bold')\n",
    "            ax_pos.set_ylabel('CAR (-1,+1)', fontsize=11, fontweight='bold')\n",
    "            ax_pos.grid(True, alpha=0.3, zorder=1)\n",
    "            ax_pos.axhline(y=0, color='gray', linestyle='-', linewidth=0.8, alpha=0.5)\n",
    "            if len(company_data) > 1:\n",
    "                ax_pos.legend(loc='best', framealpha=0.9)\n",
    "            \n",
    "            # ===== PLOT 2: NEGATIVE SENTIMENT =====\n",
    "            ax_neg = axes[1]\n",
    "            ax_neg.scatter(company_data['negative_ratio'], \n",
    "                          company_data['car'], \n",
    "                          alpha=0.6, s=50, color='#C62828', edgecolors='white', linewidth=0.5, zorder=2)\n",
    "            \n",
    "            # Negative sentiment trend line\n",
    "            if len(company_data) > 1:\n",
    "                valid_mask = company_data['negative_ratio'].notna() & company_data['car'].notna()\n",
    "                if valid_mask.sum() > 1:\n",
    "                    z = np.polyfit(company_data.loc[valid_mask, 'negative_ratio'], \n",
    "                                  company_data.loc[valid_mask, 'car'], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_line = np.linspace(company_data['negative_ratio'].min(), \n",
    "                                       company_data['negative_ratio'].max(), 100)\n",
    "                    ax_neg.plot(x_line, p(x_line), color='black', linestyle='--', \n",
    "                               linewidth=3, dashes=(5, 3), alpha=0.8, label='Trend Line', zorder=3)\n",
    "            \n",
    "            # Negative formatting\n",
    "            neg_correlation = company_info['neg_correlation']\n",
    "            neg_p_value = company_info['neg_p_value']\n",
    "            neg_significance = \"***\" if neg_p_value < 0.001 else \"**\" if neg_p_value < 0.01 else \"*\" if neg_p_value < 0.05 else \"\"\n",
    "            \n",
    "            ax_neg.set_title(f'NEGATIVE Sentiment\\nr={neg_correlation:.3f}{neg_significance}', \n",
    "                            fontsize=12, fontweight='bold', color='#C62828')\n",
    "            ax_neg.set_xlabel('Negative Sentiment Ratio', fontsize=11, fontweight='bold')\n",
    "            ax_neg.set_ylabel('CAR (-1,+1)', fontsize=11, fontweight='bold')\n",
    "            ax_neg.grid(True, alpha=0.3, zorder=1)\n",
    "            ax_neg.axhline(y=0, color='gray', linestyle='-', linewidth=0.8, alpha=0.5)\n",
    "            if len(company_data) > 1:\n",
    "                ax_neg.legend(loc='best', framealpha=0.9)\n",
    "            \n",
    "            # ===== PLOT 3: UNCERTAINTY =====\n",
    "            ax_unc = axes[2]\n",
    "            ax_unc.scatter(company_data['uncertainty_ratio'], \n",
    "                          company_data['car'], \n",
    "                          alpha=0.6, s=50, color='#F57C00', edgecolors='white', linewidth=0.5, zorder=2)\n",
    "            \n",
    "            # Uncertainty trend line\n",
    "            if len(company_data) > 1:\n",
    "                valid_mask = company_data['uncertainty_ratio'].notna() & company_data['car'].notna()\n",
    "                if valid_mask.sum() > 1:\n",
    "                    z = np.polyfit(company_data.loc[valid_mask, 'uncertainty_ratio'], \n",
    "                                  company_data.loc[valid_mask, 'car'], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    x_line = np.linspace(company_data['uncertainty_ratio'].min(), \n",
    "                                       company_data['uncertainty_ratio'].max(), 100)\n",
    "                    ax_unc.plot(x_line, p(x_line), color='black', linestyle='--', \n",
    "                               linewidth=3, dashes=(5, 3), alpha=0.8, label='Trend Line', zorder=3)\n",
    "            \n",
    "            # Uncertainty correlation\n",
    "            unc_correlation = company_data[['uncertainty_ratio', 'car']].corr().iloc[0, 1]\n",
    "            from scipy.stats import pearsonr\n",
    "            valid_data = company_data[['uncertainty_ratio', 'car']].dropna()\n",
    "            if len(valid_data) > 2:\n",
    "                _, unc_p_value = pearsonr(valid_data['uncertainty_ratio'], valid_data['car'])\n",
    "                unc_significance = \"***\" if unc_p_value < 0.001 else \"**\" if unc_p_value < 0.01 else \"*\" if unc_p_value < 0.05 else \"\"\n",
    "            else:\n",
    "                unc_significance = \"\"\n",
    "            \n",
    "            ax_unc.set_title(f'UNCERTAINTY\\nr={unc_correlation:.3f}{unc_significance}', \n",
    "                            fontsize=12, fontweight='bold', color='#F57C00')\n",
    "            ax_unc.set_xlabel('Uncertainty Ratio', fontsize=11, fontweight='bold')\n",
    "            ax_unc.set_ylabel('CAR (-1,+1)', fontsize=11, fontweight='bold')\n",
    "            ax_unc.grid(True, alpha=0.3, zorder=1)\n",
    "            ax_unc.axhline(y=0, color='gray', linestyle='-', linewidth=0.8, alpha=0.5)\n",
    "            if len(company_data) > 1:\n",
    "                ax_unc.legend(loc='best', framealpha=0.9)\n",
    "            \n",
    "            # Overall title\n",
    "            industry = company_info['industry']\n",
    "            n_obs = company_info['n_observations']\n",
    "            plt.suptitle(f'{ticker} - Sentiment vs CAR Analysis ({industry}, n={n_obs})', \n",
    "                        fontsize=16, fontweight='bold', y=1.02)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save individual company plot\n",
    "            company_plot_path = os.path.join(company_plots_dir, f\"{ticker}_Sentiment_Analysis.png\")\n",
    "            plt.savefig(company_plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            saved_files.append(company_plot_path)\n",
    "            print(f\"    Saved: {ticker}_Sentiment_Analysis.png\")\n",
    "        \n",
    "        print(f\"\\n  Created {len(saved_files)} individual company plots\")\n",
    "        print(f\" Location: {company_plots_dir}\")\n",
    "        \n",
    "        return saved_files\n",
    "\n",
    "def create_company_summary_plot(results_df: pd.DataFrame, figure_dir: str = \"exports/figures\"):\n",
    "        \"\"\"Create summary plots of company-level results\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Plot 1: Distribution of positive AND negative correlations by industry\n",
    "        ax1 = axes[0, 0]\n",
    "        industries = results_df['industry'].unique()\n",
    "        pos_data = []\n",
    "        neg_data = []\n",
    "        industry_labels = []\n",
    "        \n",
    "        for industry in industries:\n",
    "            industry_df = results_df[results_df['industry'] == industry]\n",
    "            if len(industry_df) > 0:\n",
    "                pos_data.append(industry_df['pos_correlation'].tolist())\n",
    "                neg_data.append(industry_df['neg_correlation'].tolist())\n",
    "                industry_labels.append(f\"{industry}\\n(n={len(industry_df)})\")\n",
    "        \n",
    "        # Create side-by-side box plots\n",
    "        positions_pos = np.arange(1, len(industries)*2, 2)\n",
    "        positions_neg = np.arange(2, len(industries)*2+1, 2)\n",
    "        \n",
    "        bp1_pos = ax1.boxplot(pos_data, positions=positions_pos, widths=0.6, \n",
    "                             patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
    "        bp1_neg = ax1.boxplot(neg_data, positions=positions_neg, widths=0.6,\n",
    "                             patch_artist=True, boxprops=dict(facecolor='lightcoral'))\n",
    "        \n",
    "        ax1.set_title('Sentiment-CAR Correlations by Industry', fontweight='bold')\n",
    "        ax1.set_ylabel('Correlation Coefficient')\n",
    "        ax1.set_xticks(np.arange(1.5, len(industries)*2, 2))\n",
    "        ax1.set_xticklabels(industry_labels, rotation=45, ha='right')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax1.legend([bp1_pos[\"boxes\"][0], bp1_neg[\"boxes\"][0]], \n",
    "                  ['Positive Sentiment', 'Negative Sentiment'], loc='upper right')\n",
    "        \n",
    "        # Plot 2: Correlation vs Market Cap\n",
    "        ax2 = axes[0, 1]\n",
    "        scatter = ax2.scatter(results_df['avg_market_cap_b'], \n",
    "                             results_df['pos_correlation'],\n",
    "                             c=results_df['n_observations'], \n",
    "                             cmap='viridis', \n",
    "                             alpha=0.7, s=60)\n",
    "        ax2.set_xlabel('Average Market Cap ($ Billions)')\n",
    "        ax2.set_ylabel('Positive Sentiment Correlation')\n",
    "        ax2.set_title('Sentiment Correlation vs Market Cap', fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax2, label='# Observations')\n",
    "        \n",
    "        # Add company labels for interesting cases\n",
    "        for _, row in results_df.iterrows():\n",
    "            if abs(row['pos_correlation']) > 0.3 or row['avg_market_cap_b'] > 500:\n",
    "                ax2.annotate(row['ticker'], \n",
    "                           (row['avg_market_cap_b'], row['pos_correlation']),\n",
    "                           xytext=(5, 5), textcoords='offset points', \n",
    "                           fontsize=8, alpha=0.8)\n",
    "        \n",
    "        # Plot 3: Significant vs Non-significant companies (both positive and negative)\n",
    "        ax3 = axes[1, 0]\n",
    "        \n",
    "        # Positive sentiment significance\n",
    "        sig_pos = results_df[results_df['pos_significant']]['pos_correlation']\n",
    "        nonsig_pos = results_df[~results_df['pos_significant']]['pos_correlation']\n",
    "        \n",
    "        # Negative sentiment significance  \n",
    "        sig_neg = results_df[results_df['neg_significant']]['neg_correlation']\n",
    "        nonsig_neg = results_df[~results_df['neg_significant']]['neg_correlation']\n",
    "        \n",
    "        # Create histogram\n",
    "        bins = np.linspace(-0.5, 0.5, 20)\n",
    "        ax3.hist([sig_pos, nonsig_pos], bins=bins, alpha=0.7, \n",
    "                label=[f'Pos Significant (n={len(sig_pos)})', f'Pos Non-sig (n={len(nonsig_pos)})'],\n",
    "                color=['darkgreen', 'lightgreen'])\n",
    "        ax3.hist([sig_neg, nonsig_neg], bins=bins, alpha=0.7, \n",
    "                label=[f'Neg Significant (n={len(sig_neg)})', f'Neg Non-sig (n={len(nonsig_neg)})'],\n",
    "                color=['darkred', 'lightcoral'])\n",
    "        \n",
    "        ax3.set_xlabel('Sentiment Correlation')\n",
    "        ax3.set_ylabel('Number of Companies')\n",
    "        ax3.set_title('Distribution of Sentiment Correlations\\n(Significant vs Non-significant)', fontweight='bold')\n",
    "        ax3.legend(fontsize=8)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Top positive and negative sentiment performers\n",
    "        ax4 = axes[1, 1]\n",
    "        top_n = 8\n",
    "        \n",
    "        # Top positive sentiment performers\n",
    "        top_pos = results_df.nlargest(top_n, 'pos_correlation')\n",
    "        # Top negative sentiment performers (most negative correlation)\n",
    "        top_neg = results_df.nsmallest(top_n, 'neg_correlation')\n",
    "        \n",
    "        y_pos_pos = np.arange(len(top_pos))\n",
    "        y_pos_neg = np.arange(len(top_neg)) - len(top_neg) - 1\n",
    "        \n",
    "        # Bars for positive sentiment\n",
    "        bars_pos = ax4.barh(y_pos_pos, top_pos['pos_correlation'], \n",
    "                           color='green', alpha=0.7, label=f'Top {top_n} Positive')\n",
    "        \n",
    "        # Bars for negative sentiment  \n",
    "        bars_neg = ax4.barh(y_pos_neg, top_neg['neg_correlation'], \n",
    "                           color='red', alpha=0.7, label=f'Top {top_n} Negative')\n",
    "        \n",
    "        # Add significance indicators\n",
    "        for i, (_, row) in enumerate(top_pos.iterrows()):\n",
    "            if row['pos_significant']:\n",
    "                ax4.text(row['pos_correlation'] + 0.01, y_pos_pos[i], '*', \n",
    "                        fontsize=16, fontweight='bold', va='center')\n",
    "        \n",
    "        for i, (_, row) in enumerate(top_neg.iterrows()):\n",
    "            if row['neg_significant']:\n",
    "                ax4.text(row['neg_correlation'] - 0.01, y_pos_neg[i], '*', \n",
    "                        fontsize=16, fontweight='bold', va='center')\n",
    "        \n",
    "        # Labels\n",
    "        ax4.set_yticks(list(y_pos_pos) + list(y_pos_neg))\n",
    "        ax4.set_yticklabels(list(top_pos['ticker']) + list(top_neg['ticker']))\n",
    "        ax4.set_xlabel('Sentiment Correlation (* = Significant)')\n",
    "        ax4.set_title(f'Top Companies by Positive & Negative Sentiment Effects', fontweight='bold')\n",
    "        ax4.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        os.makedirs(figure_dir, exist_ok=True)\n",
    "        summary_path = os.path.join(figure_dir, \"Company_Analysis_Summary.png\")\n",
    "        plt.savefig(summary_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\" Company summary plot saved: {summary_path}\")\n",
    "        \n",
    "        return summary_path\n",
    "\n",
    "def save_company_results(results_df: pd.DataFrame, data_dir: str = \"exports/tables\"):\n",
    "    \"\"\"Save detailed company-level results\"\"\"\n",
    "    \n",
    "    # Sort by positive correlation (descending)\n",
    "    results_df = results_df.sort_values('pos_correlation', ascending=False)\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = os.path.join(data_dir, \"Company_Specific_Analysis.csv\")\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save to Excel with multiple sheets\n",
    "    excel_path = os.path.join(data_dir, \"Company_Specific_Analysis.xlsx\")\n",
    "    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "        # All results\n",
    "        results_df.to_excel(writer, sheet_name='All_Companies', index=False)\n",
    "        \n",
    "        # Significant positive relationships\n",
    "        sig_positive = results_df[\n",
    "            (results_df['pos_significant']) & \n",
    "            (results_df['pos_correlation'] > 0)\n",
    "        ]\n",
    "        sig_positive.to_excel(writer, sheet_name='Significant_Positive', index=False)\n",
    "        \n",
    "        # By industry\n",
    "        for industry in results_df['industry'].unique():\n",
    "            industry_data = results_df[results_df['industry'] == industry]\n",
    "            sheet_name = f'Industry_{industry}'[:31]  # Excel sheet name limit\n",
    "            industry_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    print(f\" Company results saved:\")\n",
    "    print(f\"  - {csv_path}\")\n",
    "    print(f\"  - {excel_path}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def print_key_findings(results_df: pd.DataFrame):\n",
    "    \"\"\"Print summary of key findings\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPANY-SPECIFIC SENTIMENT ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    total_companies = len(results_df)\n",
    "    significant_positive = len(results_df[\n",
    "        (results_df['pos_significant']) & \n",
    "        (results_df['pos_correlation'] > 0)\n",
    "    ])\n",
    "    significant_negative = len(results_df[\n",
    "        (results_df['pos_significant']) & \n",
    "        (results_df['pos_correlation'] < 0)\n",
    "    ])\n",
    "    \n",
    "    print(f\"Total companies analyzed: {total_companies}\")\n",
    "    print(f\"Companies with significant POSITIVE sentiment effect: {significant_positive} ({significant_positive/total_companies*100:.1f}%)\")\n",
    "    print(f\"Companies with significant NEGATIVE sentiment effect: {significant_negative} ({significant_negative/total_companies*100:.1f}%)\")\n",
    "    \n",
    "    # Top positive correlations\n",
    "    print(f\"\\n TOP 5 COMPANIES - Strongest Positive Sentiment Effects:\")\n",
    "    top_positive = results_df[results_df['pos_correlation'] > 0].head(5)\n",
    "    for _, row in top_positive.iterrows():\n",
    "        sig = \"***\" if row['pos_p_value'] < 0.001 else \"**\" if row['pos_p_value'] < 0.01 else \"*\" if row['pos_significant'] else \"\"\n",
    "        print(f\"  {row['ticker']} ({row['industry']}): r={row['pos_correlation']:.3f}{sig} (n={row['n_observations']})\")\n",
    "    \n",
    "    # Most negative correlations\n",
    "    print(f\"\\n BOTTOM 5 COMPANIES - Most Negative Sentiment Effects:\")\n",
    "    bottom_negative = results_df.nsmallest(5, 'pos_correlation')\n",
    "    for _, row in bottom_negative.iterrows():\n",
    "        sig = \"***\" if row['pos_p_value'] < 0.001 else \"**\" if row['pos_p_value'] < 0.01 else \"*\" if row['pos_significant'] else \"\"\n",
    "        print(f\"  {row['ticker']} ({row['industry']}): r={row['pos_correlation']:.3f}{sig} (n={row['n_observations']})\")\n",
    "    \n",
    "    # Industry summary\n",
    "    print(f\"\\n INDUSTRY PATTERNS:\")\n",
    "    industry_summary = results_df.groupby('industry').agg({\n",
    "        'pos_correlation': ['mean', 'count'],\n",
    "        'pos_significant': 'sum'\n",
    "    }).round(3)\n",
    "    industry_summary.columns = ['Avg_Correlation', 'N_Companies', 'N_Significant']\n",
    "    industry_summary['Pct_Significant'] = (industry_summary['N_Significant'] / industry_summary['N_Companies'] * 100).round(1)\n",
    "    \n",
    "    for industry, stats in industry_summary.iterrows():\n",
    "        print(f\"  {industry}: Avg r={stats['Avg_Correlation']:.3f}, {stats['N_Significant']}/{stats['N_Companies']} significant ({stats['Pct_Significant']:.1f}%)\")\n",
    "\n",
    "\n",
    "def step9_company_specific_analysis():\n",
    "    \"\"\"Main function for company-specific analysis\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPANY-SPECIFIC SENTIMENT ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        df = load_company_data()\n",
    "        \n",
    "        # Analyze each company\n",
    "        results_df = analyze_company_correlations(df)\n",
    "        \n",
    "        # Create visualizations\n",
    "        print(\"\\nCreating company-specific visualizations...\")\n",
    "        company_plot_path = create_company_grid_plot(df, results_df)\n",
    "        summary_plot_path = create_company_summary_plot(results_df)\n",
    "        \n",
    "        # Save results\n",
    "        save_company_results(results_df)\n",
    "        \n",
    "        # Print findings\n",
    "        print_key_findings(results_df)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\" COMPANY-SPECIFIC ANALYSIS COMPLETED!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Files created:\")\n",
    "        print(f\"• {company_plot_path}\")\n",
    "        print(f\"• {summary_plot_path}\")\n",
    "        print(\"• Company_Specific_Analysis.csv\")\n",
    "        print(\"• Company_Specific_Analysis.xlsx\")\n",
    "        print(\"\\nNow you can see which specific companies drive your results! 🎯\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "step9_company_specific_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329e094",
   "metadata": {},
   "source": [
    "# Step 9: Company-Specific Analysis\n",
    "\n",
    "**Input Files:**\n",
    "- `exports/tables/Final_Regression_Dataset.csv` (from Step 6)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/figures/company_specific/*.png` (individual company plots with 3 subplots each)\n",
    "- `exports/figures/Company_Analysis_Summary.png`\n",
    "- `exports/tables/Company_Specific_Analysis.csv`\n",
    "- `exports/tables/Company_Specific_Analysis.xlsx`\n",
    "\n",
    "**Analysis:**\n",
    "- Individual company sentiment-CAR relationships\n",
    "- 3 plots per company: Positive, Negative, Uncertainty vs CAR\n",
    "- Correlation analysis by company and industry\n",
    "This script creates a comprehensive overview visualization of key results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf7c909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Selected representative companies:\n",
      "  Technology: ['MSFT', 'INTC', 'NFLX']\n",
      "  Financial: ['MA', 'V', 'JPM']\n",
      "  Healthcare: ['BMY', 'JNJ', 'AMGN']\n",
      "  Energy: ['XOM']\n",
      "[OK] Created dashboard for MSFT: Company_Dashboard_01_MSFT_Technology.png\n",
      "[OK] Created dashboard for INTC: Company_Dashboard_02_INTC_Technology.png\n",
      "[OK] Created dashboard for NFLX: Company_Dashboard_03_NFLX_Technology.png\n",
      "[OK] Created dashboard for MA: Company_Dashboard_04_MA_Financial.png\n",
      "[OK] Created dashboard for V: Company_Dashboard_05_V_Financial.png\n",
      "[OK] Created dashboard for JPM: Company_Dashboard_06_JPM_Financial.png\n",
      "[OK] Created dashboard for BMY: Company_Dashboard_07_BMY_Healthcare.png\n",
      "[OK] Created dashboard for JNJ: Company_Dashboard_08_JNJ_Healthcare.png\n",
      "[OK] Created dashboard for AMGN: Company_Dashboard_09_AMGN_Healthcare.png\n",
      "[OK] Created dashboard for XOM: Company_Dashboard_10_XOM_Energy.png\n",
      "[OK] Created Industry Comparison Dashboard: Industry_Comparison_Dashboard.png\n",
      "[OK] Created 10 individual company dashboards plus 1 industry comparison\n",
      "[OK] All dashboards saved in exports/figures/ directory\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def step10_create_summary_dashboard():\n",
    "    \"\"\"Create individual company dashboards for different industries\"\"\"\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    try:\n",
    "        # Load final dataset\n",
    "        df = pd.read_csv('exports/tables/Final_Regression_Dataset.csv')\n",
    "        df['event_date'] = pd.to_datetime(df['event_date'])\n",
    "        \n",
    "        # Define industry classifications (same as used in the analysis)\n",
    "        tech_companies = ['MSFT', 'GOOG', 'FB', 'INTC', 'ADBE', 'CRM', 'INTU', 'TSLA', 'ORCL', 'NFLX']\n",
    "        financial_companies = ['BAC', 'JPM', 'MS', 'GS', 'AXP', 'BLK', 'V', 'MA', 'MCO']\n",
    "        healthcare_companies = ['JNJ', 'ABT', 'LLY', 'AMGN', 'BMY', 'MRK', 'TMO', 'ISRG', 'MDT', 'UNH']\n",
    "        energy_companies = ['XOM']\n",
    "        \n",
    "        # Create industry mapping\n",
    "        industries = {\n",
    "            'Technology': tech_companies,\n",
    "            'Financial': financial_companies,\n",
    "            'Healthcare': healthcare_companies,\n",
    "            'Energy': energy_companies\n",
    "        }\n",
    "        \n",
    "        # Select representative companies from each industry (companies with most data)\n",
    "        selected_companies = {}\n",
    "        \n",
    "        for industry_name, company_list in industries.items():\n",
    "            # Filter companies that exist in our dataset\n",
    "            available_companies = [c for c in company_list if c in df['ticker'].unique()]\n",
    "            \n",
    "            if available_companies:\n",
    "                # Select top 2-3 companies with most observations\n",
    "                company_counts = df[df['ticker'].isin(available_companies)]['ticker'].value_counts()\n",
    "                selected_companies[industry_name] = company_counts.head(3).index.tolist()\n",
    "        \n",
    "        print(f\"[INFO] Selected representative companies:\")\n",
    "        for industry, companies in selected_companies.items():\n",
    "            print(f\"  {industry}: {companies}\")\n",
    "        \n",
    "        # Create individual dashboards\n",
    "        dashboard_count = 0\n",
    "        \n",
    "        for industry_name, company_list in selected_companies.items():\n",
    "            for company in company_list:\n",
    "                dashboard_count += 1\n",
    "                create_company_dashboard(df, company, industry_name, dashboard_count)\n",
    "        \n",
    "        # Create industry comparison dashboard\n",
    "        create_industry_comparison_dashboard(df, selected_companies)\n",
    "        \n",
    "        print(f\"[OK] Created {dashboard_count} individual company dashboards plus 1 industry comparison\")\n",
    "        print(\"[OK] All dashboards saved in exports/figures/ directory\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error creating dashboards: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def create_company_dashboard(df, ticker, industry, dashboard_num):\n",
    "    \"\"\"Create individual dashboard for a specific company\"\"\"\n",
    "    \n",
    "    # Filter data for this company\n",
    "    company_data = df[df['ticker'] == ticker].copy()\n",
    "    \n",
    "    if len(company_data) == 0:\n",
    "        print(f\"[WARNING] No data found for {ticker}\")\n",
    "        return\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # Main title\n",
    "    fig.suptitle(f'{ticker} ({industry} Industry) - Sentiment Analysis Dashboard', \n",
    "                 fontsize=20, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # 1. Time series of CAR and sentiment\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    company_data_sorted = company_data.sort_values('event_date')\n",
    "    \n",
    "    ax1_twin = ax1.twinx()\n",
    "    \n",
    "    # Plot all available CAR windows\n",
    "    lines = []\n",
    "    car_colors = ['blue', 'navy', 'darkblue']\n",
    "    car_markers = ['o', 's', '^']\n",
    "    car_columns = []\n",
    "    \n",
    "    # Check which CAR windows are available\n",
    "    if 'car_-1_1' in company_data_sorted.columns:\n",
    "        car_columns.append(('car_-1_1', 'CAR(-1,+1)'))\n",
    "    if 'car_0_1' in company_data_sorted.columns:\n",
    "        car_columns.append(('car_0_1', 'CAR(0,+1)'))\n",
    "    if 'car_0_2' in company_data_sorted.columns:\n",
    "        car_columns.append(('car_0_2', 'CAR(0,+2)'))\n",
    "    \n",
    "    # If no specific CAR columns, use the default 'car' column\n",
    "    if not car_columns and 'car' in company_data_sorted.columns:\n",
    "        car_columns.append(('car', 'CAR(-1,+1)'))\n",
    "    \n",
    "    # Plot each CAR window\n",
    "    for idx, (car_col, car_label) in enumerate(car_columns):\n",
    "        if car_col in company_data_sorted.columns:\n",
    "            line = ax1.plot(company_data_sorted['event_date'], company_data_sorted[car_col], \n",
    "                          color=car_colors[idx % len(car_colors)], marker=car_markers[idx % len(car_markers)],\n",
    "                          alpha=0.7, linewidth=2, markersize=4, label=car_label)\n",
    "            lines.extend(line)\n",
    "    \n",
    "    ax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax1.set_ylabel('Cumulative Abnormal Return', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    # Plot positive sentiment on secondary axis\n",
    "    line2 = ax1_twin.plot(company_data_sorted['event_date'], company_data_sorted['positive_ratio'], \n",
    "                          'g-s', alpha=0.7, linewidth=2, markersize=4, label='Positive Sentiment')\n",
    "    ax1_twin.set_ylabel('Positive Sentiment Ratio', color='green')\n",
    "    ax1_twin.tick_params(axis='y', labelcolor='green')\n",
    "    \n",
    "    ax1.set_title(f'{ticker} - Multiple CAR Windows vs Positive Sentiment Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Event Date')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines.extend(line2)\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='upper left', fontsize=9)\n",
    "    \n",
    "    # 2. Sentiment distribution for this company\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    sentiment_data = [\n",
    "        company_data['negative_ratio'].dropna(),\n",
    "        company_data['positive_ratio'].dropna(),\n",
    "        company_data['uncertainty_ratio'].dropna()\n",
    "    ]\n",
    "    \n",
    "    if all(len(data) > 0 for data in sentiment_data):\n",
    "        bp = ax2.boxplot(sentiment_data, tick_labels=['Negative', 'Positive', 'Uncertainty'], \n",
    "                        patch_artist=True)\n",
    "        colors = ['lightcoral', 'lightgreen', 'lightskyblue']\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "    \n",
    "    ax2.set_title(f'{ticker} Sentiment Distribution', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Sentiment Ratio')\n",
    "    \n",
    "    # 3. CAR distribution\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax3.hist(company_data['car'].dropna(), bins=15, alpha=0.7, color='orange', edgecolor='black')\n",
    "    ax3.axvline(company_data['car'].mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: {company_data[\"car\"].mean():.4f}')\n",
    "    ax3.set_title(f'{ticker} CAR Distribution', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('CAR(-1,+1)')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Sentiment vs CAR scatter\n",
    "    ax4 = fig.add_subplot(gs[1, 2])\n",
    "    ax4.scatter(company_data['positive_ratio'], company_data['car'], \n",
    "               alpha=0.7, c='blue', s=50, edgecolors='black', linewidth=0.5)\n",
    "    ax4.set_xlabel('Positive Sentiment Ratio')\n",
    "    ax4.set_ylabel('CAR(-1,+1)')\n",
    "    ax4.set_title(f'{ticker} Sentiment vs CAR', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add correlation\n",
    "    if len(company_data) > 1:\n",
    "        corr = company_data[['positive_ratio', 'car']].corr().iloc[0,1]\n",
    "        ax4.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=ax4.transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # 5. Performance metrics comparison\n",
    "    ax5 = fig.add_subplot(gs[2, 0])\n",
    "    \n",
    "    # Compare this company vs industry average\n",
    "    industry_companies = []\n",
    "    if industry == 'Technology':\n",
    "        industry_companies = ['MSFT', 'GOOG', 'FB', 'INTC', 'ADBE', 'CRM', 'INTU', 'TSLA', 'ORCL', 'NFLX']\n",
    "    elif industry == 'Financial':\n",
    "        industry_companies = ['BAC', 'JPM', 'MS', 'GS', 'AXP', 'BLK', 'V', 'MA', 'MCO']\n",
    "    elif industry == 'Healthcare':\n",
    "        industry_companies = ['JNJ', 'ABT', 'LLY', 'AMGN', 'BMY', 'MRK', 'TMO', 'ISRG', 'MDT', 'UNH']\n",
    "    elif industry == 'Energy':\n",
    "        industry_companies = ['XOM']\n",
    "    \n",
    "    industry_data = df[df['ticker'].isin(industry_companies)]\n",
    "    \n",
    "    metrics = ['CAR', 'Positive Sent.', 'Negative Sent.', 'Uncertainty']\n",
    "    company_values = [\n",
    "        company_data['car'].mean(),\n",
    "        company_data['positive_ratio'].mean(),\n",
    "        company_data['negative_ratio'].mean(),\n",
    "        company_data['uncertainty_ratio'].mean()\n",
    "    ]\n",
    "    industry_values = [\n",
    "        industry_data['car'].mean(),\n",
    "        industry_data['positive_ratio'].mean(),\n",
    "        industry_data['negative_ratio'].mean(),\n",
    "        industry_data['uncertainty_ratio'].mean()\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax5.bar(x - width/2, company_values, width, label=ticker, alpha=0.8, color='skyblue')\n",
    "    ax5.bar(x + width/2, industry_values, width, label=f'{industry} Avg', alpha=0.8, color='lightcoral')\n",
    "    \n",
    "    ax5.set_xlabel('Metrics')\n",
    "    ax5.set_ylabel('Average Values')\n",
    "    ax5.set_title(f'{ticker} vs {industry} Industry', fontsize=12, fontweight='bold')\n",
    "    ax5.set_xticks(x)\n",
    "    ax5.set_xticklabels(metrics)\n",
    "    ax5.legend()\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Statistics table\n",
    "    ax6 = fig.add_subplot(gs[2, 1:])\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Create company statistics\n",
    "    stats_data = [\n",
    "        ['Observations', f\"{len(company_data)}\"],\n",
    "        ['Time Period', f\"{company_data['event_date'].min().strftime('%Y-%m')} to {company_data['event_date'].max().strftime('%Y-%m')}\"],\n",
    "        ['Average CAR', f\"{company_data['car'].mean():.4f} ({company_data['car'].mean()*100:.2f}%)\"],\n",
    "        ['CAR Std Dev', f\"{company_data['car'].std():.4f}\"],\n",
    "        ['Best Quarter CAR', f\"{company_data['car'].max():.4f} ({company_data['car'].max()*100:.2f}%)\"],\n",
    "        ['Worst Quarter CAR', f\"{company_data['car'].min():.4f} ({company_data['car'].min()*100:.2f}%)\"],\n",
    "        ['Avg Positive Sentiment', f\"{company_data['positive_ratio'].mean():.4f}\"],\n",
    "        ['Avg Negative Sentiment', f\"{company_data['negative_ratio'].mean():.4f}\"],\n",
    "        ['Avg Uncertainty', f\"{company_data['uncertainty_ratio'].mean():.4f}\"],\n",
    "        ['Market Cap (Latest)', f\"${company_data['market_cap'].iloc[-1]/1e9:.1f}B\" if 'market_cap' in company_data.columns else 'N/A'],\n",
    "    ]\n",
    "    \n",
    "    # Create table\n",
    "    table = ax6.table(cellText=stats_data, \n",
    "                     colLabels=['Metric', 'Value'],\n",
    "                     cellLoc='left',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.4, 0.3])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "    \n",
    "    # Style the table\n",
    "    for (i, j), cell in table.get_celld().items():\n",
    "        if i == 0:  # Header row\n",
    "            cell.set_text_props(weight='bold')\n",
    "            cell.set_facecolor('#4472C4')\n",
    "            cell.set_text_props(color='white')\n",
    "        else:\n",
    "            cell.set_facecolor('#F2F2F2' if i % 2 == 0 else 'white')\n",
    "    \n",
    "    ax6.set_title(f'{ticker} Company Statistics', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Save the dashboard\n",
    "    filename = f'Company_Dashboard_{dashboard_num:02d}_{ticker}_{industry}.png'\n",
    "    plt.savefig(f'exports/figures/{filename}', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"[OK] Created dashboard for {ticker}: {filename}\")\n",
    "\n",
    "def create_industry_comparison_dashboard(df, selected_companies):\n",
    "    \"\"\"Create a dashboard comparing industries\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    fig.suptitle('Industry Comparison Dashboard - Sentiment Analysis', \n",
    "                 fontsize=24, fontweight='bold', y=0.95)\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Blue, Orange, Green, Red\n",
    "    \n",
    "    # 1. Average CAR by Industry\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    industry_car = {}\n",
    "    for industry, companies in selected_companies.items():\n",
    "        industry_data = df[df['ticker'].isin(companies)]\n",
    "        industry_car[industry] = industry_data['car'].mean()\n",
    "    \n",
    "    bars = ax1.bar(industry_car.keys(), industry_car.values(), color=colors[:len(industry_car)], alpha=0.8)\n",
    "    ax1.set_title('Average CAR by Industry', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Average CAR(-1,+1)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, industry_car.values()):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Average Positive Sentiment by Industry\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    industry_pos_sent = {}\n",
    "    for industry, companies in selected_companies.items():\n",
    "        industry_data = df[df['ticker'].isin(companies)]\n",
    "        industry_pos_sent[industry] = industry_data['positive_ratio'].mean()\n",
    "    \n",
    "    bars = ax2.bar(industry_pos_sent.keys(), industry_pos_sent.values(), color=colors[:len(industry_pos_sent)], alpha=0.8)\n",
    "    ax2.set_title('Average Positive Sentiment by Industry', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Positive Sentiment Ratio')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, industry_pos_sent.values()):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0005, \n",
    "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. CAR Volatility by Industry\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    industry_vol = {}\n",
    "    for industry, companies in selected_companies.items():\n",
    "        industry_data = df[df['ticker'].isin(companies)]\n",
    "        industry_vol[industry] = industry_data['car'].std()\n",
    "    \n",
    "    bars = ax3.bar(industry_vol.keys(), industry_vol.values(), color=colors[:len(industry_vol)], alpha=0.8)\n",
    "    ax3.set_title('CAR Volatility by Industry', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylabel('CAR Standard Deviation')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, industry_vol.values()):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Number of Observations by Industry\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    industry_obs = {}\n",
    "    for industry, companies in selected_companies.items():\n",
    "        industry_data = df[df['ticker'].isin(companies)]\n",
    "        industry_obs[industry] = len(industry_data)\n",
    "    \n",
    "    bars = ax4.bar(industry_obs.keys(), industry_obs.values(), color=colors[:len(industry_obs)], alpha=0.8)\n",
    "    ax4.set_title('Observations by Industry', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Number of Earnings Calls')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, industry_obs.values()):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "                f'{value}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. Industry Sentiment Distribution\n",
    "    ax5 = fig.add_subplot(gs[1, :2])\n",
    "    \n",
    "    industry_sentiment_data = []\n",
    "    industry_labels = []\n",
    "    \n",
    "    for industry, companies in selected_companies.items():\n",
    "        industry_data = df[df['ticker'].isin(companies)]\n",
    "        industry_sentiment_data.append(industry_data['positive_ratio'].dropna())\n",
    "        industry_labels.append(industry)\n",
    "    \n",
    "    bp = ax5.boxplot(industry_sentiment_data, tick_labels=industry_labels, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors[:len(industry_labels)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.8)\n",
    "    \n",
    "    ax5.set_title('Positive Sentiment Distribution by Industry', fontsize=14, fontweight='bold')\n",
    "    ax5.set_ylabel('Positive Sentiment Ratio')\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Industry CAR Distribution\n",
    "    ax6 = fig.add_subplot(gs[1, 2:])\n",
    "    \n",
    "    industry_car_data = []\n",
    "    industry_labels = []\n",
    "    \n",
    "    for industry, companies in selected_companies.items():\n",
    "        industry_data = df[df['ticker'].isin(companies)]\n",
    "        industry_car_data.append(industry_data['car'].dropna())\n",
    "        industry_labels.append(industry)\n",
    "    \n",
    "    bp = ax6.boxplot(industry_car_data, tick_labels=industry_labels, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors[:len(industry_labels)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.8)\n",
    "    \n",
    "    ax6.set_title('CAR Distribution by Industry', fontsize=14, fontweight='bold')\n",
    "    ax6.set_ylabel('CAR(-1,+1)')\n",
    "    ax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax6.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 7. Summary Statistics Table\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    # Create comprehensive industry comparison table\n",
    "    table_data = []\n",
    "    headers = ['Industry', 'Companies', 'Observations', 'Avg CAR', 'CAR Std Dev', \n",
    "               'Avg Pos Sentiment', 'Avg Neg Sentiment', 'Avg Uncertainty']\n",
    "    \n",
    "    for industry, companies in selected_companies.items():\n",
    "        industry_data = df[df['ticker'].isin(companies)]\n",
    "        row = [\n",
    "            industry,\n",
    "            ', '.join(companies[:3]) + ('...' if len(companies) > 3 else ''),\n",
    "            f\"{len(industry_data)}\",\n",
    "            f\"{industry_data['car'].mean():.4f}\",\n",
    "            f\"{industry_data['car'].std():.4f}\",\n",
    "            f\"{industry_data['positive_ratio'].mean():.4f}\",\n",
    "            f\"{industry_data['negative_ratio'].mean():.4f}\",\n",
    "            f\"{industry_data['uncertainty_ratio'].mean():.4f}\"\n",
    "        ]\n",
    "        table_data.append(row)\n",
    "    \n",
    "    table = ax7.table(cellText=table_data, \n",
    "                     colLabels=headers,\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.12, 0.18, 0.08, 0.08, 0.08, 0.12, 0.12, 0.12])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style the table\n",
    "    for (i, j), cell in table.get_celld().items():\n",
    "        if i == 0:  # Header row\n",
    "            cell.set_text_props(weight='bold')\n",
    "            cell.set_facecolor('#4472C4')\n",
    "            cell.set_text_props(color='white')\n",
    "        else:\n",
    "            cell.set_facecolor(colors[i-1] if i-1 < len(colors) else '#F2F2F2')\n",
    "            cell.set_alpha(0.3)\n",
    "    \n",
    "    ax7.set_title('Industry Comparison Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add methodology note\n",
    "    methodology_text = \"\"\"\n",
    "Industry Analysis: Representative companies selected based on highest observation counts per industry.\n",
    "Technology companies show different sentiment patterns compared to Financial and Healthcare sectors.\n",
    "Each industry dashboard provides detailed analysis of top companies within that sector.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figtext(0.5, 0.02, methodology_text, ha='center', fontsize=10, \n",
    "               style='italic', wrap=True)\n",
    "    \n",
    "    # Save the industry comparison dashboard\n",
    "    plt.savefig('exports/figures/Industry_Comparison_Dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"[OK] Created Industry Comparison Dashboard: Industry_Comparison_Dashboard.png\")\n",
    "\n",
    "\n",
    "step10_create_summary_dashboard()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10701ed",
   "metadata": {},
   "source": [
    "# Step 10: Summary Dashboard\n",
    "\n",
    "**Input Files:**\n",
    "- `exports/tables/Final_Regression_Dataset.csv` (from Step 6)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/figures/Summary_Dashboard.png` (comprehensive overview)\n",
    "\n",
    "**Dashboard Components:**\n",
    "- Overall sentiment-CAR relationships\n",
    "- Industry comparisons\n",
    "- Temporal patterns\n",
    "- Key statistics and findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e15b7",
   "metadata": {},
   "source": [
    "# Step 11: Report Figures for Final Publication\n",
    "\n",
    "**Input Files:**\n",
    "- `exports/tables/Final_Regression_Dataset.csv` (from Step 6)\n",
    "\n",
    "**Output Files:**\n",
    "- `exports/figures/report_figures/Table1_Descriptive_Statistics.csv`\n",
    "- `exports/figures/report_figures/Table1_Descriptive_Statistics.png`\n",
    "- `exports/figures/report_figures/Table2_Correlation_Matrix.csv`\n",
    "- `exports/figures/report_figures/Figure1_Correlation_Matrix.png`\n",
    "- `exports/figures/report_figures/Figure2_Sentiment_CAR_Scatterplots.png`\n",
    "- `exports/figures/report_figures/Table3_Industry_Statistics.csv`\n",
    "- `exports/figures/report_figures/Figure3_Industry_CAR_Boxplot.png`\n",
    "- `exports/figures/report_figures/Figure4_Control_Variables_Scatterplots.png`\n",
    "- `exports/figures/report_figures/Table4_Regression_Results.csv`\n",
    "- `exports/figures/report_figures/Table5_Model_Statistics.csv`\n",
    "- `exports/figures/report_figures/Table6_Period_Statistics.csv`\n",
    "- `exports/figures/report_figures/Figure5_Period_Comparison.png`\n",
    "\n",
    "**Publication-Ready Figures:**\n",
    "1. Descriptive statistics table\n",
    "2. Correlation matrix heatmap\n",
    "3. Sentiment vs CAR scatterplots (positive, negative, uncertainty)\n",
    "4. Industry comparison boxplots\n",
    "5. Control variables analysis\n",
    "6. OLS regression results\n",
    "7. COVID vs Non-COVID period comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d805d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE VISUALIZATION FOR FINAL REPORT\n",
      "================================================================================\n",
      "\n",
      "Dataset loaded: 4285 observations from 59 companies\n",
      "Date range: 2005-10-31 00:40:28 to 2021-03-12 01:38:07\n",
      "Figures will be saved to: exports/figures/report_figures\n",
      "\n",
      "================================================================================\n",
      "1 DESCRIPTIVE STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Descriptive Statistics (All Companies):\n",
      "                                   mean        std           var       min  \\\n",
      "CAR (-1,+1)                      0.0031     0.0508  2.600000e-03   -0.3485   \n",
      "Positive Sentiment Ratio         0.0352     0.0092  1.000000e-04    0.0072   \n",
      "Negative Sentiment Ratio         0.0217     0.0076  1.000000e-04    0.0022   \n",
      "Uncertainty Ratio                0.0126     0.0045  0.000000e+00    0.0000   \n",
      "Price Volatility (Pre-Event)     0.0172     0.0115  1.000000e-04    0.0029   \n",
      "Log Market Cap                  24.7043     0.9576  9.170000e-01   20.4489   \n",
      "Price Momentum (1 Month)         0.0202     0.0871  7.600000e-03   -0.5263   \n",
      "Abnormal Turnover (%)           87.9224   124.2784  1.544512e+04  -68.4688   \n",
      "Total Words                   3993.0026  1687.2290  2.846742e+06  156.0000   \n",
      "\n",
      "                                    25%        50%        75%         max  \n",
      "CAR (-1,+1)                     -0.0201     0.0006     0.0225      0.6211  \n",
      "Positive Sentiment Ratio         0.0286     0.0348     0.0414      0.0783  \n",
      "Negative Sentiment Ratio         0.0165     0.0207     0.0259      0.0673  \n",
      "Uncertainty Ratio                0.0095     0.0121     0.0152      0.0393  \n",
      "Price Volatility (Pre-Event)     0.0103     0.0142     0.0200      0.1307  \n",
      "Log Market Cap                  24.0858    24.7629    25.3271     28.1716  \n",
      "Price Momentum (1 Month)        -0.0247     0.0190     0.0610      0.7007  \n",
      "Abnormal Turnover (%)           11.1152    56.1313   123.0379   1748.8557  \n",
      "Total Words                   2935.0000  4091.0000  4666.0000  25586.0000  \n",
      "\n",
      " Saved: exports/figures/report_figures\\Table1_Descriptive_Statistics.csv\n",
      " Saved: exports/figures/report_figures\\Table1_Descriptive_Statistics.png\n",
      "\n",
      "================================================================================\n",
      "2 CORRELATION MATRIX\n",
      "================================================================================\n",
      "\n",
      " Saved: exports/figures/report_figures\\Figure1_Correlation_Matrix.png\n",
      " Saved: exports/figures/report_figures\\Table2_Correlation_Matrix.csv\n",
      "\n",
      "================================================================================\n",
      "3 SENTIMENT VS CAR SCATTER PLOTS\n",
      "================================================================================\n",
      "\n",
      " Saved: exports/figures/report_figures\\Figure2_Sentiment_CAR_Scatterplots.png\n",
      "\n",
      "Sentiment-CAR Correlations:\n",
      "  Positive Sentiment Ratio: 0.0196\n",
      "  Negative Sentiment Ratio: -0.0347\n",
      "  Uncertainty Ratio: 0.0237\n",
      "\n",
      "================================================================================\n",
      "4 INDUSTRY COMPARISON\n",
      "================================================================================\n",
      "\n",
      " Saved: exports/figures/report_figures\\Figure3_Industry_CAR_Boxplot.png\n",
      "\n",
      "CAR Statistics by Industry:\n",
      "            count    mean     std     min     max\n",
      "industry                                         \n",
      "Consumer      436  0.0000  0.0344 -0.1233  0.1282\n",
      "Energy         73 -0.0076  0.0232 -0.0519  0.0451\n",
      "Financial     675  0.0034  0.0470 -0.3474  0.2796\n",
      "Healthcare    663  0.0039  0.0436 -0.1936  0.3389\n",
      "Other        1657  0.0024  0.0476 -0.3485  0.3279\n",
      "Technology    617  0.0073  0.0760 -0.3153  0.6211\n",
      "\n",
      " Saved: exports/figures/report_figures\\Table3_Industry_Statistics.csv\n",
      "\n",
      "================================================================================\n",
      "5 CONTROL VARIABLE EFFECTS\n",
      "================================================================================\n",
      "\n",
      " Saved: exports/figures/report_figures\\Figure4_Control_Variables_Scatterplots.png\n",
      "\n",
      "================================================================================\n",
      "6 REGRESSION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Regression sample size: 4121 observations\n",
      "\n",
      "================================================================================\n",
      "REGRESSION RESULTS (Heteroskedasticity-Robust Standard Errors)\n",
      "================================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    car   R-squared:                       0.246\n",
      "Model:                            OLS   Adj. R-squared:                  0.244\n",
      "Method:                 Least Squares   F-statistic:                     23.83\n",
      "Date:                Sun, 02 Nov 2025   Prob (F-statistic):           3.43e-52\n",
      "Time:                        13:34:35   Log-Likelihood:                 7015.3\n",
      "No. Observations:                4121   AIC:                        -1.400e+04\n",
      "Df Residuals:                    4108   BIC:                        -1.392e+04\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:                  HC3                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "const                          0.0401      0.029      1.378      0.168      -0.017       0.097\n",
      "positive_ratio                 0.1503      0.084      1.785      0.074      -0.015       0.315\n",
      "negative_ratio                -0.3978      0.104     -3.813      0.000      -0.602      -0.193\n",
      "uncertainty_ratio              0.1906      0.173      1.100      0.271      -0.149       0.530\n",
      "price_volatility_pre_event     0.6539      0.162      4.031      0.000       0.336       0.972\n",
      "log_market_cap                -0.0021      0.001     -1.833      0.067      -0.004       0.000\n",
      "price_momentum_1month          0.2729      0.018     14.750      0.000       0.237       0.309\n",
      "abnormal_turnover          -1.163e-05   1.38e-05     -0.840      0.401   -3.88e-05    1.55e-05\n",
      "tech                          -0.0010      0.003     -0.329      0.742      -0.007       0.005\n",
      "financial                     -0.0003      0.002     -0.146      0.884      -0.004       0.004\n",
      "healthcare                     0.0022      0.002      1.173      0.241      -0.001       0.006\n",
      "energy                        -0.0010      0.003     -0.289      0.773      -0.008       0.006\n",
      "consumer                       0.0006      0.002      0.305      0.761      -0.003       0.004\n",
      "==============================================================================\n",
      "Omnibus:                      597.482   Durbin-Watson:                   1.978\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8718.535\n",
      "Skew:                           0.096   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.123   Cond. No.                     3.81e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n",
      "[2] The condition number is large, 3.81e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "================================================================================\n",
      "Regression Coefficients Table:\n",
      "================================================================================\n",
      "                      Variable  Coefficient  Std Error  t-statistic  P-value  \\\n",
      "0                        const       0.0401     0.0291       1.3784   0.1681   \n",
      "1               positive_ratio       0.1503     0.0842       1.7849   0.0743   \n",
      "2               negative_ratio      -0.3978     0.1043      -3.8130   0.0001   \n",
      "3            uncertainty_ratio       0.1906     0.1733       1.0998   0.2714   \n",
      "4   price_volatility_pre_event       0.6539     0.1622       4.0314   0.0001   \n",
      "5               log_market_cap      -0.0021     0.0011      -1.8329   0.0668   \n",
      "6        price_momentum_1month       0.2729     0.0185      14.7499   0.0000   \n",
      "7            abnormal_turnover      -0.0000     0.0000      -0.8401   0.4009   \n",
      "8                         tech      -0.0010     0.0030      -0.3291   0.7421   \n",
      "9                    financial      -0.0003     0.0021      -0.1460   0.8839   \n",
      "10                  healthcare       0.0022     0.0019       1.1727   0.2409   \n",
      "11                      energy      -0.0010     0.0035      -0.2890   0.7726   \n",
      "12                    consumer       0.0006     0.0019       0.3047   0.7606   \n",
      "\n",
      "   Significant  \n",
      "0               \n",
      "1            *  \n",
      "2          ***  \n",
      "3               \n",
      "4          ***  \n",
      "5            *  \n",
      "6          ***  \n",
      "7               \n",
      "8               \n",
      "9               \n",
      "10              \n",
      "11              \n",
      "12              \n",
      "\n",
      " Saved: exports/figures/report_figures\\Table4_Regression_Results.csv\n",
      "\n",
      "================================================================================\n",
      "Model Statistics:\n",
      "================================================================================\n",
      "            Statistic         Value\n",
      "0           R-squared  2.462689e-01\n",
      "1      Adj. R-squared  2.440672e-01\n",
      "2         F-statistic  2.383227e+01\n",
      "3  Prob (F-statistic)  3.425509e-52\n",
      "4    No. Observations  4.121000e+03\n",
      " Saved: exports/figures/report_figures\\Table5_Model_Statistics.csv\n",
      "\n",
      "================================================================================\n",
      "7 PERIOD COMPARISON (COVID vs Non-COVID)\n",
      "================================================================================\n",
      "\n",
      "CAR by Period:\n",
      "      Period  Count  Mean CAR   Std Dev\n",
      "0  Non-COVID   3794  0.003565  0.050705\n",
      "1      COVID    327 -0.002035  0.051739\n",
      "\n",
      " Saved: exports/figures/report_figures\\Figure5_Period_Comparison.png\n",
      "\n",
      "T-test for difference in means:\n",
      "  t-statistic: -1.9132\n",
      "  p-value: 0.0558\n",
      "  Significant: No\n",
      " Saved: exports/figures/report_figures\\Table6_Period_Statistics.csv\n",
      "\n",
      "================================================================================\n",
      " ALL VISUALIZATIONS COMPLETED!\n",
      "================================================================================\n",
      "\n",
      "All figures and tables saved to: exports/figures/report_figures\n",
      "\n",
      "Generated files:\n",
      "  Tables:\n",
      "    - Table1_Descriptive_Statistics.csv & .png\n",
      "    - Table2_Correlation_Matrix.csv\n",
      "    - Table3_Industry_Statistics.csv\n",
      "    - Table4_Regression_Results.csv\n",
      "    - Table5_Model_Statistics.csv\n",
      "    - Table6_Period_Statistics.csv\n",
      "\n",
      "  Figures:\n",
      "    - Figure1_Correlation_Matrix.png\n",
      "    - Figure2_Sentiment_CAR_Scatterplots.png\n",
      "    - Figure3_Industry_CAR_Boxplot.png\n",
      "    - Figure4_Control_Variables_Scatterplots.png\n",
      "    - Figure5_Period_Comparison.png\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import os\n",
    "\n",
    "# Set style for professional figures\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "\n",
    "# Load data\n",
    "data_path = \"exports/tables/Final_Regression_Dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE VISUALIZATION FOR FINAL REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDataset loaded: {len(df)} observations from {df['ticker'].nunique()} companies\")\n",
    "print(f\"Date range: {df['event_date'].min()} to {df['event_date'].max()}\")\n",
    "\n",
    "# Create output directory for figures\n",
    "figure_dir = \"exports/figures/report_figures\"\n",
    "os.makedirs(figure_dir, exist_ok=True)\n",
    "print(f\"Figures will be saved to: {figure_dir}\")\n",
    "\n",
    "# ========================================\n",
    "# 1️ DESCRIPTIVE STATISTICS TABLE\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1 DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "descriptive_cols = [\n",
    "    \"car\", \"positive_ratio\", \"negative_ratio\", \"uncertainty_ratio\",\n",
    "    \"price_volatility_pre_event\", \"log_market_cap\",\n",
    "    \"price_momentum_1month\", \"abnormal_turnover\", \"total_words\"\n",
    "]\n",
    "\n",
    "# Create descriptive statistics table\n",
    "desc_stats = df[descriptive_cols].describe().T\n",
    "desc_stats['var'] = df[descriptive_cols].var()\n",
    "desc_stats = desc_stats[['mean', 'std', 'var', 'min', '25%', '50%', '75%', 'max']]\n",
    "\n",
    "# Rename for better presentation\n",
    "desc_stats.index = [\n",
    "    'CAR (-1,+1)',\n",
    "    'Positive Sentiment Ratio',\n",
    "    'Negative Sentiment Ratio',\n",
    "    'Uncertainty Ratio',\n",
    "    'Price Volatility (Pre-Event)',\n",
    "    'Log Market Cap',\n",
    "    'Price Momentum (1 Month)',\n",
    "    'Abnormal Turnover (%)',\n",
    "    'Total Words'\n",
    "]\n",
    "\n",
    "print(\"\\nDescriptive Statistics (All Companies):\")\n",
    "print(desc_stats.round(4))\n",
    "\n",
    "# Save to CSV\n",
    "desc_path = os.path.join(figure_dir, \"Table1_Descriptive_Statistics.csv\")\n",
    "desc_stats.to_csv(desc_path)\n",
    "print(f\"\\n Saved: {desc_path}\")\n",
    "\n",
    "# Create a visual table\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table_data = desc_stats.round(4).reset_index()\n",
    "table_data.columns = ['Variable', 'Mean', 'Std Dev', 'Variance', 'Min', '25%', 'Median', '75%', 'Max']\n",
    "\n",
    "table = ax.table(cellText=table_data.values,\n",
    "                colLabels=table_data.columns,\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colWidths=[0.25] + [0.09]*8)\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Color header\n",
    "for i in range(len(table_data.columns)):\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(table_data) + 1):\n",
    "    for j in range(len(table_data.columns)):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#E7E6E6')\n",
    "        else:\n",
    "            table[(i, j)].set_facecolor('#F2F2F2')\n",
    "\n",
    "plt.title('Table 1: Descriptive Statistics (All Companies)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "table_fig_path = os.path.join(figure_dir, \"Table1_Descriptive_Statistics.png\")\n",
    "plt.savefig(table_fig_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\" Saved: {table_fig_path}\")\n",
    "\n",
    "# ========================================\n",
    "# 2 CORRELATION MATRIX\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2 CORRELATION MATRIX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "correlation_cols = [\n",
    "    \"car\", \"positive_ratio\", \"negative_ratio\", \"uncertainty_ratio\",\n",
    "    \"price_volatility_pre_event\", \"log_market_cap\",\n",
    "    \"price_momentum_1month\", \"abnormal_turnover\"\n",
    "]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df[correlation_cols].corr()\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Create heatmap\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            fmt='.3f',\n",
    "            cmap=cmap,\n",
    "            center=0,\n",
    "            square=True,\n",
    "            linewidths=1,\n",
    "            cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation Coefficient\"},\n",
    "            ax=ax,\n",
    "            vmin=-1, vmax=1)\n",
    "\n",
    "# Better labels\n",
    "labels = [\n",
    "    'CAR\\n(-1,+1)',\n",
    "    'Positive\\nRatio',\n",
    "    'Negative\\nRatio',\n",
    "    'Uncertainty\\nRatio',\n",
    "    'Price\\nVolatility',\n",
    "    'Log Market\\nCap',\n",
    "    'Price\\nMomentum',\n",
    "    'Abnormal\\nTurnover'\n",
    "]\n",
    "\n",
    "ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "ax.set_yticklabels(labels, rotation=0)\n",
    "\n",
    "plt.title('Figure 1: Correlation Matrix of Key Variables (All Companies)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "corr_path = os.path.join(figure_dir, \"Figure1_Correlation_Matrix.png\")\n",
    "plt.savefig(corr_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n Saved: {corr_path}\")\n",
    "\n",
    "# Save correlation matrix to CSV\n",
    "corr_csv_path = os.path.join(figure_dir, \"Table2_Correlation_Matrix.csv\")\n",
    "corr_matrix.to_csv(corr_csv_path)\n",
    "print(f\" Saved: {corr_csv_path}\")\n",
    "\n",
    "# ========================================\n",
    "# 3 SCATTER PLOTS: SENTIMENT vs CAR\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3 SENTIMENT VS CAR SCATTER PLOTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create 3 scatterplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sentiment_vars = [\n",
    "    ('positive_ratio', 'Positive Sentiment Ratio', '#2E7D32'),\n",
    "    ('negative_ratio', 'Negative Sentiment Ratio', '#C62828'),\n",
    "    ('uncertainty_ratio', 'Uncertainty Ratio', '#F57C00')\n",
    "]\n",
    "\n",
    "for idx, (var, title, color) in enumerate(sentiment_vars):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(df[var], df['car'], alpha=0.5, s=30, color=color, edgecolors='white', linewidth=0.5, zorder=2)\n",
    "    \n",
    "    # Add regression line - FIXED: more visible with proper styling\n",
    "    valid_mask = df[var].notna() & df['car'].notna()\n",
    "    if valid_mask.sum() > 1:\n",
    "        z = np.polyfit(df.loc[valid_mask, var], df.loc[valid_mask, 'car'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(df[var].min(), df[var].max(), 100)\n",
    "        ax.plot(x_line, p(x_line), \n",
    "                color='black', \n",
    "                linewidth=3, \n",
    "                linestyle='--', \n",
    "                dashes=(5, 3),  # Explicit dash pattern\n",
    "                label='Regression Line',\n",
    "                zorder=3,  # Draw on top\n",
    "                alpha=0.8)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr = df[[var, 'car']].corr().iloc[0, 1]\n",
    "    \n",
    "    # Add correlation annotation\n",
    "    ax.text(0.05, 0.95, f'ρ = {corr:.3f}', \n",
    "            transform=ax.transAxes, fontsize=11, fontweight='bold',\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel(title, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('CAR (-1, +1)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{title} vs CAR', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax.grid(True, alpha=0.3, zorder=1)\n",
    "    ax.legend(loc='lower right', framealpha=0.9)\n",
    "\n",
    "plt.suptitle('Figure 2: Sentiment Ratios vs Cumulative Abnormal Returns (All Companies)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "scatter_path = os.path.join(figure_dir, \"Figure2_Sentiment_CAR_Scatterplots.png\")\n",
    "plt.savefig(scatter_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n Saved: {scatter_path}\")\n",
    "\n",
    "# Print correlations\n",
    "print(\"\\nSentiment-CAR Correlations:\")\n",
    "for var, title, _ in sentiment_vars:\n",
    "    corr = df[[var, 'car']].corr().iloc[0, 1]\n",
    "    print(f\"  {title}: {corr:.4f}\")\n",
    "\n",
    "# ========================================\n",
    "# 4 INDUSTRY COMPARISON (Boxplot)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4 INDUSTRY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create industry label from dummy variables\n",
    "industry_cols = [\"tech\", \"financial\", \"healthcare\", \"energy\", \"consumer\", \"other\"]\n",
    "df_industry = df.copy()\n",
    "\n",
    "# Map industry dummies to labels\n",
    "def get_industry(row):\n",
    "    if row['tech'] == 1:\n",
    "        return 'Technology'\n",
    "    elif row['financial'] == 1:\n",
    "        return 'Financial'\n",
    "    elif row['healthcare'] == 1:\n",
    "        return 'Healthcare'\n",
    "    elif row['energy'] == 1:\n",
    "        return 'Energy'\n",
    "    elif row['consumer'] == 1:\n",
    "        return 'Consumer'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_industry['industry'] = df_industry.apply(get_industry, axis=1)\n",
    "\n",
    "# Create boxplot\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Define colors for each industry\n",
    "industry_colors = {\n",
    "    'Technology': '#2196F3',\n",
    "    'Financial': '#4CAF50',\n",
    "    'Healthcare': '#F44336',\n",
    "    'Energy': '#FF9800',\n",
    "    'Consumer': '#9C27B0',\n",
    "    'Other': '#607D8B'\n",
    "}\n",
    "\n",
    "# Create boxplot with custom colors\n",
    "industries = ['Technology', 'Financial', 'Healthcare', 'Energy', 'Consumer', 'Other']\n",
    "positions = range(len(industries))\n",
    "box_data = [df_industry[df_industry['industry'] == ind]['car'].dropna() for ind in industries]\n",
    "\n",
    "bp = ax.boxplot(box_data, positions=positions, labels=industries,\n",
    "                patch_artist=True, widths=0.6,\n",
    "                boxprops=dict(linewidth=1.5),\n",
    "                medianprops=dict(linewidth=2, color='darkred'),\n",
    "                whiskerprops=dict(linewidth=1.5),\n",
    "                capprops=dict(linewidth=1.5))\n",
    "\n",
    "# Color boxes\n",
    "for patch, industry in zip(bp['boxes'], industries):\n",
    "    patch.set_facecolor(industry_colors[industry])\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "# Add mean markers\n",
    "means = [data.mean() for data in box_data]\n",
    "ax.plot(positions, means, 'D', color='black', markersize=8, \n",
    "        label='Mean', markerfacecolor='yellow', markeredgewidth=2)\n",
    "\n",
    "# Add horizontal line at y=0\n",
    "ax.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Industry Sector', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('CAR (-1, +1)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Figure 3: Distribution of CAR by Industry Sector (All Companies)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "boxplot_path = os.path.join(figure_dir, \"Figure3_Industry_CAR_Boxplot.png\")\n",
    "plt.savefig(boxplot_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n Saved: {boxplot_path}\")\n",
    "\n",
    "# Print industry statistics\n",
    "print(\"\\nCAR Statistics by Industry:\")\n",
    "industry_stats = df_industry.groupby('industry')['car'].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "industry_stats = industry_stats.round(4)\n",
    "print(industry_stats)\n",
    "\n",
    "# Save to CSV\n",
    "industry_stats_path = os.path.join(figure_dir, \"Table3_Industry_Statistics.csv\")\n",
    "industry_stats.to_csv(industry_stats_path)\n",
    "print(f\"\\n Saved: {industry_stats_path}\")\n",
    "\n",
    "# ========================================\n",
    "# 5 CONTROL EFFECTS (Scatter Plots)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5 CONTROL VARIABLE EFFECTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "control_vars = [\n",
    "    ('log_market_cap', 'Log Market Capitalization', '#1976D2'),\n",
    "    ('price_volatility_pre_event', 'Price Volatility (Pre-Event)', '#D32F2F')\n",
    "]\n",
    "\n",
    "for idx, (var, title, color) in enumerate(control_vars):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(df[var], df['car'], alpha=0.5, s=30, color=color, edgecolors='white', linewidth=0.5)\n",
    "    \n",
    "    # Add regression line\n",
    "    valid_data = df[[var, 'car']].dropna()\n",
    "    if len(valid_data) > 0:\n",
    "        z = np.polyfit(valid_data[var], valid_data['car'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_line = np.linspace(df[var].min(), df[var].max(), 100)\n",
    "        ax.plot(x_line, p(x_line), color='darkred', linewidth=2, linestyle='--', label='Regression Line')\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = valid_data.corr().iloc[0, 1]\n",
    "        \n",
    "        # Add correlation annotation\n",
    "        ax.text(0.05, 0.95, f'ρ = {corr:.3f}', \n",
    "                transform=ax.transAxes, fontsize=11, fontweight='bold',\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel(title, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('CAR (-1, +1)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{title} vs CAR', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "plt.suptitle('Figure 4: Control Variables vs Cumulative Abnormal Returns (All Companies)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "control_path = os.path.join(figure_dir, \"Figure4_Control_Variables_Scatterplots.png\")\n",
    "plt.savefig(control_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\n Saved: {control_path}\")\n",
    "\n",
    "# ========================================\n",
    "# 6 REGRESSION ANALYSIS (OLS)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"6 REGRESSION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare regression data\n",
    "dependent_var = \"car\"\n",
    "independent_vars = [\n",
    "    \"positive_ratio\", \"negative_ratio\", \"uncertainty_ratio\",\n",
    "    \"price_volatility_pre_event\", \"log_market_cap\",\n",
    "    \"price_momentum_1month\", \"abnormal_turnover\",\n",
    "    \"tech\", \"financial\", \"healthcare\", \"energy\", \"consumer\"\n",
    "]\n",
    "\n",
    "# Create regression dataset (drop rows with missing values)\n",
    "reg_data = df[[dependent_var] + independent_vars].dropna()\n",
    "\n",
    "print(f\"\\nRegression sample size: {len(reg_data)} observations\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = reg_data[independent_vars]\n",
    "y = reg_data[dependent_var]\n",
    "\n",
    "# Add constant\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Run OLS regression with robust standard errors\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit(cov_type='HC3')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REGRESSION RESULTS (Heteroskedasticity-Robust Standard Errors)\")\n",
    "print(\"=\" * 80)\n",
    "print(results.summary())\n",
    "\n",
    "# Create regression results table\n",
    "reg_table = pd.DataFrame({\n",
    "    'Variable': results.params.index,\n",
    "    'Coefficient': results.params.values,\n",
    "    'Std Error': results.bse.values,\n",
    "    't-statistic': results.tvalues.values,\n",
    "    'P-value': results.pvalues.values,\n",
    "    'Significant': ['***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.1 else '' \n",
    "                    for p in results.pvalues.values]\n",
    "})\n",
    "\n",
    "reg_table = reg_table.round(4)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Regression Coefficients Table:\")\n",
    "print(\"=\" * 80)\n",
    "print(reg_table)\n",
    "\n",
    "# Save regression results\n",
    "reg_results_path = os.path.join(figure_dir, \"Table4_Regression_Results.csv\")\n",
    "reg_table.to_csv(reg_results_path, index=False)\n",
    "print(f\"\\n Saved: {reg_results_path}\")\n",
    "\n",
    "# Model statistics\n",
    "model_stats = pd.DataFrame({\n",
    "    'Statistic': ['R-squared', 'Adj. R-squared', 'F-statistic', 'Prob (F-statistic)', 'No. Observations'],\n",
    "    'Value': [results.rsquared, results.rsquared_adj, results.fvalue, results.f_pvalue, results.nobs]\n",
    "})\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Model Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(model_stats)\n",
    "\n",
    "model_stats_path = os.path.join(figure_dir, \"Table5_Model_Statistics.csv\")\n",
    "model_stats.to_csv(model_stats_path, index=False)\n",
    "print(f\" Saved: {model_stats_path}\")\n",
    "\n",
    "# ========================================\n",
    "# 7 PERIOD COMPARISON (COVID vs Non-COVID)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"7 PERIOD COMPARISON (COVID vs Non-COVID)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'covid_period' in df.columns:\n",
    "    # Calculate mean CAR by period\n",
    "    period_stats = df.groupby('covid_period')['car'].agg(['count', 'mean', 'std']).reset_index()\n",
    "    period_stats.columns = ['Period', 'Count', 'Mean CAR', 'Std Dev']\n",
    "    period_stats['Period'] = period_stats['Period'].map({0: 'Non-COVID', 1: 'COVID'})\n",
    "    \n",
    "    print(\"\\nCAR by Period:\")\n",
    "    print(period_stats)\n",
    "    \n",
    "    # Create bar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['#4CAF50', '#F44336']\n",
    "    bars = ax.bar(period_stats['Period'], period_stats['Mean CAR'], \n",
    "                  color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Add error bars (standard deviation)\n",
    "    ax.errorbar(period_stats['Period'], period_stats['Mean CAR'], \n",
    "                yerr=period_stats['Std Dev'], fmt='none', \n",
    "                ecolor='black', capsize=10, capthick=2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, period_stats['Mean CAR'])):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{val:.4f}',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Add sample size labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, period_stats['Count'])):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., -0.005,\n",
    "                f'n = {count}',\n",
    "                ha='center', va='top', fontsize=10, style='italic')\n",
    "    \n",
    "    ax.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    ax.set_ylabel('Mean CAR (-1, +1)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Period', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Figure 5: CAR Comparison - COVID vs Non-COVID Period (All Companies)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    period_path = os.path.join(figure_dir, \"Figure5_Period_Comparison.png\")\n",
    "    plt.savefig(period_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n Saved: {period_path}\")\n",
    "    \n",
    "    # T-test for difference\n",
    "    covid_car = df[df['covid_period'] == 1]['car'].dropna()\n",
    "    non_covid_car = df[df['covid_period'] == 0]['car'].dropna()\n",
    "    \n",
    "    t_stat, p_value = stats.ttest_ind(covid_car, non_covid_car)\n",
    "    print(f\"\\nT-test for difference in means:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "    \n",
    "    # Save period statistics\n",
    "    period_stats_path = os.path.join(figure_dir, \"Table6_Period_Statistics.csv\")\n",
    "    period_stats.to_csv(period_stats_path, index=False)\n",
    "    print(f\" Saved: {period_stats_path}\")\n",
    "else:\n",
    "    print(\" 'covid_period' column not found in dataset. Skipping period comparison.\")\n",
    "\n",
    "# ========================================\n",
    "# FINAL SUMMARY\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" ALL VISUALIZATIONS COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAll figures and tables saved to: {figure_dir}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  Tables:\")\n",
    "print(\"    - Table1_Descriptive_Statistics.csv & .png\")\n",
    "print(\"    - Table2_Correlation_Matrix.csv\")\n",
    "print(\"    - Table3_Industry_Statistics.csv\")\n",
    "print(\"    - Table4_Regression_Results.csv\")\n",
    "print(\"    - Table5_Model_Statistics.csv\")\n",
    "if 'covid_period' in df.columns:\n",
    "    print(\"    - Table6_Period_Statistics.csv\")\n",
    "print(\"\\n  Figures:\")\n",
    "print(\"    - Figure1_Correlation_Matrix.png\")\n",
    "print(\"    - Figure2_Sentiment_CAR_Scatterplots.png\")\n",
    "print(\"    - Figure3_Industry_CAR_Boxplot.png\")\n",
    "print(\"    - Figure4_Control_Variables_Scatterplots.png\")\n",
    "if 'covid_period' in df.columns:\n",
    "    print(\"    - Figure5_Period_Comparison.png\")\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
